{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 202), (6, 202))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrna_loader.get_fold(0)[0].shape, mirna_loader.get_fold(0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_mutation/mrna.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    # \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_mutation/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_mutation/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:29,885] A new study created in memory with name: no-name-aff5f2f3-7e42-4802-9278-809a49e9b386\n",
      "[I 2024-11-21 00:30:29,980] Trial 0 finished with value: 0.5015952108843537 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,064] Trial 1 finished with value: 0.15415789115646264 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,149] Trial 2 finished with value: 0.07202418367346938 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.5015952108843537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.502\n",
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.219\n",
      "F1 Macro: 0.783 ± 0.262\n",
      "F1 Weighted: 0.794 ± 0.241\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:30,234] Trial 3 finished with value: 0.191465 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,338] Trial 4 finished with value: 0.4831143764172336 and parameters: {'n_neighbors': 4}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,424] Trial 5 finished with value: 0.49026612244897955 and parameters: {'n_neighbors': 3}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,507] Trial 6 finished with value: 0.15063922146636433 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,593] Trial 7 finished with value: 0.15063922146636427 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,677] Trial 8 finished with value: 0.05594671201814059 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,763] Trial 9 finished with value: 0.49026612244897955 and parameters: {'n_neighbors': 3}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,848] Trial 10 finished with value: 0.25180891912320497 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:30,936] Trial 11 finished with value: 0.42958367346938797 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,022] Trial 12 finished with value: 0.2616025532879819 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,107] Trial 13 finished with value: 0.42958367346938797 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,194] Trial 14 finished with value: 0.2616025532879819 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,285] Trial 15 finished with value: 0.3107291753590325 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,370] Trial 16 finished with value: 0.5015952108843537 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,456] Trial 17 finished with value: 0.19458592592592597 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,552] Trial 18 finished with value: 0.2616025532879819 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.5015952108843537.\n",
      "[I 2024-11-21 00:30:31,639] Trial 19 finished with value: 0.2616025532879819 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.5015952108843537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_mutation/mrna_mirna_te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:31,674] A new study created in memory with name: no-name-f8039305-ed28-4445-9df4-98ed042ddec7\n",
      "[I 2024-11-21 00:30:31,740] Trial 0 finished with value: 0.42958367346938797 and parameters: {'C': 0.041236986192132334, 'class_weight': None}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:31,827] Trial 1 finished with value: 0.3596973635676495 and parameters: {'C': 0.3165462498442943, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.430\n",
      "Best model performance:\n",
      "Accuracy: 0.767 ± 0.198\n",
      "F1 Macro: 0.743 ± 0.240\n",
      "F1 Weighted: 0.754 ± 0.219\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:31,902] Trial 2 finished with value: 0.2812586666666667 and parameters: {'C': 4.594062397972703, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:31,971] Trial 3 finished with value: 0.3596973635676495 and parameters: {'C': 0.7629416764135862, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,034] Trial 4 finished with value: 0.42958367346938797 and parameters: {'C': 0.037869699580094826, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,104] Trial 5 finished with value: 0.2812586666666667 and parameters: {'C': 2.116597926904375, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,173] Trial 6 finished with value: 0.2812586666666667 and parameters: {'C': 1.022869480469173, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,241] Trial 7 finished with value: 0.3596973635676495 and parameters: {'C': 0.13265903473879087, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,309] Trial 8 finished with value: 0.3596973635676495 and parameters: {'C': 0.5368674246359296, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,372] Trial 9 finished with value: 0.42958367346938797 and parameters: {'C': 0.01911565566481855, 'class_weight': None}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,440] Trial 10 finished with value: 0.42958367346938797 and parameters: {'C': 0.07399177390701539, 'class_weight': None}. Best is trial 0 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:32,502] Trial 11 finished with value: 0.49026612244897955 and parameters: {'C': 0.010327724510976206, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,584] Trial 12 finished with value: 0.49026612244897955 and parameters: {'C': 0.012771412157183644, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,645] Trial 13 finished with value: 0.49026612244897955 and parameters: {'C': 0.011616806651683862, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.490\n",
      "Best model performance:\n",
      "Accuracy: 0.800 ± 0.219\n",
      "F1 Macro: 0.777 ± 0.261\n",
      "F1 Weighted: 0.789 ± 0.240\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:32,709] Trial 14 finished with value: 0.49026612244897955 and parameters: {'C': 0.01019553646387849, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,777] Trial 15 finished with value: 0.3596973635676495 and parameters: {'C': 0.16463808400953098, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,842] Trial 16 finished with value: 0.42958367346938797 and parameters: {'C': 0.030652447991480643, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,909] Trial 17 finished with value: 0.42958367346938797 and parameters: {'C': 0.06521273627490695, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:32,972] Trial 18 finished with value: 0.42958367346938797 and parameters: {'C': 0.019781742280381744, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,043] Trial 19 finished with value: 0.2812586666666667 and parameters: {'C': 8.78728049567163, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,127] Trial 20 finished with value: 0.3596973635676495 and parameters: {'C': 0.1638079504171343, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,188] Trial 21 finished with value: 0.49026612244897955 and parameters: {'C': 0.010258426160866817, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,253] Trial 22 finished with value: 0.49026612244897955 and parameters: {'C': 0.017246692537703924, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,316] Trial 23 finished with value: 0.49026612244897955 and parameters: {'C': 0.011098041234899533, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,382] Trial 24 finished with value: 0.42958367346938797 and parameters: {'C': 0.02235543448039816, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,448] Trial 25 finished with value: 0.42958367346938797 and parameters: {'C': 0.078431571915546, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,515] Trial 26 finished with value: 0.42958367346938797 and parameters: {'C': 0.04872595534572418, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,578] Trial 27 finished with value: 0.49026612244897955 and parameters: {'C': 0.015579972011794669, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,661] Trial 28 finished with value: 0.42958367346938797 and parameters: {'C': 0.029801405033701798, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,726] Trial 29 finished with value: 0.42958367346938797 and parameters: {'C': 0.04608975603858428, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,793] Trial 30 finished with value: 0.3596973635676495 and parameters: {'C': 0.10389349512506328, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,856] Trial 31 finished with value: 0.49026612244897955 and parameters: {'C': 0.01058078078866625, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,921] Trial 32 finished with value: 0.49026612244897955 and parameters: {'C': 0.014058339776238786, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:33,987] Trial 33 finished with value: 0.42958367346938797 and parameters: {'C': 0.02467819665167896, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,051] Trial 34 finished with value: 0.49026612244897955 and parameters: {'C': 0.014372148605595728, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,116] Trial 35 finished with value: 0.42958367346938797 and parameters: {'C': 0.03645142482781544, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,195] Trial 36 finished with value: 0.49026612244897955 and parameters: {'C': 0.010001030348816541, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,262] Trial 37 finished with value: 0.42958367346938797 and parameters: {'C': 0.025172667261517595, 'class_weight': 'balanced'}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,330] Trial 38 finished with value: 0.2812586666666667 and parameters: {'C': 2.865248495525527, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,398] Trial 39 finished with value: 0.42958367346938797 and parameters: {'C': 0.052098701979700734, 'class_weight': 'balanced'}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,468] Trial 40 finished with value: 0.3596973635676495 and parameters: {'C': 0.24938005866337312, 'class_weight': 'balanced'}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,532] Trial 41 finished with value: 0.49026612244897955 and parameters: {'C': 0.012426357588020102, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,596] Trial 42 finished with value: 0.49026612244897955 and parameters: {'C': 0.016765222646191597, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,660] Trial 43 finished with value: 0.49026612244897955 and parameters: {'C': 0.01044359742887079, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,740] Trial 44 finished with value: 0.42958367346938797 and parameters: {'C': 0.021467360328176643, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,809] Trial 45 finished with value: 0.2812586666666667 and parameters: {'C': 1.074008471514867, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,874] Trial 46 finished with value: 0.42958367346938797 and parameters: {'C': 0.035150115312646085, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:34,940] Trial 47 finished with value: 0.42958367346938797 and parameters: {'C': 0.014900527755194399, 'class_weight': 'balanced'}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:35,009] Trial 48 finished with value: 0.3596973635676495 and parameters: {'C': 0.4391611994984102, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n",
      "[I 2024-11-21 00:30:35,074] Trial 49 finished with value: 0.42958367346938797 and parameters: {'C': 0.027579506039767183, 'class_weight': None}. Best is trial 11 with value: 0.49026612244897955.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:35,112] A new study created in memory with name: no-name-8f858caa-27dc-42d9-be7e-e6d47b858406\n",
      "[I 2024-11-21 00:30:35,321] Trial 0 finished with value: 0.2582725925925926 and parameters: {'booster': 'gbtree', 'lambda': 5.23505090170881e-07, 'alpha': 0.0008071606999466375, 'max_depth': 4, 'eta': 5.103935255475582e-06, 'gamma': 4.9903453896901794e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.2582725925925926.\n",
      "[I 2024-11-21 00:30:35,431] Trial 1 finished with value: 0.3596973635676495 and parameters: {'booster': 'gblinear', 'lambda': 0.27295614619199216, 'alpha': 2.2734779398327574e-05}. Best is trial 1 with value: 0.3596973635676495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.258\n",
      "Best model performance:\n",
      "Accuracy: 0.653 ± 0.148\n",
      "F1 Macro: 0.620 ± 0.183\n",
      "F1 Weighted: 0.638 ± 0.165\n",
      "[{'acc': 0.6666666666666666, 'f1_macro': np.float64(0.6666666666666666), 'f1_weighted': np.float64(0.6666666666666666)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.6)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n",
      "New best score: 0.360\n",
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.207\n",
      "F1 Macro: 0.700 ± 0.246\n",
      "F1 Weighted: 0.708 ± 0.229\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.5666666666666667)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:35,566] Trial 2 finished with value: 0.33674565457294026 and parameters: {'booster': 'gbtree', 'lambda': 0.00017240975103795434, 'alpha': 0.014900024299436137, 'max_depth': 6, 'eta': 0.003421170642741289, 'gamma': 0.016400102509908768, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:35,713] Trial 3 finished with value: 0.33674565457294026 and parameters: {'booster': 'gbtree', 'lambda': 0.40736735619943437, 'alpha': 4.055651295265588e-05, 'max_depth': 9, 'eta': 0.011000616614151654, 'gamma': 0.20540175004557543, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:35,850] Trial 4 finished with value: 0.2840100075585789 and parameters: {'booster': 'gbtree', 'lambda': 0.0008279521360049471, 'alpha': 3.6167846160228685e-06, 'max_depth': 3, 'eta': 0.00033133289651322454, 'gamma': 0.07865679865163754, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,022] Trial 5 finished with value: 0.33674565457294026 and parameters: {'booster': 'dart', 'lambda': 9.717112625204987e-07, 'alpha': 0.17883601228013585, 'max_depth': 7, 'eta': 0.011311454494196708, 'gamma': 2.2118256220470144e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 2.3548271042864956e-05, 'skip_drop': 3.8206564751605325e-07}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,203] Trial 6 finished with value: 0.2582725925925926 and parameters: {'booster': 'gbtree', 'lambda': 1.6222728232292653e-08, 'alpha': 1.4301282577253659e-08, 'max_depth': 4, 'eta': 2.221287106918498e-05, 'gamma': 6.9370349425138734e-06, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,412] Trial 7 finished with value: 0.33674565457294026 and parameters: {'booster': 'dart', 'lambda': 3.3101822161646322e-06, 'alpha': 0.10221803433612038, 'max_depth': 4, 'eta': 0.0028692545006127846, 'gamma': 9.863915461208068e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.065421510725191, 'skip_drop': 0.1754675186532454}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,581] Trial 8 finished with value: 0.2582725925925926 and parameters: {'booster': 'gbtree', 'lambda': 0.0013296944796239803, 'alpha': 1.2550395857204374e-05, 'max_depth': 3, 'eta': 3.049100367882981e-08, 'gamma': 2.2588269408166748e-08, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,811] Trial 9 finished with value: 0.29145508692365835 and parameters: {'booster': 'dart', 'lambda': 2.1013483750933644e-08, 'alpha': 0.2793606838273885, 'max_depth': 2, 'eta': 1.2415937983252009e-05, 'gamma': 0.060857101948476114, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.0650037526094585e-05, 'skip_drop': 1.3491633651054523e-07}. Best is trial 1 with value: 0.3596973635676495.\n",
      "[I 2024-11-21 00:30:36,976] Trial 10 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.6798552202735352, 'alpha': 2.463010858827033e-07}. Best is trial 10 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:37,144] Trial 11 finished with value: 0.3596973635676495 and parameters: {'booster': 'gblinear', 'lambda': 0.44087009035371194, 'alpha': 1.523852874042534e-07}. Best is trial 10 with value: 0.42958367346938797.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.430\n",
      "Best model performance:\n",
      "Accuracy: 0.767 ± 0.198\n",
      "F1 Macro: 0.743 ± 0.240\n",
      "F1 Weighted: 0.754 ± 0.219\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:37,303] Trial 12 finished with value: 0.3596973635676495 and parameters: {'booster': 'gblinear', 'lambda': 0.01213849908772703, 'alpha': 7.926912443221581e-07}. Best is trial 10 with value: 0.42958367346938797.\n",
      "[I 2024-11-21 00:30:37,468] Trial 13 finished with value: 0.5215017687074832 and parameters: {'booster': 'gblinear', 'lambda': 0.021907054267036763, 'alpha': 0.0004743751942390825}. Best is trial 13 with value: 0.5215017687074832.\n",
      "[I 2024-11-21 00:30:37,627] Trial 14 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.01741263546362905, 'alpha': 0.0007784497702992752}. Best is trial 14 with value: 0.5974049281934997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.522\n",
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.127\n",
      "F1 Macro: 0.802 ± 0.132\n",
      "F1 Weighted: 0.806 ± 0.127\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.6)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n",
      "New best score: 0.597\n",
      "Best model performance:\n",
      "Accuracy: 0.847 ± 0.078\n",
      "F1 Macro: 0.838 ± 0.084\n",
      "F1 Weighted: 0.842 ± 0.080\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:37,753] Trial 15 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.016394243578652098, 'alpha': 0.001130203308511788}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:37,914] Trial 16 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.01871421570988724, 'alpha': 0.0044857432248215385}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,041] Trial 17 finished with value: 0.3925777777777778 and parameters: {'booster': 'gblinear', 'lambda': 2.3139714971404177e-05, 'alpha': 0.00022071213254360185}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,156] Trial 18 finished with value: 0.44755392290249446 and parameters: {'booster': 'gblinear', 'lambda': 0.002833153472057915, 'alpha': 0.005504604152425021}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,252] Trial 19 finished with value: 0.5162637399848828 and parameters: {'booster': 'gblinear', 'lambda': 0.057206902120750455, 'alpha': 0.02779088701858893}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,376] Trial 20 finished with value: 0.3925777777777778 and parameters: {'booster': 'gblinear', 'lambda': 8.934604347745596e-05, 'alpha': 0.0018414339018266162}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,507] Trial 21 finished with value: 0.5162637399848828 and parameters: {'booster': 'gblinear', 'lambda': 0.006374348778004115, 'alpha': 0.0033322408240115775}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,624] Trial 22 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.052193598233432024, 'alpha': 0.0001537178588400319}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,773] Trial 23 finished with value: 0.521501768707483 and parameters: {'booster': 'gblinear', 'lambda': 0.0004782199730127292, 'alpha': 0.02702336651643707}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:38,877] Trial 24 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.10499170659361963, 'alpha': 0.0008008520060015673}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,042] Trial 25 finished with value: 0.3732041360544219 and parameters: {'booster': 'dart', 'lambda': 0.0055879629107944235, 'alpha': 0.936970772234808, 'max_depth': 1, 'eta': 0.59398143982489, 'gamma': 1.0711603451465435e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.16665065454445713, 'skip_drop': 0.4397402947121732}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,214] Trial 26 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.02585242633414522, 'alpha': 0.0070225840792675015}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,346] Trial 27 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.14779328950432166, 'alpha': 7.371906142473931e-05}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,447] Trial 28 finished with value: 0.3925777777777778 and parameters: {'booster': 'gblinear', 'lambda': 0.00023989092660988535, 'alpha': 0.002204551656772486}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,670] Trial 29 finished with value: 0.2582725925925926 and parameters: {'booster': 'dart', 'lambda': 0.0030124722591634845, 'alpha': 0.0004245034486252569, 'max_depth': 9, 'eta': 4.697919073996154e-08, 'gamma': 6.020979671610211e-07, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 6.917777771397614e-08, 'skip_drop': 0.0003432025660465159}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:39,863] Trial 30 finished with value: 0.33008829629629627 and parameters: {'booster': 'gblinear', 'lambda': 1.674883707835947e-05, 'alpha': 0.0011465658540750984}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,047] Trial 31 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.028731406847577013, 'alpha': 0.006866563749702204}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,160] Trial 32 finished with value: 0.521501768707483 and parameters: {'booster': 'gblinear', 'lambda': 0.014933838171436257, 'alpha': 0.05845872310407878}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,263] Trial 33 finished with value: 0.44755392290249446 and parameters: {'booster': 'gblinear', 'lambda': 0.001680339046628817, 'alpha': 0.01066837028665647}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,413] Trial 34 finished with value: 0.3596973635676495 and parameters: {'booster': 'gblinear', 'lambda': 0.24988168792402785, 'alpha': 1.2691418456309482e-05}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,559] Trial 35 finished with value: 0.30013281330309904 and parameters: {'booster': 'gbtree', 'lambda': 0.048891246636602236, 'alpha': 0.018435769785795253, 'max_depth': 7, 'eta': 0.7754880977946259, 'gamma': 0.0012947334421677915, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,664] Trial 36 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.008481500774738386, 'alpha': 5.5181294031412044e-05}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,792] Trial 37 finished with value: 0.5215017687074832 and parameters: {'booster': 'gblinear', 'lambda': 0.0004462202927850591, 'alpha': 0.0003469514610069693}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:40,986] Trial 38 finished with value: 0.29145508692365835 and parameters: {'booster': 'gbtree', 'lambda': 0.09356773800982941, 'alpha': 0.0030384366753684728, 'max_depth': 1, 'eta': 2.0744581261646814e-07, 'gamma': 0.0014087016114701049, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,248] Trial 39 finished with value: 0.2582725925925926 and parameters: {'booster': 'dart', 'lambda': 0.7581272158007532, 'alpha': 0.04826400140444697, 'max_depth': 6, 'eta': 5.912509129001316e-07, 'gamma': 1.7018672068502618e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.7111701852390982e-08, 'skip_drop': 0.00020310410759011505}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,393] Trial 40 finished with value: 0.3907851851851852 and parameters: {'booster': 'gblinear', 'lambda': 1.3554910303140697e-07, 'alpha': 2.0699776990473805e-05}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,518] Trial 41 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.024644963771518397, 'alpha': 0.006949263061074934}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,658] Trial 42 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.023762642030858934, 'alpha': 0.008383955476949641}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,797] Trial 43 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.21305256875960746, 'alpha': 0.0010166104822561123}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:41,985] Trial 44 finished with value: 0.29145508692365835 and parameters: {'booster': 'gbtree', 'lambda': 0.0024839306551728666, 'alpha': 0.1620908008257127, 'max_depth': 8, 'eta': 0.0001569951440487404, 'gamma': 1.0807692887358145e-07, 'grow_policy': 'lossguide'}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,083] Trial 45 finished with value: 0.5162637399848828 and parameters: {'booster': 'gblinear', 'lambda': 0.03216713160200565, 'alpha': 0.004052547899819369}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,256] Trial 46 finished with value: 0.5215017687074832 and parameters: {'booster': 'gblinear', 'lambda': 0.00586002130301211, 'alpha': 0.00011971980976038945}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,390] Trial 47 finished with value: 0.44755392290249446 and parameters: {'booster': 'gblinear', 'lambda': 0.012210801837881992, 'alpha': 0.013496191378956232}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,597] Trial 48 finished with value: 0.3938012887377172 and parameters: {'booster': 'dart', 'lambda': 0.00010102019958611623, 'alpha': 0.0008275649596293176, 'max_depth': 5, 'eta': 0.15320589720544267, 'gamma': 0.756046135248484, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0010628876092446677, 'skip_drop': 1.013441406366558e-08}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,745] Trial 49 finished with value: 0.2582725925925926 and parameters: {'booster': 'gbtree', 'lambda': 0.0009487388359862044, 'alpha': 2.491488789021372e-06, 'max_depth': 8, 'eta': 1.997340708607905e-06, 'gamma': 0.0017690701362293136, 'grow_policy': 'depthwise'}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:42,879] Trial 50 finished with value: 0.07202418367346938 and parameters: {'booster': 'gblinear', 'lambda': 0.08437924988793134, 'alpha': 0.4348138423600184}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,029] Trial 51 finished with value: 0.5846643809523812 and parameters: {'booster': 'gblinear', 'lambda': 0.026866858474869408, 'alpha': 0.006339031077829652}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,147] Trial 52 finished with value: 0.42958367346938797 and parameters: {'booster': 'gblinear', 'lambda': 0.33611518325809286, 'alpha': 0.0018558353714952752}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,322] Trial 53 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.004034169661540756, 'alpha': 0.0002272865676704801}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,512] Trial 54 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.0031331453271797316, 'alpha': 0.00022138194226687015}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,639] Trial 55 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.003802940605410921, 'alpha': 0.0003151518757474715}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,769] Trial 56 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.003749883416508087, 'alpha': 0.00028842564314552195}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:43,869] Trial 57 finished with value: 0.5215017687074832 and parameters: {'booster': 'gblinear', 'lambda': 0.00064004201613256, 'alpha': 3.419795631411404e-05}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:44,013] Trial 58 finished with value: 0.5974049281934997 and parameters: {'booster': 'gblinear', 'lambda': 0.001599060376028611, 'alpha': 0.0002497486023777401}. Best is trial 14 with value: 0.5974049281934997.\n",
      "[I 2024-11-21 00:30:44,144] Trial 59 finished with value: 0.5215017687074832 and parameters: {'booster': 'gblinear', 'lambda': 0.002766947679798513, 'alpha': 0.00011896572653852831}. Best is trial 14 with value: 0.5974049281934997.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.847 ± 0.078\n",
      "F1 Macro: 0.838 ± 0.084\n",
      "F1 Weighted: 0.842 ± 0.080\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:30:44,190] A new study created in memory with name: no-name-058d015a-5799-44a1-831e-731a625d452f\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:45,318] Trial 0 finished with value: 0.42958367346938797 and parameters: {'lr': 0.006265914892111137, 'dropout': 0.3936038236701973}. Best is trial 0 with value: 0.42958367346938797.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.430\n",
      "Best model performance:\n",
      "Accuracy: 0.767 ± 0.198\n",
      "F1 Macro: 0.743 ± 0.240\n",
      "F1 Weighted: 0.754 ± 0.219\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.4, 'f1_macro': np.float64(0.2857142857142857), 'f1_weighted': np.float64(0.34285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:46,403] Trial 1 finished with value: 0.44755392290249446 and parameters: {'lr': 0.0003091566434978957, 'dropout': 0.5988602906642828}. Best is trial 1 with value: 0.44755392290249446.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.448\n",
      "Best model performance:\n",
      "Accuracy: 0.767 ± 0.084\n",
      "F1 Macro: 0.762 ± 0.090\n",
      "F1 Weighted: 0.766 ± 0.084\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.6)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:47,498] Trial 2 finished with value: 0.3596973635676495 and parameters: {'lr': 0.004628673485147053, 'dropout': 0.37746695655027707}. Best is trial 1 with value: 0.44755392290249446.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:48,585] Trial 3 finished with value: 0.5236682448979592 and parameters: {'lr': 0.00011170588776907684, 'dropout': 0.36487055100991794}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.524\n",
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.013\n",
      "F1 Macro: 0.806 ± 0.011\n",
      "F1 Weighted: 0.806 ± 0.011\n",
      "[{'acc': 0.8333333333333334, 'f1_macro': np.float64(0.8285714285714285), 'f1_weighted': np.float64(0.8285714285714286)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:49,696] Trial 4 finished with value: 0.3791914739229025 and parameters: {'lr': 0.00012100445296514338, 'dropout': 0.558992948991019}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:50,772] Trial 5 finished with value: 0.44755392290249446 and parameters: {'lr': 0.00023792168554725776, 'dropout': 0.4149655707616621}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:51,874] Trial 6 finished with value: 0.44171752078609233 and parameters: {'lr': 0.0021463682642744284, 'dropout': 0.30347730978413967}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:52,969] Trial 7 finished with value: 0.44171752078609233 and parameters: {'lr': 0.002023149324313387, 'dropout': 0.1950182153500454}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:54,077] Trial 8 finished with value: 0.44755392290249446 and parameters: {'lr': 0.00014792195456271118, 'dropout': 0.22118893509389767}. Best is trial 3 with value: 0.5236682448979592.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:30:55,171] Trial 9 finished with value: 0.3023466515495088 and parameters: {'lr': 0.00030942510763608055, 'dropout': 0.16695076204958378}. Best is trial 3 with value: 0.5236682448979592.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.013\n",
      "F1 Macro: 0.806 ± 0.011\n",
      "F1 Weighted: 0.806 ± 0.011\n",
      "Best hyperparameters:\n",
      "{'lr': 0.00011170588776907684, 'dropout': 0.36487055100991794}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-12-08 15:38:24,609] A new study created in memory with name: no-name-0669d676-ee34-4260-b270-ffbaacea6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 15:39:06,540] Trial 0 finished with value: 1.0 and parameters: {}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 1.000\n",
      "Best model performance:\n",
      "Accuracy: 1.000 ± 0.000\n",
      "F1 Macro: 1.000 ± 0.000\n",
      "F1 Weighted: 1.000 ± 0.000\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 1.000 ± 0.000\n",
      "F1 Macro: 1.000 ± 0.000\n",
      "F1 Weighted: 1.000 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(24.2308)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(20.3846)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(24.2308)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(20.3846)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:31:14,429] A new study created in memory with name: no-name-44d91e7e-539e-4480-a160-bc20b749b14f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(24.2308)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(20.3846)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6883, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6881, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.6881, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.6889, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.6881, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.6881, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.6881, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(4) tensor(23.5000)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1154)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(3) tensor(14.9615)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5357, Train Acc: 0.7143, Train F1 Macro: 0.6500, Train F1 Weighted: 0.6714\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5667, Val Geometric Mean: 0.5832\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5667\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1732, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0240, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0010, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1339, Train Acc: 0.9048, Train F1 Macro: 0.9045, Train F1 Weighted: 0.9052\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(24.5000)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(20.7692)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(2) tensor(16.2308)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6561, Train Acc: 0.5238, Train F1 Macro: 0.3438, Train F1 Weighted: 0.3601\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1535, Train Acc: 0.9524, Train F1 Macro: 0.9519, Train F1 Weighted: 0.9522\n",
      "Val Acc: 0.2000, Val F1 Macro: 0.1667, Val F1 Weighted: 0.1333, Val Geometric Mean: 0.1644\n",
      "Test Acc: 0.2000, Test F1 Macro: 0.1667, Test F1 Weighted: 0.1333\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0698, Train Acc: 0.9524, Train F1 Macro: 0.9524, Train F1 Weighted: 0.9524\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.2286, Val Geometric Mean: 0.2967\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.2286\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1418, Train Acc: 0.9524, Train F1 Macro: 0.9524, Train F1 Weighted: 0.9524\n",
      "Val Acc: 0.2000, Val F1 Macro: 0.1667, Val F1 Weighted: 0.1333, Val Geometric Mean: 0.1644\n",
      "Test Acc: 0.2000, Test F1 Macro: 0.1667, Test F1 Weighted: 0.1333\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0002, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.2000, Val F1 Macro: 0.1667, Val F1 Weighted: 0.1333, Val Geometric Mean: 0.1644\n",
      "Test Acc: 0.2000, Test F1 Macro: 0.1667, Test F1 Weighted: 0.1333\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.2286, Val Geometric Mean: 0.2967\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.2286\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0004, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.2286, Val Geometric Mean: 0.2967\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.2286\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(23.7692)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(22.2308)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(16.2692)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3575, Train Acc: 0.8095, Train F1 Macro: 0.7981, Train F1 Weighted: 0.8004\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0624, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.4000, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.4000\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.4000, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0862, Train Acc: 0.9524, Train F1 Macro: 0.9524, Train F1 Weighted: 0.9524\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.3429, Val Geometric Mean: 0.3397\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.3429\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.4000, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.4000\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.4000, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0554, Train Acc: 0.9524, Train F1 Macro: 0.9524, Train F1 Weighted: 0.9524\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0535, Train Acc: 0.9524, Train F1 Macro: 0.9519, Train F1 Weighted: 0.9522\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(3) tensor(23.8077)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(20.0769)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(2) tensor(15.9231)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4688, Train Acc: 0.6667, Train F1 Macro: 0.6541, Train F1 Weighted: 0.6573\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.7619, Val F1 Weighted: 0.7810, Val Geometric Mean: 0.7808\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.7619, Test F1 Weighted: 0.7810\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4575, Train Acc: 0.7143, Train F1 Macro: 0.7083, Train F1 Weighted: 0.7103\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.3429, Val Geometric Mean: 0.3397\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.3429\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.3766, Train Acc: 0.8571, Train F1 Macro: 0.8571, Train F1 Weighted: 0.8571\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5667, Val Geometric Mean: 0.5832\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5667\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2097, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0686, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0375, Train Acc: 0.9524, Train F1 Macro: 0.9524, Train F1 Weighted: 0.9524\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:32:41,913] Trial 0 finished with value: 0.43291733333333343 and parameters: {}. Best is trial 0 with value: 0.43291733333333343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0453, Train Acc: 0.9524, Train F1 Macro: 0.9519, Train F1 Weighted: 0.9522\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "New best score: 0.433\n",
      "Best model performance:\n",
      "Accuracy: 0.780 ± 0.204\n",
      "F1 Macro: 0.743 ± 0.256\n",
      "F1 Weighted: 0.747 ± 0.254\n",
      "[{'acc': 0.5, 'f1_macro': np.float64(0.3333333333333333), 'f1_weighted': np.float64(0.3333333333333333)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.6)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.780 ± 0.204\n",
      "F1 Macro: 0.743 ± 0.256\n",
      "F1 Weighted: 0.747 ± 0.254\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": True,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
