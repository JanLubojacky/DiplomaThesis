{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_mutation/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_mutation/te.csv'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    # \"mrna\": mrna_loader,\n",
    "    # \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_mutation/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_mutation/te.csv'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:21,294] A new study created in memory with name: no-name-7681ad45-d27e-4ec5-9483-5a46d81f6b51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:21,342] Trial 0 finished with value: 0.2503608888888888 and parameters: {'n_neighbors': 3}. Best is trial 0 with value: 0.2503608888888888.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.250\n",
      "Best model performance:\n",
      "Accuracy: 0.653 ± 0.232\n",
      "F1 Macro: 0.623 ± 0.256\n",
      "F1 Weighted: 0.615 ± 0.273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:21,385] Trial 1 finished with value: 0.07202418367346938 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,430] Trial 2 finished with value: 0.07202418367346938 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,470] Trial 3 finished with value: 0.2503608888888888 and parameters: {'n_neighbors': 3}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,507] Trial 4 finished with value: 0.16350140589569157 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,545] Trial 5 finished with value: 0.08695346938775511 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,583] Trial 6 finished with value: 0.07202418367346938 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,620] Trial 7 finished with value: 0.11088258503401362 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,660] Trial 8 finished with value: 0.07202418367346938 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.2503608888888888.\n",
      "[I 2024-11-18 00:31:21,700] Trial 9 finished with value: 0.25735166666666665 and parameters: {'n_neighbors': 6}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:21,744] Trial 10 finished with value: 0.14713173015873013 and parameters: {'n_neighbors': 8}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:21,787] Trial 11 finished with value: 0.1268119123204838 and parameters: {'n_neighbors': 1}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:21,830] Trial 12 finished with value: 0.2009313378684807 and parameters: {'n_neighbors': 7}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:21,870] Trial 13 finished with value: 0.2009313378684807 and parameters: {'n_neighbors': 7}. Best is trial 9 with value: 0.25735166666666665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.257\n",
      "Best model performance:\n",
      "Accuracy: 0.653 ± 0.078\n",
      "F1 Macro: 0.627 ± 0.069\n",
      "F1 Weighted: 0.628 ± 0.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:21,911] Trial 14 finished with value: 0.1452048412698413 and parameters: {'n_neighbors': 2}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:21,958] Trial 15 finished with value: 0.11088258503401362 and parameters: {'n_neighbors': 10}. Best is trial 9 with value: 0.25735166666666665.\n",
      "[I 2024-11-18 00:31:22,000] Trial 16 finished with value: 0.30671297959183674 and parameters: {'n_neighbors': 4}. Best is trial 16 with value: 0.30671297959183674.\n",
      "[I 2024-11-18 00:31:22,040] Trial 17 finished with value: 0.16350140589569157 and parameters: {'n_neighbors': 5}. Best is trial 16 with value: 0.30671297959183674.\n",
      "[I 2024-11-18 00:31:22,083] Trial 18 finished with value: 0.11088258503401362 and parameters: {'n_neighbors': 10}. Best is trial 16 with value: 0.30671297959183674.\n",
      "[I 2024-11-18 00:31:22,122] Trial 19 finished with value: 0.16350140589569157 and parameters: {'n_neighbors': 5}. Best is trial 16 with value: 0.30671297959183674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.307\n",
      "Best model performance:\n",
      "Accuracy: 0.687 ± 0.107\n",
      "F1 Macro: 0.668 ± 0.106\n",
      "F1 Weighted: 0.669 ± 0.113\n",
      "Results saved to logs/mds_mutation/te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:22,168] A new study created in memory with name: no-name-1002a790-b935-4a91-b36d-f00a54b72f3f\n",
      "[I 2024-11-18 00:31:22,202] Trial 0 finished with value: 0.20328731670445957 and parameters: {'C': 0.03688004810309476, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.20328731670445957.\n",
      "[I 2024-11-18 00:31:22,238] Trial 1 finished with value: 0.33129795918367344 and parameters: {'C': 1.971435918408769, 'class_weight': None}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,269] Trial 2 finished with value: 0.16703631141345426 and parameters: {'C': 0.019300742437931024, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,299] Trial 3 finished with value: 0.23912413605442182 and parameters: {'C': 0.07038142444210083, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,336] Trial 4 finished with value: 0.33129795918367344 and parameters: {'C': 2.22635278602477, 'class_weight': None}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,374] Trial 5 finished with value: 0.33129795918367344 and parameters: {'C': 4.855017564413183, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,402] Trial 6 finished with value: 0.15988443310657596 and parameters: {'C': 0.013911064530276468, 'class_weight': None}. Best is trial 1 with value: 0.33129795918367344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.203\n",
      "Best model performance:\n",
      "Accuracy: 0.613 ± 0.129\n",
      "F1 Macro: 0.576 ± 0.159\n",
      "F1 Weighted: 0.575 ± 0.186\n",
      "New best score: 0.331\n",
      "Best model performance:\n",
      "Accuracy: 0.720 ± 0.204\n",
      "F1 Macro: 0.679 ± 0.237\n",
      "F1 Weighted: 0.678 ± 0.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:22,433] Trial 7 finished with value: 0.23912413605442182 and parameters: {'C': 0.07833629613743714, 'class_weight': 'balanced'}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:22,468] Trial 8 finished with value: 0.3984226757369614 and parameters: {'C': 0.47269394722891844, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,498] Trial 9 finished with value: 0.16703631141345426 and parameters: {'C': 0.023038817327635016, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,534] Trial 10 finished with value: 0.3984226757369614 and parameters: {'C': 0.3512329047467694, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,574] Trial 11 finished with value: 0.3984226757369614 and parameters: {'C': 0.38204991527353127, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,611] Trial 12 finished with value: 0.3984226757369614 and parameters: {'C': 0.3659293915909551, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,651] Trial 13 finished with value: 0.33129795918367344 and parameters: {'C': 1.008510933506152, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.398\n",
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.233\n",
      "F1 Macro: 0.726 ± 0.270\n",
      "F1 Weighted: 0.722 ± 0.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:22,688] Trial 14 finished with value: 0.3984226757369614 and parameters: {'C': 0.15305892678303948, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,728] Trial 15 finished with value: 0.33129795918367344 and parameters: {'C': 0.5548466767915107, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,763] Trial 16 finished with value: 0.3984226757369614 and parameters: {'C': 0.1611381251801277, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,803] Trial 17 finished with value: 0.33129795918367344 and parameters: {'C': 1.322502566163086, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,842] Trial 18 finished with value: 0.33129795918367344 and parameters: {'C': 9.582534762747473, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,877] Trial 19 finished with value: 0.3984226757369614 and parameters: {'C': 0.2040556392571111, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,915] Trial 20 finished with value: 0.33129795918367344 and parameters: {'C': 0.6872085764982516, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,951] Trial 21 finished with value: 0.3984226757369614 and parameters: {'C': 0.31063124240578005, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:22,988] Trial 22 finished with value: 0.3984226757369614 and parameters: {'C': 0.47770244758486863, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,021] Trial 23 finished with value: 0.3457239425547998 and parameters: {'C': 0.08821843202914745, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,057] Trial 24 finished with value: 0.3984226757369614 and parameters: {'C': 0.25485755327582094, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,097] Trial 25 finished with value: 0.33129795918367344 and parameters: {'C': 0.7168547587513939, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,132] Trial 26 finished with value: 0.3984226757369614 and parameters: {'C': 0.11860087384360143, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,171] Trial 27 finished with value: 0.33129795918367344 and parameters: {'C': 1.1892236652915429, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,203] Trial 28 finished with value: 0.20143858956916103 and parameters: {'C': 0.04493988440560362, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,244] Trial 29 finished with value: 0.33129795918367344 and parameters: {'C': 2.6771744031949667, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,283] Trial 30 finished with value: 0.3984226757369614 and parameters: {'C': 0.3867637731918394, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,330] Trial 31 finished with value: 0.3984226757369614 and parameters: {'C': 0.3124309657357498, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,381] Trial 32 finished with value: 0.33129795918367344 and parameters: {'C': 0.7579102677367299, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,418] Trial 33 finished with value: 0.3984226757369614 and parameters: {'C': 0.4157200970941304, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,454] Trial 34 finished with value: 0.3984226757369614 and parameters: {'C': 0.21965531317119383, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,494] Trial 35 finished with value: 0.33129795918367344 and parameters: {'C': 1.6424187758272897, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,526] Trial 36 finished with value: 0.20143858956916103 and parameters: {'C': 0.04386407371182401, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,565] Trial 37 finished with value: 0.33129795918367344 and parameters: {'C': 0.9843206410533001, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,606] Trial 38 finished with value: 0.33129795918367344 and parameters: {'C': 3.4292292081892652, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,640] Trial 39 finished with value: 0.3984226757369614 and parameters: {'C': 0.1267730345747471, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,679] Trial 40 finished with value: 0.33129795918367344 and parameters: {'C': 0.541207358615794, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,715] Trial 41 finished with value: 0.3984226757369614 and parameters: {'C': 0.17920238849913717, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,750] Trial 42 finished with value: 0.3984226757369614 and parameters: {'C': 0.12582895818316292, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,786] Trial 43 finished with value: 0.3984226757369614 and parameters: {'C': 0.2897230136280067, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,819] Trial 44 finished with value: 0.3457239425547998 and parameters: {'C': 0.07275728078491073, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,856] Trial 45 finished with value: 0.3984226757369614 and parameters: {'C': 0.37934437880153027, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,892] Trial 46 finished with value: 0.3984226757369614 and parameters: {'C': 0.22756104641272362, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,932] Trial 47 finished with value: 0.33129795918367344 and parameters: {'C': 0.8361719245728708, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:23,967] Trial 48 finished with value: 0.33129795918367344 and parameters: {'C': 0.15122982806285992, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:24,006] Trial 49 finished with value: 0.33129795918367344 and parameters: {'C': 0.5722329331851287, 'class_weight': None}. Best is trial 8 with value: 0.3984226757369614.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:24,037] A new study created in memory with name: no-name-f946613e-1a24-4b0d-b5e1-412da4ed6743\n",
      "[I 2024-11-18 00:31:24,132] Trial 0 finished with value: 0.22400650642479217 and parameters: {'lambda': 0.024762836896165426, 'alpha': 0.0008434208607902249}. Best is trial 0 with value: 0.22400650642479217.\n",
      "[I 2024-11-18 00:31:24,238] Trial 1 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.140545272799262, 'alpha': 2.633810458250674e-05}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:24,302] Trial 2 finished with value: 0.29938223733938013 and parameters: {'lambda': 5.226703194990738e-08, 'alpha': 0.08359162829104794}. Best is trial 1 with value: 0.33129795918367344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.224\n",
      "Best model performance:\n",
      "Accuracy: 0.647 ± 0.202\n",
      "F1 Macro: 0.585 ± 0.245\n",
      "F1 Weighted: 0.592 ± 0.254\n",
      "New best score: 0.331\n",
      "Best model performance:\n",
      "Accuracy: 0.720 ± 0.204\n",
      "F1 Macro: 0.679 ± 0.237\n",
      "F1 Weighted: 0.678 ± 0.258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:24,377] Trial 3 finished with value: 0.2301846296296296 and parameters: {'lambda': 4.31285554365321e-08, 'alpha': 1.1614379765098794e-06}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:24,551] Trial 4 finished with value: 0.24366551851851845 and parameters: {'lambda': 3.0427329920268166e-07, 'alpha': 3.0762148988209354e-06}. Best is trial 1 with value: 0.33129795918367344.\n",
      "[I 2024-11-18 00:31:24,660] Trial 5 finished with value: 0.3457239425547998 and parameters: {'lambda': 4.6106588203774704e-05, 'alpha': 3.815278818344569e-07}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:24,755] Trial 6 finished with value: 0.2762561209372638 and parameters: {'lambda': 0.0005802014423618152, 'alpha': 7.165536381678066e-06}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:24,820] Trial 7 finished with value: 0.07202418367346938 and parameters: {'lambda': 0.19044157461006872, 'alpha': 0.29279159498800916}. Best is trial 5 with value: 0.3457239425547998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.346\n",
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.207\n",
      "F1 Macro: 0.692 ± 0.243\n",
      "F1 Weighted: 0.688 ± 0.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:24,899] Trial 8 finished with value: 0.18408153439153435 and parameters: {'lambda': 0.001048721736306589, 'alpha': 0.0008829719798169641}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:24,982] Trial 9 finished with value: 0.29938223733938013 and parameters: {'lambda': 2.6787064087526505e-07, 'alpha': 0.09640647898948532}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:25,097] Trial 10 finished with value: 0.3457239425547998 and parameters: {'lambda': 1.0450720502246986e-05, 'alpha': 1.6188380875696858e-08}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:25,165] Trial 11 finished with value: 0.3457239425547998 and parameters: {'lambda': 7.518805000899733e-06, 'alpha': 1.1053232658208815e-08}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:25,252] Trial 12 finished with value: 0.3457239425547998 and parameters: {'lambda': 1.3141306797161628e-05, 'alpha': 2.342083339721162e-08}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:25,319] Trial 13 finished with value: 0.3457239425547998 and parameters: {'lambda': 2.27689073095865e-05, 'alpha': 2.3404819796065652e-07}. Best is trial 5 with value: 0.3457239425547998.\n",
      "[I 2024-11-18 00:31:25,419] Trial 14 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0004354510514538266, 'alpha': 1.3827768163994003e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,482] Trial 15 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0029762057138382563, 'alpha': 2.7425918713136204e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,554] Trial 16 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.007502130052052272, 'alpha': 0.00021995140963909122}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,621] Trial 17 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.0005925629596109124, 'alpha': 1.677435985822878e-07}. Best is trial 14 with value: 0.3984226757369614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.398\n",
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.233\n",
      "F1 Macro: 0.726 ± 0.270\n",
      "F1 Weighted: 0.722 ± 0.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:25,702] Trial 18 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.8701950261638475, 'alpha': 1.4513367376429416e-05}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,776] Trial 19 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.006418681345822637, 'alpha': 1.2228217952913206e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,875] Trial 20 finished with value: 0.18408153439153435 and parameters: {'lambda': 0.000192063730984887, 'alpha': 0.005341172946128528}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:25,950] Trial 21 finished with value: 0.3457239425547998 and parameters: {'lambda': 6.368819407324962e-05, 'alpha': 8.054303506214285e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,060] Trial 22 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.0034200989515734558, 'alpha': 7.48721375149149e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,190] Trial 23 finished with value: 0.2913833408919123 and parameters: {'lambda': 1.409039432452313e-06, 'alpha': 5.926334800639941e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,279] Trial 24 finished with value: 0.3457239425547998 and parameters: {'lambda': 8.887916148434624e-05, 'alpha': 3.0835894170575513e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,377] Trial 25 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.03660171491551921, 'alpha': 4.385503722767559e-05}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,444] Trial 26 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.00177011123678681, 'alpha': 4.3587392087216016e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,541] Trial 27 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0014146586276475657, 'alpha': 4.621170285004108e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,690] Trial 28 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0002521529724655864, 'alpha': 5.966066732747149e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,752] Trial 29 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.02356934490815305, 'alpha': 2.040288522738371e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,847] Trial 30 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.043085617968455434, 'alpha': 0.00010057396213656854}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:26,940] Trial 31 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.0019258704601127475, 'alpha': 3.42008447173411e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,003] Trial 32 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.010958810162219109, 'alpha': 7.800449920588595e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,084] Trial 33 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.0002811820501252933, 'alpha': 2.596386359289966e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,168] Trial 34 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0015842016274643215, 'alpha': 1.1835162447786226e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,241] Trial 35 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0034889647561539556, 'alpha': 3.685866703804358e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,337] Trial 36 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.19384379396143983, 'alpha': 1.0037575685512688e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,523] Trial 37 finished with value: 0.2848274648526078 and parameters: {'lambda': 0.0005984930236484797, 'alpha': 3.5124456285806563e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,595] Trial 38 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.015292471178647753, 'alpha': 5.298229213586311e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,710] Trial 39 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.00015065769025665506, 'alpha': 1.7547482676665953e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,791] Trial 40 finished with value: 0.22400650642479217 and parameters: {'lambda': 0.07473239196489853, 'alpha': 0.007028081020791232}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:27,900] Trial 41 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.00032120098326079776, 'alpha': 4.521296445450297e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,017] Trial 42 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0013503318233380978, 'alpha': 8.774706395366264e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,085] Trial 43 finished with value: 0.3457239425547998 and parameters: {'lambda': 3.552474871862951e-05, 'alpha': 3.233080905578463e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,171] Trial 44 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.0030145705871924807, 'alpha': 8.918246842558646e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,251] Trial 45 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.0007016093005402499, 'alpha': 6.626125921597825e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,381] Trial 46 finished with value: 0.2762561209372638 and parameters: {'lambda': 3.727563073911014e-06, 'alpha': 3.789406648646559e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,451] Trial 47 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.0003029327244553492, 'alpha': 1.7318602272168306e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,522] Trial 48 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.0055946674475153045, 'alpha': 1.8969740289344247e-06}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,677] Trial 49 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.00011566951689565316, 'alpha': 1.1508788211126791e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,774] Trial 50 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0009247959683580501, 'alpha': 1.455960893113045e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,877] Trial 51 finished with value: 0.3457239425547998 and parameters: {'lambda': 0.0017213235445442412, 'alpha': 1.0441982133535569e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:28,958] Trial 52 finished with value: 0.2913833408919123 and parameters: {'lambda': 2.470213204457273e-08, 'alpha': 2.517969825509437e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,050] Trial 53 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0022496027001673623, 'alpha': 3.025370908596658e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,136] Trial 54 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.00035629811663518253, 'alpha': 5.676879566223588e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,268] Trial 55 finished with value: 0.33129795918367344 and parameters: {'lambda': 0.005970738707288067, 'alpha': 1.4907226595453956e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,413] Trial 56 finished with value: 0.3984226757369614 and parameters: {'lambda': 0.0010080830000720028, 'alpha': 1.0580486189356711e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,515] Trial 57 finished with value: 0.3457239425547998 and parameters: {'lambda': 3.8665688545898056e-05, 'alpha': 2.283832054460998e-08}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,602] Trial 58 finished with value: 0.3457239425547998 and parameters: {'lambda': 6.90039353380412e-05, 'alpha': 1.9374559605148177e-07}. Best is trial 14 with value: 0.3984226757369614.\n",
      "[I 2024-11-18 00:31:29,702] Trial 59 finished with value: 0.3457239425547998 and parameters: {'lambda': 1.9057508752129818e-05, 'alpha': 1.4139067285656947e-06}. Best is trial 14 with value: 0.3984226757369614.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.233\n",
      "F1 Macro: 0.726 ± 0.270\n",
      "F1 Weighted: 0.722 ± 0.289\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:29,734] A new study created in memory with name: no-name-59034f2f-6da7-4a87-a8c7-49e0ddfa8f71\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:30,654] Trial 0 finished with value: 0.20328731670445957 and parameters: {'lr': 0.0012855192533905992, 'dropout': 0.19919553424917388}. Best is trial 0 with value: 0.20328731670445957.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.203\n",
      "Best model performance:\n",
      "Accuracy: 0.613 ± 0.129\n",
      "F1 Macro: 0.576 ± 0.159\n",
      "F1 Weighted: 0.575 ± 0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:31,585] Trial 1 finished with value: 0.29962061980347693 and parameters: {'lr': 0.0015705682478599963, 'dropout': 0.3917102527776928}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.300\n",
      "Best model performance:\n",
      "Accuracy: 0.693 ± 0.200\n",
      "F1 Macro: 0.660 ± 0.233\n",
      "F1 Weighted: 0.655 ± 0.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:32,497] Trial 2 finished with value: 0.23782055555555554 and parameters: {'lr': 0.002759349097982776, 'dropout': 0.3404978899457234}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:33,397] Trial 3 finished with value: 0.12805472789115646 and parameters: {'lr': 0.00011544599852897212, 'dropout': 0.4987193409291879}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:34,368] Trial 4 finished with value: 0.2301846296296296 and parameters: {'lr': 0.003870678397775211, 'dropout': 0.19158587320323536}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:35,296] Trial 5 finished with value: 0.2762561209372638 and parameters: {'lr': 0.004149422437527915, 'dropout': 0.26630132592639055}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:36,215] Trial 6 finished with value: 0.11336855631141347 and parameters: {'lr': 0.00037082515038405864, 'dropout': 0.31752849712309184}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:37,107] Trial 7 finished with value: 0.20328731670445957 and parameters: {'lr': 0.0012658350820332787, 'dropout': 0.21072201405529797}. Best is trial 1 with value: 0.29962061980347693.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:38,019] Trial 8 finished with value: 0.3457239425547998 and parameters: {'lr': 0.008478449364513136, 'dropout': 0.11615904249081146}. Best is trial 8 with value: 0.3457239425547998.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.346\n",
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.207\n",
      "F1 Macro: 0.692 ± 0.243\n",
      "F1 Weighted: 0.688 ± 0.263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:31:38,929] Trial 9 finished with value: 0.1984179954648526 and parameters: {'lr': 0.0032354599892589152, 'dropout': 0.5675335919934774}. Best is trial 8 with value: 0.3457239425547998.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.207\n",
      "F1 Macro: 0.692 ± 0.243\n",
      "F1 Weighted: 0.688 ± 0.263\n",
      "Best hyperparameters:\n",
      "{'lr': 0.008478449364513136, 'dropout': 0.11615904249081146}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:39,031] A new study created in memory with name: no-name-e17524ce-4897-405b-bd3e-e4fe1dd193ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:47,757] Trial 0 finished with value: 0.37396639455782305 and parameters: {}. Best is trial 0 with value: 0.37396639455782305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.374\n",
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.164\n",
      "F1 Macro: 0.716 ± 0.171\n",
      "F1 Weighted: 0.719 ± 0.169\n",
      "Best model performance:\n",
      "Accuracy: 0.727 ± 0.164\n",
      "F1 Macro: 0.716 ± 0.171\n",
      "F1 Weighted: 0.719 ± 0.169\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:31:47,810] A new study created in memory with name: no-name-ae444f85-bbfc-4831-9104-d1a3f570750d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(15.8077)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6794, Train Acc: 0.5500, Train F1 Macro: 0.3548, Train F1 Weighted: 0.3903\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6713, Train Acc: 0.5000, Train F1 Macro: 0.4048, Train F1 Weighted: 0.4286\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4398, Train Acc: 0.7500, Train F1 Macro: 0.7494, Train F1 Weighted: 0.7506\n",
      "Val Acc: 0.3333, Val F1 Macro: 0.2500, Val F1 Weighted: 0.2500, Val Geometric Mean: 0.2752\n",
      "Test Acc: 0.3333, Test F1 Macro: 0.2500, Test F1 Weighted: 0.2500\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1173, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6667, Val F1 Macro: 0.6667, Val F1 Weighted: 0.6667, Val Geometric Mean: 0.6667\n",
      "Test Acc: 0.6667, Test F1 Macro: 0.6667, Test F1 Weighted: 0.6667\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0743, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.3333, Val F1 Macro: 0.2500, Val F1 Weighted: 0.2500, Val Geometric Mean: 0.2752\n",
      "Test Acc: 0.3333, Test F1 Macro: 0.2500, Test F1 Weighted: 0.2500\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0477, Train Acc: 0.9500, Train F1 Macro: 0.9499, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.3333, Val F1 Macro: 0.2500, Val F1 Weighted: 0.2500, Val Geometric Mean: 0.2752\n",
      "Test Acc: 0.3333, Test F1 Macro: 0.2500, Test F1 Weighted: 0.2500\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.2306, Train Acc: 0.9500, Train F1 Macro: 0.9488, Train F1 Weighted: 0.9496\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.4857, Val F1 Weighted: 0.4857, Val Geometric Mean: 0.4904\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.4857, Test F1 Weighted: 0.4857\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(3) tensor(14.9615)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5936, Train Acc: 0.5714, Train F1 Macro: 0.3636, Train F1 Weighted: 0.4156\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.2857, Val F1 Weighted: 0.2286, Val Geometric Mean: 0.2967\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.2857, Test F1 Weighted: 0.2286\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0924, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0386, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0243, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5667, Val Geometric Mean: 0.5832\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5667\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0594, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0176, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0107, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(2) tensor(16.2308)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6649, Train Acc: 0.6190, Train F1 Macro: 0.6182, Train F1 Weighted: 0.6173\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5667, Val Geometric Mean: 0.5832\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5667\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4519, Train Acc: 0.7619, Train F1 Macro: 0.7407, Train F1 Weighted: 0.7443\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2157, Train Acc: 0.8095, Train F1 Macro: 0.7981, Train F1 Weighted: 0.8004\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1944, Train Acc: 0.8095, Train F1 Macro: 0.7981, Train F1 Weighted: 0.8004\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0300, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5667, Val Geometric Mean: 0.5832\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5667\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0035, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0042, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(1) tensor(16.2692)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6948, Train Acc: 0.5238, Train F1 Macro: 0.3438, Train F1 Weighted: 0.3601\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6631, Train Acc: 0.6667, Train F1 Macro: 0.6101, Train F1 Weighted: 0.6172\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.3732, Train Acc: 0.9048, Train F1 Macro: 0.9045, Train F1 Weighted: 0.9048\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2282, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1596, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1333, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1056, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(2) tensor(15.9231)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6181, Train Acc: 0.7143, Train F1 Macro: 0.6786, Train F1 Weighted: 0.6837\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0858, Train Acc: 0.9048, Train F1 Macro: 0.9028, Train F1 Weighted: 0.9034\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0275, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0404, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0041, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0091, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:32:14,985] Trial 0 finished with value: 0.47601153061224494 and parameters: {}. Best is trial 0 with value: 0.47601153061224494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0012, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "New best score: 0.476\n",
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.127\n",
      "F1 Macro: 0.761 ± 0.207\n",
      "F1 Weighted: 0.776 ± 0.179\n",
      "Best model performance:\n",
      "Accuracy: 0.807 ± 0.127\n",
      "F1 Macro: 0.761 ± 0.207\n",
      "F1 Weighted: 0.776 ± 0.179\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": True,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
