{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_circrna_te.csv'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:50,513] A new study created in memory with name: no-name-874da8ca-1795-4656-a64d-fc031d195920\n",
      "[I 2024-11-17 21:05:50,599] Trial 0 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.27794757275052406.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.278\n",
      "Best model performance:\n",
      "Accuracy: 0.825 ± 0.030\n",
      "F1 Macro: 0.452 ± 0.009\n",
      "F1 Weighted: 0.746 ± 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:50,663] Trial 1 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 6}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:50,727] Trial 2 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:50,788] Trial 3 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.6528081564994511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.653\n",
      "Best model performance:\n",
      "Accuracy: 0.907 ± 0.053\n",
      "F1 Macro: 0.806 ± 0.106\n",
      "F1 Weighted: 0.893 ± 0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:50,852] Trial 4 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:50,913] Trial 5 finished with value: 0.46772551930590295 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:50,976] Trial 6 finished with value: 0.46772551930590295 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,045] Trial 7 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,112] Trial 8 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 14}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,177] Trial 9 finished with value: 0.4419268561846254 and parameters: {'n_neighbors': 1}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,241] Trial 10 finished with value: 0.22915102904497914 and parameters: {'n_neighbors': 2}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,306] Trial 11 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 6}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,370] Trial 12 finished with value: 0.5057181971981872 and parameters: {'n_neighbors': 4}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,437] Trial 13 finished with value: 0.4477242436013849 and parameters: {'n_neighbors': 9}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,501] Trial 14 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,565] Trial 15 finished with value: 0.5085070524273005 and parameters: {'n_neighbors': 10}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,647] Trial 16 finished with value: 0.6139268814359082 and parameters: {'n_neighbors': 3}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,712] Trial 17 finished with value: 0.5296409475158396 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,779] Trial 18 finished with value: 0.3379123226778965 and parameters: {'n_neighbors': 16}. Best is trial 1 with value: 0.6528081564994511.\n",
      "[I 2024-11-17 21:05:51,844] Trial 19 finished with value: 0.5057181971981872 and parameters: {'n_neighbors': 4}. Best is trial 1 with value: 0.6528081564994511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_disease/te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:51,874] A new study created in memory with name: no-name-3649c405-4775-425f-921e-c96a64d938fe\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:51,968] Trial 0 finished with value: 0.7389770960193865 and parameters: {'C': 2.175199734600259, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,027] Trial 1 finished with value: 0.5614209168802691 and parameters: {'C': 0.017940740966147034, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.739\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:52,099] Trial 2 finished with value: 0.7389770960193865 and parameters: {'C': 0.190355706863225, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,173] Trial 3 finished with value: 0.5329721476347162 and parameters: {'C': 0.010173158170458907, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,243] Trial 4 finished with value: 0.7389770960193865 and parameters: {'C': 0.37137764156062775, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,331] Trial 5 finished with value: 0.7039337563024542 and parameters: {'C': 1.0018313664643688, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,411] Trial 6 finished with value: 0.6354207758053912 and parameters: {'C': 0.20756595151118823, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,496] Trial 7 finished with value: 0.7039337563024542 and parameters: {'C': 0.9896927914865387, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,554] Trial 8 finished with value: 0.4477242436013849 and parameters: {'C': 0.03517248076809698, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,645] Trial 9 finished with value: 0.7039337563024542 and parameters: {'C': 0.7225679452733509, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:52,740] Trial 10 finished with value: 0.7389770960193865 and parameters: {'C': 8.043784363916453, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:52,834] Trial 11 finished with value: 0.7389770960193865 and parameters: {'C': 3.730906438783207, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,901] Trial 12 finished with value: 0.7389770960193865 and parameters: {'C': 0.14000846648802082, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:52,962] Trial 13 finished with value: 0.553150014770979 and parameters: {'C': 0.06383238044483854, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:53,057] Trial 14 finished with value: 0.7389770960193865 and parameters: {'C': 2.1553983439432334, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,119] Trial 15 finished with value: 0.7389770960193865 and parameters: {'C': 0.10458038195744745, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,220] Trial 16 finished with value: 0.6682316917536709 and parameters: {'C': 0.542626241858557, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:53,311] Trial 17 finished with value: 0.7389770960193865 and parameters: {'C': 2.1248460070259028, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:53,404] Trial 18 finished with value: 0.7389770960193865 and parameters: {'C': 6.578586198065251, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,489] Trial 19 finished with value: 0.6682316917536709 and parameters: {'C': 0.29456475688440376, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:53,582] Trial 20 finished with value: 0.7389770960193865 and parameters: {'C': 1.8194328058244809, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,658] Trial 21 finished with value: 0.7389770960193865 and parameters: {'C': 0.3451608642197551, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,719] Trial 22 finished with value: 0.553150014770979 and parameters: {'C': 0.061625291828078535, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,798] Trial 23 finished with value: 0.7389770960193865 and parameters: {'C': 0.42931686159665733, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:53,867] Trial 24 finished with value: 0.7389770960193865 and parameters: {'C': 0.16274106242694047, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:53,963] Trial 25 finished with value: 0.7389770960193865 and parameters: {'C': 4.072653303272963, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,056] Trial 26 finished with value: 0.7389770960193865 and parameters: {'C': 1.342523758376522, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,118] Trial 27 finished with value: 0.6024938699160863 and parameters: {'C': 0.07956467316459452, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,203] Trial 28 finished with value: 0.6354207758053912 and parameters: {'C': 0.2169375920932685, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,268] Trial 29 finished with value: 0.6069596596049786 and parameters: {'C': 0.04202784212407549, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,349] Trial 30 finished with value: 0.3908956158579512 and parameters: {'C': 0.02376939203559689, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,448] Trial 31 finished with value: 0.7389770960193865 and parameters: {'C': 8.869184338832524, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,544] Trial 32 finished with value: 0.7389770960193865 and parameters: {'C': 4.371404270551114, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,641] Trial 33 finished with value: 0.7389770960193865 and parameters: {'C': 9.65512203279863, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,697] Trial 34 finished with value: 0.27794757275052406 and parameters: {'C': 0.01206680342309124, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,797] Trial 35 finished with value: 0.7389770960193865 and parameters: {'C': 3.0210392141171276, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:54,897] Trial 36 finished with value: 0.7389770960193865 and parameters: {'C': 5.879361717498502, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:54,981] Trial 37 finished with value: 0.7389770960193865 and parameters: {'C': 0.8229607052765023, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,080] Trial 38 finished with value: 0.7389770960193865 and parameters: {'C': 1.4514301156430576, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:55,163] Trial 39 finished with value: 0.7389770960193865 and parameters: {'C': 0.512768047995054, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:55,247] Trial 40 finished with value: 0.7389770960193865 and parameters: {'C': 0.24565162911804053, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,352] Trial 41 finished with value: 0.7389770960193865 and parameters: {'C': 3.0003408376250027, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,448] Trial 42 finished with value: 0.7389770960193865 and parameters: {'C': 5.619328318873163, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,541] Trial 43 finished with value: 0.7389770960193865 and parameters: {'C': 3.321892545432015, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:55,607] Trial 44 finished with value: 0.7389770960193865 and parameters: {'C': 0.12976520499047042, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,703] Trial 45 finished with value: 0.7389770960193865 and parameters: {'C': 6.611642304277486, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:55,794] Trial 46 finished with value: 0.7389770960193865 and parameters: {'C': 1.099116917563901, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,891] Trial 47 finished with value: 0.7389770960193865 and parameters: {'C': 2.38905972653115, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:55,994] Trial 48 finished with value: 0.7039337563024542 and parameters: {'C': 0.6858247921789073, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-17 21:05:56,093] Trial 49 finished with value: 0.7389770960193865 and parameters: {'C': 4.573624499715188, 'class_weight': None}. Best is trial 0 with value: 0.7389770960193865.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:56,125] A new study created in memory with name: no-name-d099f064-6173-4ef4-a5bf-4f86ad7245ed\n",
      "[I 2024-11-17 21:05:56,196] Trial 0 finished with value: 0.7389770960193865 and parameters: {'lambda': 7.608655845700436e-05, 'alpha': 0.0191210546499596}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:56,270] Trial 1 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.01119519855589457, 'alpha': 1.9342855459957053e-06}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:56,343] Trial 2 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.0018460422588911568, 'alpha': 1.1667913504568424e-05}. Best is trial 0 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.739\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:56,431] Trial 3 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.01831112180093915, 'alpha': 3.4934732637298096e-08}. Best is trial 0 with value: 0.7389770960193865.\n",
      "[I 2024-11-17 21:05:56,510] Trial 4 finished with value: 0.7581635155117249 and parameters: {'lambda': 7.155342168382797e-06, 'alpha': 0.00011441997432654826}. Best is trial 4 with value: 0.7581635155117249.\n",
      "[I 2024-11-17 21:05:56,578] Trial 5 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.2698271656565215e-08, 'alpha': 4.713328847699792e-05}. Best is trial 5 with value: 0.7948768203234714.\n",
      "[I 2024-11-17 21:05:56,645] Trial 6 finished with value: 0.27794757275052406 and parameters: {'lambda': 2.8174953641681496e-07, 'alpha': 0.3576092545055356}. Best is trial 5 with value: 0.7948768203234714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "New best score: 0.795\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.894 ± 0.106\n",
      "F1 Weighted: 0.939 ± 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:56,715] Trial 7 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.004304511530976998, 'alpha': 0.024498026054953062}. Best is trial 5 with value: 0.7948768203234714.\n",
      "[I 2024-11-17 21:05:56,784] Trial 8 finished with value: 0.3908956158579512 and parameters: {'lambda': 0.003326260439943442, 'alpha': 0.08275031292710416}. Best is trial 5 with value: 0.7948768203234714.\n",
      "[I 2024-11-17 21:05:56,855] Trial 9 finished with value: 0.8011193651582444 and parameters: {'lambda': 4.018664776877043e-07, 'alpha': 2.8629988909277157e-06}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:56,929] Trial 10 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.5390845083711636e-06, 'alpha': 1.5047986188478118e-08}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,008] Trial 11 finished with value: 0.7389770960193865 and parameters: {'lambda': 1.0202475155720426e-08, 'alpha': 0.0003258510791609648}. Best is trial 9 with value: 0.8011193651582444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.801\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.900 ± 0.106\n",
      "F1 Weighted: 0.941 ± 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:57,081] Trial 12 finished with value: 0.7581635155117249 and parameters: {'lambda': 1.1638802462134763e-08, 'alpha': 1.0789669151809172e-06}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,154] Trial 13 finished with value: 0.7021274450808679 and parameters: {'lambda': 1.7127788954939889e-07, 'alpha': 0.0011876355243646036}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,230] Trial 14 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.2747083682342178e-07, 'alpha': 1.0192153993425504e-05}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,310] Trial 15 finished with value: 0.5239116671839724 and parameters: {'lambda': 0.777806727228329, 'alpha': 2.0375054016490454e-07}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,390] Trial 16 finished with value: 0.7389770960193865 and parameters: {'lambda': 1.881372941266466e-05, 'alpha': 0.0017416820267313622}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,464] Trial 17 finished with value: 0.7581635155117249 and parameters: {'lambda': 8.849176506431903e-07, 'alpha': 2.0925685542108094e-05}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,540] Trial 18 finished with value: 0.8011193651582444 and parameters: {'lambda': 4.0811575000261146e-08, 'alpha': 5.587819725035293e-07}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,615] Trial 19 finished with value: 0.7948768203234714 and parameters: {'lambda': 3.3608131751801242e-06, 'alpha': 2.3679135341933415e-07}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,710] Trial 20 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.00024232809525260115, 'alpha': 2.0775174279345546e-06}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,786] Trial 21 finished with value: 0.7948768203234714 and parameters: {'lambda': 4.663751709122295e-08, 'alpha': 2.950536276731288e-07}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,860] Trial 22 finished with value: 0.7948768203234714 and parameters: {'lambda': 4.355505012816763e-08, 'alpha': 4.3694894427791765e-05}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:57,935] Trial 23 finished with value: 0.7948768203234714 and parameters: {'lambda': 6.009022854004601e-07, 'alpha': 3.777521401499641e-06}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,012] Trial 24 finished with value: 0.7389770960193865 and parameters: {'lambda': 3.271352163809097e-08, 'alpha': 0.0001494288362760527}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,087] Trial 25 finished with value: 0.7581635155117249 and parameters: {'lambda': 1.2002424260042377e-07, 'alpha': 5.717115682616603e-07}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,161] Trial 26 finished with value: 0.7948768203234714 and parameters: {'lambda': 3.162791784329316e-05, 'alpha': 8.769097658793701e-08}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,241] Trial 27 finished with value: 0.7948768203234714 and parameters: {'lambda': 3.4022155222863386e-08, 'alpha': 5.144300242808122e-06}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,324] Trial 28 finished with value: 0.7389770960193865 and parameters: {'lambda': 4.2529574217689726e-07, 'alpha': 0.0006570634510033021}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,396] Trial 29 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.00017844494520995278, 'alpha': 0.008669401912159742}. Best is trial 9 with value: 0.8011193651582444.\n",
      "[I 2024-11-17 21:05:58,471] Trial 30 finished with value: 0.8403990328047336 and parameters: {'lambda': 7.939683230746169e-06, 'alpha': 4.4985329459730504e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:58,545] Trial 31 finished with value: 0.7581635155117249 and parameters: {'lambda': 3.422782320961733e-06, 'alpha': 4.258093791224034e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:58,621] Trial 32 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.5851478790199453e-06, 'alpha': 2.4344416798270518e-05}. Best is trial 30 with value: 0.8403990328047336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.840\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.918 ± 0.113\n",
      "F1 Weighted: 0.953 ± 0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:05:58,700] Trial 33 finished with value: 0.8011193651582444 and parameters: {'lambda': 1.0274434301778616e-05, 'alpha': 5.9039717014673634e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:58,805] Trial 34 finished with value: 0.7948768203234714 and parameters: {'lambda': 5.4579666059930605e-05, 'alpha': 1.1177628817652933e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:58,878] Trial 35 finished with value: 0.8403990328047336 and parameters: {'lambda': 6.633884268833129e-06, 'alpha': 5.963075822354323e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:58,952] Trial 36 finished with value: 0.7948768203234714 and parameters: {'lambda': 0.0005809140957209196, 'alpha': 6.732344835858891e-08}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,031] Trial 37 finished with value: 0.7948768203234714 and parameters: {'lambda': 2.982735105384673e-06, 'alpha': 2.711817838521352e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,105] Trial 38 finished with value: 0.7948768203234714 and parameters: {'lambda': 8.868178883559004e-06, 'alpha': 1.128284603772137e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,186] Trial 39 finished with value: 0.7948768203234714 and parameters: {'lambda': 8.009775631100108e-05, 'alpha': 0.00013588550106872452}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,262] Trial 40 finished with value: 0.7581635155117249 and parameters: {'lambda': 2.9357459166327733e-07, 'alpha': 6.878862271447717e-07}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,338] Trial 41 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.1787364908872053e-05, 'alpha': 6.8953048826314465e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,413] Trial 42 finished with value: 0.8403990328047336 and parameters: {'lambda': 1.763428985592828e-05, 'alpha': 2.4663150630663723e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,486] Trial 43 finished with value: 0.8011193651582444 and parameters: {'lambda': 4.2213909676646785e-06, 'alpha': 2.024705998705641e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,562] Trial 44 finished with value: 0.8403990328047336 and parameters: {'lambda': 1.1821625023073154e-06, 'alpha': 1.1430007867304083e-08}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,643] Trial 45 finished with value: 0.7581635155117249 and parameters: {'lambda': 1.1093118988093333e-06, 'alpha': 4.261182601935234e-08}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,718] Trial 46 finished with value: 0.7948768203234714 and parameters: {'lambda': 3.156733285053447e-05, 'alpha': 1.0207503777678905e-08}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,815] Trial 47 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.0004810659478942082, 'alpha': 8.595247995243057e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,890] Trial 48 finished with value: 0.7389770960193865 and parameters: {'lambda': 2.464627576241632e-05, 'alpha': 0.00035412274375472866}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:05:59,968] Trial 49 finished with value: 0.7021274450808679 and parameters: {'lambda': 2.2823265763028537e-06, 'alpha': 0.006534824651426124}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,048] Trial 50 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.09680743762650719, 'alpha': 1.152884127777122e-07}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,124] Trial 51 finished with value: 0.8403990328047336 and parameters: {'lambda': 1.1294584192107417e-07, 'alpha': 1.6279188536171107e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,201] Trial 52 finished with value: 0.7581635155117249 and parameters: {'lambda': 1.637237214004715e-07, 'alpha': 2.152308641056257e-08}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,273] Trial 53 finished with value: 0.7948768203234714 and parameters: {'lambda': 6.588301942725496e-07, 'alpha': 1.3478492074623086e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,348] Trial 54 finished with value: 0.8011193651582444 and parameters: {'lambda': 5.3708771869489206e-06, 'alpha': 2.246237540474659e-06}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,424] Trial 55 finished with value: 0.8403990328047336 and parameters: {'lambda': 2.8004108966247505e-07, 'alpha': 1.8921311157269727e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,502] Trial 56 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.2816196124702612e-06, 'alpha': 1.6559045892053834e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,581] Trial 57 finished with value: 0.7948768203234714 and parameters: {'lambda': 1.3581479657600181e-05, 'alpha': 6.224451871139237e-05}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,654] Trial 58 finished with value: 0.27794757275052406 and parameters: {'lambda': 2.8349579420694307e-07, 'alpha': 0.26645364315074477}. Best is trial 30 with value: 0.8403990328047336.\n",
      "[I 2024-11-17 21:06:00,732] Trial 59 finished with value: 0.7389770960193865 and parameters: {'lambda': 6.325652862955487e-08, 'alpha': 0.00021946603901659806}. Best is trial 30 with value: 0.8403990328047336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.918 ± 0.113\n",
      "F1 Weighted: 0.953 ± 0.064\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:06:00,763] A new study created in memory with name: no-name-4058343b-a46b-4637-b1d3-c0065d3be176\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:02,163] Trial 0 finished with value: 0.32661842676062103 and parameters: {'lr': 0.0009787300813213033, 'dropout': 0.42386587263036213}. Best is trial 0 with value: 0.32661842676062103.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.327\n",
      "Best model performance:\n",
      "Accuracy: 0.838 ± 0.031\n",
      "F1 Macro: 0.505 ± 0.103\n",
      "F1 Weighted: 0.771 ± 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:03,519] Trial 1 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0001858529245660219, 'dropout': 0.12393723953593991}. Best is trial 0 with value: 0.32661842676062103.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:04,881] Trial 2 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00014615499793584974, 'dropout': 0.44884608656495706}. Best is trial 0 with value: 0.32661842676062103.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:06,288] Trial 3 finished with value: 0.7039337563024542 and parameters: {'lr': 0.002579583955973857, 'dropout': 0.47167302280825596}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:07,636] Trial 4 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0006324751349222266, 'dropout': 0.4452131064436292}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:09,035] Trial 5 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0002857666393124804, 'dropout': 0.30823809351249953}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:10,375] Trial 6 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0027877128841443375, 'dropout': 0.4457839545701857}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:11,740] Trial 7 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0024717406707368164, 'dropout': 0.24047207637354975}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:13,070] Trial 8 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00039520906961137563, 'dropout': 0.12141582979822466}. Best is trial 3 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-17 21:06:14,476] Trial 9 finished with value: 0.5198770460513231 and parameters: {'lr': 0.0011098120500341903, 'dropout': 0.20051635906241835}. Best is trial 3 with value: 0.7039337563024542.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "Best hyperparameters:\n",
      "{'lr': 0.002579583955973857, 'dropout': 0.47167302280825596}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:35:01,979] A new study created in memory with name: no-name-c45dede4-3987-4583-8217-5ebdaed1759e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:35:12,454] Trial 0 finished with value: 0.8601248426666666 and parameters: {}. Best is trial 0 with value: 0.8601248426666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.860\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.934 ± 0.085\n",
      "F1 Weighted: 0.959 ± 0.054\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.934 ± 0.085\n",
      "F1 Weighted: 0.959 ± 0.054\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "# omic_data_loaders = {\n",
    "#     \"mrna\": mrna_loader,\n",
    "#     # \"mirna\": mirna_loader,\n",
    "#     # \"circrna\": circrna_loader,\n",
    "#     # # \"pirna\": pirna_loader,\n",
    "#     # \"te\": te_loader,\n",
    "# }\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "\n",
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:42:15,921] A new study created in memory with name: no-name-c968c791-c334-4a18-8f15-8e7f34cbb609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4453, Train Acc: 0.7966, Train F1 Macro: 0.4434, Train F1 Weighted: 0.7365\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1416, Train Acc: 0.9322, Train F1 Macro: 0.8554, Train F1 Weighted: 0.9251\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1164, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3506, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9481\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1771, Train Acc: 0.8644, Train F1 Macro: 0.6289, Train F1 Weighted: 0.8243\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0599, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0635, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(8) tensor(15.4730)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2372, Train Acc: 0.8814, Train F1 Macro: 0.6974, Train F1 Weighted: 0.8534\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.4231, Val F1 Weighted: 0.6769, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.4231, Test F1 Weighted: 0.6769\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1481, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.6667, Val F1 Macro: 0.4000, Val F1 Weighted: 0.6400, Val Geometric Mean: 0.5547\n",
      "Test Acc: 0.6667, Test F1 Macro: 0.4000, Test F1 Weighted: 0.6400\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0976, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3747, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.2426, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0918, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.4231, Val F1 Weighted: 0.6769, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.4231, Test F1 Weighted: 0.6769\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0778, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.4054)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(9) tensor(16.4189)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3870, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1682, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1629, Train Acc: 0.9153, Train F1 Macro: 0.8552, Train F1 Weighted: 0.9168\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.6400, Val F1 Weighted: 0.7840, Val Geometric Mean: 0.7377\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.6400, Test F1 Weighted: 0.7840\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1606, Train Acc: 0.9322, Train F1 Macro: 0.8954, Train F1 Weighted: 0.9364\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1029, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1752, Train Acc: 0.8983, Train F1 Macro: 0.8033, Train F1 Weighted: 0.8937\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8409\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1321, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9517\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.6400, Val F1 Weighted: 0.7840, Val Geometric Mean: 0.7377\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.6400, Test F1 Weighted: 0.7840\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.8108)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(9) tensor(16.3378)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(16.0270)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3438, Train Acc: 0.7797, Train F1 Macro: 0.7266, Train F1 Weighted: 0.8021\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2383, Train Acc: 0.8983, Train F1 Macro: 0.8431, Train F1 Weighted: 0.9015\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1815, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9482\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1637, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1224, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9661\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1662, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8115\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1288, Train Acc: 0.9661, Train F1 Macro: 0.9477, Train F1 Weighted: 0.9672\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7432)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.7297)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(15.5135)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3970, Train Acc: 0.8500, Train F1 Macro: 0.7403, Train F1 Weighted: 0.8472\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1613, Train Acc: 0.9167, Train F1 Macro: 0.8287, Train F1 Weighted: 0.9064\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1240, Train Acc: 0.9833, Train F1 Macro: 0.9711, Train F1 Weighted: 0.9830\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.7083, Val F1 Weighted: 0.8571, Val Geometric Mean: 0.8044\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.7083, Test F1 Weighted: 0.8571\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0549, Train Acc: 0.9833, Train F1 Macro: 0.9711, Train F1 Weighted: 0.9830\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.2490, Train Acc: 0.8333, Train F1 Macro: 0.7869, Train F1 Weighted: 0.8499\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1261, Train Acc: 0.9667, Train F1 Macro: 0.9443, Train F1 Weighted: 0.9667\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.4400, Val F1 Weighted: 0.7543, Val Geometric Mean: 0.6389\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.4400, Test F1 Weighted: 0.7543\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 21:45:09,700] Trial 0 finished with value: 0.8403990328047336 and parameters: {}. Best is trial 0 with value: 0.8403990328047336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1649, Train Acc: 0.9333, Train F1 Macro: 0.8958, Train F1 Weighted: 0.9354\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.5758, Val F1 Weighted: 0.7489, Val Geometric Mean: 0.6753\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.5758, Test F1 Weighted: 0.7489\n",
      "##################################################\n",
      "New best score: 0.840\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.918 ± 0.113\n",
      "F1 Weighted: 0.953 ± 0.064\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.918 ± 0.113\n",
      "F1 Weighted: 0.953 ± 0.064\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": True,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "# birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrna, mirna, circrna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, circrna, 2L no interactions\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.918 ± 0.113\n",
    "F1 Weighted: 0.953 ± 0.064\n",
    "---\n",
    "Accuracy: 0.946 ± 0.027\n",
    "F1 Macro: 0.904 ± 0.048\n",
    "F1 Weighted: 0.944 ± 0.028\n",
    "# mrna, mirna, circrna 3L, interactions, degree ~20 in diff exp graphs, larger degree shows degraded performance\n",
    "# making the avg degree to high shows large jumps on the validation set during training\n",
    "Accuracy: 0.945 ± 0.053\n",
    "F1 Macro: 0.910 ± 0.081\n",
    "F1 Weighted: 0.946 ± 0.048\n",
    "# mrna, mirna, circrna 2L, interactions, 64 cap\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053\n",
    "# mrna, mirna, circrna 3L, interactions, 64 cap\n",
    "Accuracy: 0.891 ± 0.054\n",
    "F1 Macro: 0.801 ± 0.088\n",
    "F1 Weighted: 0.888 ± 0.056\n",
    "# mrna, mirna, circrna 3L\n",
    "Accuracy: 0.920 ± 0.050\n",
    "F1 Macro: 0.829 ± 0.112\n",
    "F1 Weighted: 0.907 ± 0.062\n",
    "# mrna, mirna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, 3L\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.897 ± 0.089\n",
    "F1 Weighted: 0.944 ± 0.051"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
