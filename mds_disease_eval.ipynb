{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.benchmarks.data_manager import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n"
     ]
    }
   ],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  1\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  2\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  3\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 48    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 13    │\n",
      "└───────┴───────┘\n",
      "fold:  4\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "for fold_idx in range(5):\n",
    "    train_df, test_df = mrna_loader.get_fold(fold_idx)\n",
    "\n",
    "    print(\"fold: \", fold_idx)\n",
    "    print(train_df[\"class\"].value_counts(), test_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    # kjj\"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.feature_dim, odm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,063] A new study created in memory with name: no-name-487dc805-fc43-418d-8a2d-a156c69f7612\n",
      "[I 2024-11-10 19:15:05,137] Trial 0 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,204] Trial 1 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,288] Trial 2 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,353] Trial 3 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,421] Trial 4 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,489] Trial 5 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,552] Trial 6 finished with value: 0.58024233114353 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,615] Trial 7 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,682] Trial 8 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,747] Trial 9 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,818] Trial 10 finished with value: 0.6723834634892419 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,886] Trial 11 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,955] Trial 12 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,042] Trial 13 finished with value: 0.5846632831891617 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,113] Trial 14 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,182] Trial 15 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,251] Trial 16 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,322] Trial 17 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,392] Trial 18 finished with value: 0.5888211266094818 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,458] Trial 19 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'n_neighbors': 16}\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,374] A new study created in memory with name: no-name-17b65a7d-695e-4e4c-9283-d9a803e51944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,465] Trial 0 finished with value: 0.6497992012967035 and parameters: {'C': 0.5912776084859577, 'class_weight': None, 'rfe_step': 0.18135798798684793, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,574] Trial 1 finished with value: 0.6497992012967035 and parameters: {'C': 1.92761937700467, 'class_weight': 'balanced', 'rfe_step': 0.12740681956658, 'rfe_n_features': 111}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,656] Trial 2 finished with value: 0.6477728936997709 and parameters: {'C': 0.04431683340011391, 'class_weight': 'balanced', 'rfe_step': 0.0561795144594292, 'rfe_n_features': 170}. Best is trial 0 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,742] Trial 3 finished with value: 0.5947745825537253 and parameters: {'C': 6.239037597620005, 'class_weight': None, 'rfe_step': 0.16137130043179504, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,804] Trial 4 finished with value: 0.6497992012967035 and parameters: {'C': 1.8024991990621209, 'class_weight': None, 'rfe_step': 0.16636180097742892, 'rfe_n_features': 200}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,914] Trial 5 finished with value: 0.5947745825537253 and parameters: {'C': 5.247154408538435, 'class_weight': None, 'rfe_step': 0.06264027199095756, 'rfe_n_features': 156}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,008] Trial 6 finished with value: 0.5947745825537253 and parameters: {'C': 9.321484902226024, 'class_weight': None, 'rfe_step': 0.16378265009504533, 'rfe_n_features': 154}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,119] Trial 7 finished with value: 0.5947745825537253 and parameters: {'C': 8.202967718129962, 'class_weight': None, 'rfe_step': 0.05914431146688243, 'rfe_n_features': 158}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,184] Trial 8 finished with value: 0.6477728936997709 and parameters: {'C': 0.04494677478861271, 'class_weight': 'balanced', 'rfe_step': 0.16643442620663507, 'rfe_n_features': 172}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,248] Trial 9 finished with value: 0.6477728936997709 and parameters: {'C': 0.019873129915673807, 'class_weight': 'balanced', 'rfe_step': 0.13255506502150974, 'rfe_n_features': 180}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,334] Trial 10 finished with value: 0.6288118975723532 and parameters: {'C': 0.2890553099749232, 'class_weight': None, 'rfe_step': 0.19984885818456594, 'rfe_n_features': 126}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,448] Trial 11 finished with value: 0.6497992012967035 and parameters: {'C': 0.7846558478244101, 'class_weight': 'balanced', 'rfe_step': 0.1008311361438369, 'rfe_n_features': 101}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,550] Trial 12 finished with value: 0.6787024344579722 and parameters: {'C': 0.25258158703203787, 'class_weight': 'balanced', 'rfe_step': 0.10914476888380255, 'rfe_n_features': 126}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,656] Trial 13 finished with value: 0.6787024344579722 and parameters: {'C': 0.21138530733394667, 'class_weight': 'balanced', 'rfe_step': 0.08776062841727579, 'rfe_n_features': 131}. Best is trial 12 with value: 0.6787024344579722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.679\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.905 ± 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:58,755] Trial 14 finished with value: 0.6787024344579722 and parameters: {'C': 0.15592096415861068, 'class_weight': 'balanced', 'rfe_step': 0.0936740719697733, 'rfe_n_features': 129}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,870] Trial 15 finished with value: 0.7331794285989254 and parameters: {'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:58,960] Trial 16 finished with value: 0.6809510924096367 and parameters: {'C': 0.050080573100068776, 'class_weight': 'balanced', 'rfe_step': 0.1057608612630386, 'rfe_n_features': 120}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,068] Trial 17 finished with value: 0.6422617001076846 and parameters: {'C': 0.07668997260540834, 'class_weight': 'balanced', 'rfe_step': 0.0832177373698388, 'rfe_n_features': 114}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.733\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:59,147] Trial 18 finished with value: 0.6477728936997709 and parameters: {'C': 0.010268450364919926, 'class_weight': 'balanced', 'rfe_step': 0.11357879132217108, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,238] Trial 19 finished with value: 0.6809510924096367 and parameters: {'C': 0.0644825018841236, 'class_weight': 'balanced', 'rfe_step': 0.14146501839591458, 'rfe_n_features': 116}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,340] Trial 20 finished with value: 0.728115464597245 and parameters: {'C': 0.10675039358903013, 'class_weight': 'balanced', 'rfe_step': 0.07651257359386716, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,445] Trial 21 finished with value: 0.728115464597245 and parameters: {'C': 0.1155640352564498, 'class_weight': 'balanced', 'rfe_step': 0.07635708884773021, 'rfe_n_features': 138}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,554] Trial 22 finished with value: 0.728115464597245 and parameters: {'C': 0.13297292036765632, 'class_weight': 'balanced', 'rfe_step': 0.0736767585819143, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,655] Trial 23 finished with value: 0.7331794285989254 and parameters: {'C': 0.0839042307438939, 'class_weight': 'balanced', 'rfe_step': 0.07312489903466748, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,748] Trial 24 finished with value: 0.6477728936997709 and parameters: {'C': 0.022068952997355576, 'class_weight': 'balanced', 'rfe_step': 0.06952300589961795, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,855] Trial 25 finished with value: 0.6497992012967035 and parameters: {'C': 0.4403890185742033, 'class_weight': 'balanced', 'rfe_step': 0.0517673718488775, 'rfe_n_features': 164}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,941] Trial 26 finished with value: 0.6477728936997709 and parameters: {'C': 0.023322190329971796, 'class_weight': 'balanced', 'rfe_step': 0.09350830825927489, 'rfe_n_features': 136}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,038] Trial 27 finished with value: 0.6926488504188454 and parameters: {'C': 0.09203815468971016, 'class_weight': 'balanced', 'rfe_step': 0.07812215855799436, 'rfe_n_features': 147}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,130] Trial 28 finished with value: 0.6809510924096367 and parameters: {'C': 0.03393875406287643, 'class_weight': 'balanced', 'rfe_step': 0.11620427238691917, 'rfe_n_features': 103}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,264] Trial 29 finished with value: 0.6497992012967035 and parameters: {'C': 0.5654000035290196, 'class_weight': 'balanced', 'rfe_step': 0.06790668927665994, 'rfe_n_features': 146}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.938 ± 0.058\n",
    "F1 Macro: 0.684 ± 0.258\n",
    "F1 Weighted: 0.924 ± 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:41,344] A new study created in memory with name: no-name-d74618ba-7c3c-4ca8-8ef7-5eab635f3ab8\n",
      "[I 2024-11-10 19:18:41,468] Trial 0 finished with value: 0.5046011800339922 and parameters: {'booster': 'gbtree', 'lambda': 1.090093549619324e-05, 'alpha': 0.0017384853973629977, 'max_depth': 6, 'eta': 0.024990822730382434, 'gamma': 0.0007448711414900602, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5046011800339922.\n",
      "[I 2024-11-10 19:18:41,557] Trial 1 finished with value: 0.0013903478254046716 and parameters: {'booster': 'dart', 'lambda': 7.953733877278384e-07, 'alpha': 0.012311349329884858, 'max_depth': 1, 'eta': 1.3106729678135571e-08, 'gamma': 1.8912671827677967e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.915196931299182e-06, 'skip_drop': 0.18697866330618118}. Best is trial 0 with value: 0.5046011800339922.\n",
      "[I 2024-11-10 19:18:41,664] Trial 2 finished with value: 0.5947745825537253 and parameters: {'booster': 'gbtree', 'lambda': 6.058943782920171e-06, 'alpha': 1.031488556994315e-06, 'max_depth': 7, 'eta': 7.307520855607819e-07, 'gamma': 0.00016232868881341684, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.5947745825537253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.505\n",
      "Best model performance:\n",
      "Accuracy: 0.839 ± 0.079\n",
      "F1 Macro: 0.716 ± 0.161\n",
      "F1 Weighted: 0.839 ± 0.077\n",
      "New best score: 0.595\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.077\n",
      "F1 Macro: 0.771 ± 0.177\n",
      "F1 Weighted: 0.878 ± 0.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:41,755] Trial 3 finished with value: 0.4773113900356288 and parameters: {'booster': 'dart', 'lambda': 0.12408402814102056, 'alpha': 9.110488005138047e-07, 'max_depth': 2, 'eta': 0.002647083756160064, 'gamma': 0.0003482942304648263, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.7436722774157082, 'skip_drop': 0.43481543073977125}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,833] Trial 4 finished with value: 0.553150014770979 and parameters: {'booster': 'gbtree', 'lambda': 0.2792310057749791, 'alpha': 0.8027307283253481, 'max_depth': 1, 'eta': 0.061263458984386496, 'gamma': 0.0005126645470834501, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,926] Trial 5 finished with value: 0.5201471744784413 and parameters: {'booster': 'dart', 'lambda': 0.0014880441853571502, 'alpha': 3.0784878218976924e-08, 'max_depth': 2, 'eta': 0.5338883219491826, 'gamma': 6.866987234602545e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.009400221878030034, 'skip_drop': 0.8992827497341372}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,992] Trial 6 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.004875944790006727, 'alpha': 8.278525432543186e-07}. Best is trial 6 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 19:18:42,080] Trial 7 finished with value: 0.5957839834409895 and parameters: {'booster': 'gbtree', 'lambda': 3.742688380242692e-05, 'alpha': 6.370361951091849e-07, 'max_depth': 3, 'eta': 0.8133082302804496, 'gamma': 5.634591381378434e-05, 'grow_policy': 'lossguide'}. Best is trial 6 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:42,204] Trial 8 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.649370961811432, 'alpha': 4.868692580741491e-07, 'max_depth': 8, 'eta': 9.401586697777986e-05, 'gamma': 1.4036135030391968e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 8.381540989037945e-07, 'skip_drop': 0.04372103568102062}. Best is trial 6 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 19:18:42,273] Trial 9 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00014034045703319477, 'alpha': 3.3108716699093745e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,344] Trial 10 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 4.068048925310283e-08, 'alpha': 8.019133438677717e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,416] Trial 11 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 9.064174582070118e-08, 'alpha': 9.451074100898181e-05}. Best is trial 9 with value: 0.7012351321451076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.701\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.053\n",
      "F1 Macro: 0.851 ± 0.082\n",
      "F1 Weighted: 0.910 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:42,489] Trial 12 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 1.5959550373664294e-08, 'alpha': 0.00010698868845880366}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,562] Trial 13 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.00035866097780415574, 'alpha': 7.113577646025195e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,632] Trial 14 finished with value: 0.5636131189143786 and parameters: {'booster': 'gblinear', 'lambda': 5.831129710366558e-07, 'alpha': 0.001684214548016691}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,721] Trial 15 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.008447414817215762, 'alpha': 1.1290634951871592e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,790] Trial 16 finished with value: 0.5652708188613967 and parameters: {'booster': 'gblinear', 'lambda': 6.269592200405432e-05, 'alpha': 0.03427034067784772}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,865] Trial 17 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 2.385851657678274e-06, 'alpha': 2.4872522750497344e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,938] Trial 18 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.3206352393950663e-06, 'alpha': 8.95730116612407e-06}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,007] Trial 19 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.0002905407250572697, 'alpha': 3.169797980340927e-08}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,078] Trial 20 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 5.4453402105410365e-06, 'alpha': 1.0381639916892128e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,148] Trial 21 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 0.00045823825607512223, 'alpha': 1.673455869535136e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,222] Trial 22 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 0.0003008590890939841, 'alpha': 1.0399964313573931e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,289] Trial 23 finished with value: 0.6787024344579722 and parameters: {'booster': 'gblinear', 'lambda': 0.01211266400768025, 'alpha': 1.01203440092278e-08}. Best is trial 21 with value: 0.7361252920039949.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.736\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.868 ± 0.077\n",
      "F1 Weighted: 0.922 ± 0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:43,360] Trial 24 finished with value: 0.6712488553683833 and parameters: {'booster': 'gblinear', 'lambda': 3.3004014002630894e-05, 'alpha': 0.0006338456286067703}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,432] Trial 25 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0014121273544600171, 'alpha': 9.036017225149375e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,501] Trial 26 finished with value: 0.6787024344579722 and parameters: {'booster': 'gblinear', 'lambda': 0.032572906176680905, 'alpha': 0.0004046808096819849}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,568] Trial 27 finished with value: 0.612600042864821 and parameters: {'booster': 'gblinear', 'lambda': 2.7072163844552095e-07, 'alpha': 0.03251672985150403}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,690] Trial 28 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 3.2733728230848342e-06, 'alpha': 2.7390995376131084e-06, 'max_depth': 9, 'eta': 2.061457500786986e-05, 'gamma': 0.8401252851807804, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,821] Trial 29 finished with value: 0.0013903478254046716 and parameters: {'booster': 'dart', 'lambda': 1.3191777368429335e-05, 'alpha': 1.5451584673844528e-07, 'max_depth': 4, 'eta': 1.0754501284708833e-08, 'gamma': 1.3314581903620458e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.4079321561139179e-08, 'skip_drop': 1.4530117839254435e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,962] Trial 30 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 0.0001589567437812458, 'alpha': 3.219396774832125e-05, 'max_depth': 5, 'eta': 1.3331324483248153e-06, 'gamma': 0.881860676232723, 'grow_policy': 'lossguide'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,033] Trial 31 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0012097325177458764, 'alpha': 1.6081927283618518e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,103] Trial 32 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0005688788450926426, 'alpha': 2.95063555560621e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,173] Trial 33 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.5348054512345863e-05, 'alpha': 8.660050850899507e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,247] Trial 34 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00010019594718231664, 'alpha': 2.7384727065502e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,380] Trial 35 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.00015652446655657268, 'alpha': 2.4574239322668134e-07, 'max_depth': 9, 'eta': 0.0023308867543869742, 'gamma': 0.02022329491796311, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0016980134341777087, 'skip_drop': 7.121720341343025e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,506] Trial 36 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 0.004956739581441004, 'alpha': 5.56605578510018e-08, 'max_depth': 5, 'eta': 6.117033611818185e-07, 'gamma': 1.5025892054292314e-08, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,579] Trial 37 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 2.450175338402121e-06, 'alpha': 2.4430838546086436e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,711] Trial 38 finished with value: 0.5388231705365875 and parameters: {'booster': 'dart', 'lambda': 2.3541694178127426e-05, 'alpha': 0.5584646777655228, 'max_depth': 7, 'eta': 0.0005593645174947266, 'gamma': 0.01519419702938851, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.2693524381175154, 'skip_drop': 0.00024072915496283813}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,785] Trial 39 finished with value: 0.6122666381403189 and parameters: {'booster': 'gblinear', 'lambda': 0.0007983977846240928, 'alpha': 0.006144404440826941}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,903] Trial 40 finished with value: 0.5288763416697589 and parameters: {'booster': 'gbtree', 'lambda': 0.024895198820290943, 'alpha': 2.810254760556719e-07, 'max_depth': 4, 'eta': 5.9108631876170084e-06, 'gamma': 4.521538455136203e-07, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,976] Trial 41 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 7.056742905382381e-06, 'alpha': 1.8339921869772347e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,049] Trial 42 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 5.396304600763562e-05, 'alpha': 0.00018969516896196462}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,120] Trial 43 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.0026864412711625264, 'alpha': 2.945173429413845e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,194] Trial 44 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 4.643284043831129e-06, 'alpha': 4.0869159017792e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,269] Trial 45 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 3.0196533887378714e-07, 'alpha': 5.073582089866101e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,338] Trial 46 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00027359635947917654, 'alpha': 1.1883518471032676e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,487] Trial 47 finished with value: 0.5587244304571011 and parameters: {'booster': 'dart', 'lambda': 8.446585593490166e-06, 'alpha': 0.00026755584535787273, 'max_depth': 7, 'eta': 9.74462883298785e-08, 'gamma': 0.02272354982446128, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.064724094184567e-08, 'skip_drop': 0.00037889227065196355}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,557] Trial 48 finished with value: 0.5947745825537253 and parameters: {'booster': 'gblinear', 'lambda': 1.7109916415773722e-06, 'alpha': 0.0013398074341008051}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,626] Trial 49 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 7.855917112352985e-05, 'alpha': 2.5683469783657177e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,695] Trial 50 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 5.790427409665389e-07, 'alpha': 1.033175348350204e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,766] Trial 51 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00012378111850352747, 'alpha': 5.2481560130789185e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,837] Trial 52 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0005056923954403906, 'alpha': 1.932750235405262e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,907] Trial 53 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 3.7524617940600926e-05, 'alpha': 4.689554474205462e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,005] Trial 54 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.002972435146225337, 'alpha': 6.760168850393416e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,081] Trial 55 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 0.0002893758890897774, 'alpha': 1.5356842908428545e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,153] Trial 56 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.8890483214727648e-05, 'alpha': 5.287869418033139e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,230] Trial 57 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 6.749349378808268e-08, 'alpha': 1.657118793967962e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,303] Trial 58 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 9.034618527677589e-05, 'alpha': 0.0001563407466528211}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,434] Trial 59 finished with value: 0.52987054189302 and parameters: {'booster': 'dart', 'lambda': 0.0002057213154824834, 'alpha': 4.067766212756909e-05, 'max_depth': 3, 'eta': 0.04813707597036521, 'gamma': 6.687206129035324e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 9.836850868242433e-05, 'skip_drop': 4.078942122084955e-06}. Best is trial 21 with value: 0.7361252920039949.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'booster': 'gblinear', 'lambda': 0.00045823825607512223, 'alpha': 1.673455869535136e-08}\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.868 ± 0.077\n",
      "F1 Weighted: 0.922 ± 0.047\n"
     ]
    }
   ],
   "source": [
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:36:47,263] A new study created in memory with name: no-name-92cd28b8-c532-4781-91cb-8c063cd37386\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:48,437] Trial 0 finished with value: 0.7155098095140549 and parameters: {'lr': 0.001347965986198317, 'dropout': 0.2733918479520943}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:49,640] Trial 1 finished with value: 0.7129200558332369 and parameters: {'lr': 0.004939639488805297, 'dropout': 0.25410572749953775}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:50,862] Trial 2 finished with value: 0.4477242436013849 and parameters: {'lr': 0.00019647765269764202, 'dropout': 0.39952190349118755}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:52,078] Trial 3 finished with value: 0.7155098095140549 and parameters: {'lr': 0.0005088833552048445, 'dropout': 0.2643513606494006}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:53,270] Trial 4 finished with value: 0.3908956158579512 and parameters: {'lr': 0.00017321185200803305, 'dropout': 0.24851510208473024}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:54,459] Trial 5 finished with value: 0.7129200558332369 and parameters: {'lr': 0.009608208391947646, 'dropout': 0.1201079183693087}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:55,649] Trial 6 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00015723768440938503, 'dropout': 0.3545248824666347}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:56,822] Trial 7 finished with value: 0.643905171245766 and parameters: {'lr': 0.007090338030126664, 'dropout': 0.40896972549134003}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:58,071] Trial 8 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00031201435560173463, 'dropout': 0.1322839086904345}. Best is trial 0 with value: 0.7155098095140549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-10 19:36:59,244] Trial 9 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00034226233489435066, 'dropout': 0.14461085168323737}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.5],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n",
      "Best hyperparameters:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPEvaluator' object has no attribute 'best_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mlp_eval\u001b[38;5;241m.\u001b[39mprint_best_results()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmlp_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_best_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DiplomaGeneral/src/base_classes/evaluator.py:120\u001b[0m, in \u001b[0;36mModelEvaluator.print_best_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_best_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLPEvaluator' object has no attribute 'best_params'"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"knn\")\n",
    "svm_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"svm\")\n",
    "xgb_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.923 ± 0.084\n",
      "F1 Macro: 0.825 ± 0.208\n",
      "F1 Weighted: 0.933 ± 0.072\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "knn_eval.print_best_results()\n",
    "svm_eval.print_best_results()\n",
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(odm.train_y)\n",
    "print(odm.test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (53, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ hsa-miR-2 ┆ hsa-miR-6 ┆ hsa-miR-6 ┆ hsa-miR- │\n",
       " │ 113552    ┆ 204252    ┆ 171067    ┆ 078804    ┆   ┆ 9b-3p     ┆ 808-3p    ┆ 60-3p     ┆ 337-3p   │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.101906  ┆ 0.67432   ┆ 0.553844  ┆ 0.427465  ┆ … ┆ 0.332964  ┆ 0.491693  ┆ 0.228302  ┆ 0.478208 │\n",
       " │ 0.211153  ┆ 0.79269   ┆ 0.443358  ┆ 0.538522  ┆ … ┆ 0.231004  ┆ 0.367655  ┆ 0.0       ┆ 0.463624 │\n",
       " │ 0.248448  ┆ 0.901163  ┆ 0.376432  ┆ 0.818353  ┆ … ┆ 0.567736  ┆ 0.200131  ┆ 0.0       ┆ 0.377795 │\n",
       " │ 0.0       ┆ 0.855781  ┆ 0.095358  ┆ 0.86714   ┆ … ┆ 0.390975  ┆ 0.009925  ┆ 0.0       ┆ 0.353016 │\n",
       " │ 0.027486  ┆ 0.848784  ┆ 0.037625  ┆ 0.83246   ┆ … ┆ 0.394873  ┆ 0.0       ┆ 0.448706  ┆ 0.3035   │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.10708   ┆ 0.863094  ┆ 0.246341  ┆ 0.428165  ┆ … ┆ 0.781878  ┆ 0.147892  ┆ 0.0       ┆ 0.840782 │\n",
       " │ 0.539877  ┆ 0.812806  ┆ 0.505951  ┆ 0.548271  ┆ … ┆ 0.775824  ┆ 0.267936  ┆ 0.0       ┆ 0.670413 │\n",
       " │ 0.714042  ┆ 0.274164  ┆ 0.199133  ┆ 0.287362  ┆ … ┆ 0.733015  ┆ 0.7218    ┆ 0.423134  ┆ 0.496924 │\n",
       " │ 0.43811   ┆ 0.776641  ┆ 0.548896  ┆ 0.568222  ┆ … ┆ 0.61335   ┆ 0.655129  ┆ 0.0       ┆ 0.566595 │\n",
       " │ 0.226454  ┆ 0.0       ┆ 0.291807  ┆ 0.238834  ┆ … ┆ 1.0       ┆ 0.646295  ┆ 0.55724   ┆ 0.850641 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " shape: (13, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ hsa-miR-2 ┆ hsa-miR-6 ┆ hsa-miR-6 ┆ hsa-miR- │\n",
       " │ 113552    ┆ 204252    ┆ 171067    ┆ 078804    ┆   ┆ 9b-3p     ┆ 808-3p    ┆ 60-3p     ┆ 337-3p   │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ -0.296586 ┆ 0.799635  ┆ 0.166325  ┆ 0.924323  ┆ … ┆ 0.394054  ┆ 0.149989  ┆ 0.386551  ┆ 0.370419 │\n",
       " │ -0.005302 ┆ 0.79881   ┆ 0.452376  ┆ 1.10556   ┆ … ┆ 0.392635  ┆ 0.308032  ┆ 0.0       ┆ 0.449365 │\n",
       " │ 0.70828   ┆ 0.985678  ┆ 0.109132  ┆ 0.624708  ┆ … ┆ 0.14714   ┆ 0.877482  ┆ 0.0       ┆ 0.673075 │\n",
       " │ 0.265529  ┆ 0.724728  ┆ 0.383254  ┆ 0.399813  ┆ … ┆ 0.592625  ┆ 0.388665  ┆ 0.387819  ┆ 0.5304   │\n",
       " │ 0.814142  ┆ 0.632889  ┆ 0.346469  ┆ 0.182513  ┆ … ┆ -0.033362 ┆ 0.113175  ┆ 0.0       ┆ 0.333708 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.561813  ┆ 0.755388  ┆ 0.289011  ┆ 0.427494  ┆ … ┆ 0.507615  ┆ 0.304396  ┆ 0.0       ┆ 0.306399 │\n",
       " │ 0.066722  ┆ 0.45259   ┆ 0.90341   ┆ 0.637337  ┆ … ┆ 0.575779  ┆ 0.473346  ┆ 0.696137  ┆ 1.216344 │\n",
       " │ -0.002529 ┆ 0.360621  ┆ 0.711604  ┆ 0.459057  ┆ … ┆ 0.334073  ┆ 0.580128  ┆ 0.0       ┆ 1.206152 │\n",
       " │ 0.288076  ┆ 0.236961  ┆ 0.645504  ┆ 0.491042  ┆ … ┆ 1.096767  ┆ 0.404194  ┆ 0.386418  ┆ 0.766329 │\n",
       " │ 0.329202  ┆ 0.554336  ┆ 0.390962  ┆ 0.477453  ┆ … ┆ 0.430964  ┆ 0.18261   ┆ 0.470895  ┆ 0.472578 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.get_split(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
