{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_circrna_te.csv'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:26,481] A new study created in memory with name: no-name-07aba30c-ad27-4944-8764-f5095cecd085\n",
      "[I 2024-12-07 22:14:26,695] Trial 0 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.612600042864821.\n",
      "[I 2024-12-07 22:14:26,893] Trial 1 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 15}. Best is trial 1 with value: 0.6192439604955923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.613\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.763 ± 0.163\n",
      "F1 Weighted: 0.887 ± 0.054\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.4642857142857143), 'f1_weighted': np.float64(0.8047619047619048)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n",
      "New best score: 0.619\n",
      "Best model performance:\n",
      "Accuracy: 0.907 ± 0.053\n",
      "F1 Macro: 0.769 ± 0.180\n",
      "F1 Weighted: 0.888 ± 0.069\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.4642857142857143), 'f1_weighted': np.float64(0.8047619047619048)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:27,085] Trial 2 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 15}. Best is trial 1 with value: 0.6192439604955923.\n",
      "[I 2024-12-07 22:14:27,275] Trial 3 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:27,463] Trial 4 finished with value: 0.6862235118921005 and parameters: {'n_neighbors': 18}. Best is trial 3 with value: 0.7039337563024542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:27,675] Trial 5 finished with value: 0.6014691748907844 and parameters: {'n_neighbors': 8}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:27,866] Trial 6 finished with value: 0.5472774151647614 and parameters: {'n_neighbors': 19}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:28,065] Trial 7 finished with value: 0.6146283953157812 and parameters: {'n_neighbors': 17}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:28,253] Trial 8 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 15}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:28,442] Trial 9 finished with value: 0.40046307690049987 and parameters: {'n_neighbors': 2}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:28,633] Trial 10 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:28,838] Trial 11 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:29,027] Trial 12 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:29,216] Trial 13 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:29,406] Trial 14 finished with value: 0.6396775991382492 and parameters: {'n_neighbors': 3}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:29,614] Trial 15 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 13}. Best is trial 3 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:29,801] Trial 16 finished with value: 0.7389770960193865 and parameters: {'n_neighbors': 9}. Best is trial 16 with value: 0.7389770960193865.\n",
      "[I 2024-12-07 22:14:29,988] Trial 17 finished with value: 0.6623938241144065 and parameters: {'n_neighbors': 5}. Best is trial 16 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.739\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:30,177] Trial 18 finished with value: 0.6014691748907844 and parameters: {'n_neighbors': 8}. Best is trial 16 with value: 0.7389770960193865.\n",
      "[I 2024-12-07 22:14:30,374] Trial 19 finished with value: 0.6623938241144065 and parameters: {'n_neighbors': 5}. Best is trial 16 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:30,410] A new study created in memory with name: no-name-af95b62c-a13e-4f24-99a7-931b8ecbb5e1\n",
      "[I 2024-12-07 22:14:30,599] Trial 0 finished with value: 0.7039337563024542 and parameters: {'C': 0.03696927231237428, 'class_weight': None}. Best is trial 0 with value: 0.7039337563024542.\n",
      "[I 2024-12-07 22:14:30,792] Trial 1 finished with value: 0.7039337563024542 and parameters: {'C': 0.04499582723460574, 'class_weight': None}. Best is trial 0 with value: 0.7039337563024542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:30,996] Trial 2 finished with value: 0.7573892492973633 and parameters: {'C': 0.7327880174152276, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:31,176] Trial 3 finished with value: 0.7039337563024542 and parameters: {'C': 0.014790879250326041, 'class_weight': None}. Best is trial 2 with value: 0.7573892492973633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.757\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.873 ± 0.072\n",
      "F1 Weighted: 0.929 ± 0.042\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:31,380] Trial 4 finished with value: 0.7573892492973633 and parameters: {'C': 1.926035170020619, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:31,578] Trial 5 finished with value: 0.7155098095140549 and parameters: {'C': 0.039141525228436225, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:31,805] Trial 6 finished with value: 0.7039337563024542 and parameters: {'C': 0.15689888855597295, 'class_weight': None}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:32,022] Trial 7 finished with value: 0.7573892492973633 and parameters: {'C': 5.3393000292769255, 'class_weight': None}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:32,235] Trial 8 finished with value: 0.7039337563024542 and parameters: {'C': 0.01811482714876531, 'class_weight': None}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:32,455] Trial 9 finished with value: 0.7573892492973633 and parameters: {'C': 3.9973760697332867, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:32,662] Trial 10 finished with value: 0.7573892492973633 and parameters: {'C': 0.723289028494131, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:32,879] Trial 11 finished with value: 0.7573892492973633 and parameters: {'C': 1.08451713310716, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:33,092] Trial 12 finished with value: 0.7573892492973633 and parameters: {'C': 1.4595104812186352, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:33,305] Trial 13 finished with value: 0.7573892492973633 and parameters: {'C': 0.3227898700181083, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:33,514] Trial 14 finished with value: 0.7573892492973633 and parameters: {'C': 2.220016561950288, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:33,723] Trial 15 finished with value: 0.7573892492973633 and parameters: {'C': 0.4100811329892255, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:33,932] Trial 16 finished with value: 0.7573892492973633 and parameters: {'C': 9.475354515627513, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7573892492973633.\n",
      "[I 2024-12-07 22:14:34,265] Trial 17 finished with value: 0.7581635155117249 and parameters: {'C': 0.1382554786699702, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:34,469] Trial 18 finished with value: 0.7581635155117249 and parameters: {'C': 0.129264399116046, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:34,694] Trial 19 finished with value: 0.7581635155117249 and parameters: {'C': 0.1240264039282578, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:34,901] Trial 20 finished with value: 0.7581635155117249 and parameters: {'C': 0.11846975066980431, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:35,115] Trial 21 finished with value: 0.7581635155117249 and parameters: {'C': 0.11807135060217297, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:35,327] Trial 22 finished with value: 0.7581635155117249 and parameters: {'C': 0.1874051254692188, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:35,534] Trial 23 finished with value: 0.7155098095140549 and parameters: {'C': 0.07722304371156181, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:35,744] Trial 24 finished with value: 0.7155098095140549 and parameters: {'C': 0.07037527821764386, 'class_weight': 'balanced'}. Best is trial 17 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:14:35,947] Trial 25 finished with value: 0.8139820349629631 and parameters: {'C': 0.2535641973481975, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.814\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.945 ± 0.051\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:36,151] Trial 26 finished with value: 0.8139820349629631 and parameters: {'C': 0.27421304152757614, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:36,361] Trial 27 finished with value: 0.7573892492973633 and parameters: {'C': 0.455727155435319, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:36,582] Trial 28 finished with value: 0.8139820349629631 and parameters: {'C': 0.23508305060865917, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:36,791] Trial 29 finished with value: 0.7039337563024542 and parameters: {'C': 0.2534063931231969, 'class_weight': None}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:36,997] Trial 30 finished with value: 0.7573892492973633 and parameters: {'C': 0.6081219533341583, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:37,202] Trial 31 finished with value: 0.8139820349629631 and parameters: {'C': 0.23716214947570277, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:37,411] Trial 32 finished with value: 0.8139820349629631 and parameters: {'C': 0.24648751805829908, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:37,621] Trial 33 finished with value: 0.7155098095140549 and parameters: {'C': 0.0647916341935727, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:37,833] Trial 34 finished with value: 0.7039337563024542 and parameters: {'C': 0.3868978569664616, 'class_weight': None}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:38,032] Trial 35 finished with value: 0.7155098095140549 and parameters: {'C': 0.024373583468379038, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:38,239] Trial 36 finished with value: 0.8139820349629631 and parameters: {'C': 0.216254119012149, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:38,467] Trial 37 finished with value: 0.7573892492973633 and parameters: {'C': 1.0499968908847037, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:38,676] Trial 38 finished with value: 0.7039337563024542 and parameters: {'C': 0.5211395677800424, 'class_weight': None}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:38,885] Trial 39 finished with value: 0.8139820349629631 and parameters: {'C': 0.2740832593074323, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:39,062] Trial 40 finished with value: 0.7039337563024542 and parameters: {'C': 0.010679393702523847, 'class_weight': None}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:39,273] Trial 41 finished with value: 0.7581635155117249 and parameters: {'C': 0.19415981638375474, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:39,477] Trial 42 finished with value: 0.8139820349629631 and parameters: {'C': 0.3047766796538134, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:39,678] Trial 43 finished with value: 0.7155098095140549 and parameters: {'C': 0.04645423906452971, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:39,889] Trial 44 finished with value: 0.7573892492973633 and parameters: {'C': 0.8384055445986138, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:40,096] Trial 45 finished with value: 0.7581635155117249 and parameters: {'C': 0.17219504561157437, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:40,325] Trial 46 finished with value: 0.7581635155117249 and parameters: {'C': 0.09781998781191692, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:40,528] Trial 47 finished with value: 0.7573892492973633 and parameters: {'C': 0.33625928709158115, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:40,728] Trial 48 finished with value: 0.7039337563024542 and parameters: {'C': 0.2483733562326447, 'class_weight': None}. Best is trial 25 with value: 0.8139820349629631.\n",
      "[I 2024-12-07 22:14:40,933] Trial 49 finished with value: 0.7573892492973633 and parameters: {'C': 0.6363977368360346, 'class_weight': 'balanced'}. Best is trial 25 with value: 0.8139820349629631.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:13:45,397] A new study created in memory with name: no-name-57031d7d-b726-4aa0-a584-08fd401f5182\n",
      "[I 2024-12-07 22:13:45,626] Trial 0 finished with value: 0.5675043225090064 and parameters: {'booster': 'gbtree', 'lambda': 0.2258161216799686, 'alpha': 0.0009606628003389111, 'max_depth': 3, 'eta': 1.1219467333837462e-06, 'gamma': 5.705570530267046e-08, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5675043225090064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.568\n",
      "Best model performance:\n",
      "Accuracy: 0.866 ± 0.084\n",
      "F1 Macro: 0.756 ± 0.178\n",
      "F1 Weighted: 0.867 ± 0.081\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8, 'f1_macro': np.float64(0.7204968944099379), 'f1_weighted': np.float64(0.8099378881987577)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.7333333333333333, 'f1_macro': np.float64(0.4230769230769231), 'f1_weighted': np.float64(0.7333333333333333)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:13:45,870] Trial 1 finished with value: 0.48162878238196427 and parameters: {'booster': 'dart', 'lambda': 7.069550131503006e-05, 'alpha': 3.561886371432537e-06, 'max_depth': 3, 'eta': 0.038781166188858004, 'gamma': 4.438850227128154e-05, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 6.72383985801384e-08, 'skip_drop': 0.005743015697836312}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:46,103] Trial 2 finished with value: 0.5591177623513526 and parameters: {'booster': 'gbtree', 'lambda': 0.14009061160979316, 'alpha': 1.8880862903511775e-08, 'max_depth': 7, 'eta': 0.0008860530213730884, 'gamma': 0.0003791725838373204, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:46,321] Trial 3 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.0008502292033967831, 'alpha': 1.0205059298042949e-08, 'max_depth': 3, 'eta': 9.694197783968133e-08, 'gamma': 1.1194689834197103e-07, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:46,518] Trial 4 finished with value: 0.4666746613229245 and parameters: {'booster': 'gbtree', 'lambda': 1.8190830787135366e-08, 'alpha': 3.324923159579374e-05, 'max_depth': 2, 'eta': 0.03956908171480727, 'gamma': 0.6283709352346671, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:46,786] Trial 5 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 6.9675651906729195e-06, 'alpha': 0.00011003484757323273, 'max_depth': 8, 'eta': 4.541170999967922e-07, 'gamma': 4.1839901111852977e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.022136871672694045, 'skip_drop': 0.0967389547334329}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:47,025] Trial 6 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 1.2783909577482578e-07, 'alpha': 0.008999948373335656, 'max_depth': 7, 'eta': 3.6751819601063875e-07, 'gamma': 8.585988091784436e-05, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5675043225090064.\n",
      "[I 2024-12-07 22:13:47,173] Trial 7 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.012106232661233231, 'alpha': 1.2694062465277085e-05}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:47,326] Trial 8 finished with value: 0.6013127244639374 and parameters: {'booster': 'gblinear', 'lambda': 8.972916874076115e-08, 'alpha': 0.0007924136129746989}. Best is trial 7 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:13:47,474] Trial 9 finished with value: 0.6288118975723532 and parameters: {'booster': 'gblinear', 'lambda': 0.04231946442032059, 'alpha': 0.0024687025297133783}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:47,624] Trial 10 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 0.002788118632007606, 'alpha': 0.8192488278637083}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:47,776] Trial 11 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.01071967663457468, 'alpha': 9.138660565942961e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:47,928] Trial 12 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.006144272560781403, 'alpha': 8.980818483779622e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,084] Trial 13 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 0.00021173549359171145, 'alpha': 5.213775171089734e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,239] Trial 14 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.019917508190237253, 'alpha': 1.0530792380192003e-05}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,397] Trial 15 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 1.238290465008106e-05, 'alpha': 1.7260635363534304e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,554] Trial 16 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0012309292329126674, 'alpha': 0.00010178361458349275}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,820] Trial 17 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.35097929662730376, 'alpha': 8.686013864056363e-08, 'max_depth': 5, 'eta': 0.00015095242628419058, 'gamma': 0.3085914531515926, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.9246144497050892, 'skip_drop': 1.8392941296682715e-08}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:48,977] Trial 18 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.9709712399059991, 'alpha': 2.9329852604534214e-06}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:49,125] Trial 19 finished with value: 0.612600042864821 and parameters: {'booster': 'gblinear', 'lambda': 0.02587787581642649, 'alpha': 0.04472149171654276}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:49,386] Trial 20 finished with value: 0.585901803069261 and parameters: {'booster': 'dart', 'lambda': 0.00014731262005430022, 'alpha': 1.6049476760373905e-05, 'max_depth': 9, 'eta': 0.4812795537386019, 'gamma': 0.0065838799432135185, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 3.191133526444267e-08, 'skip_drop': 7.537406206262196e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:49,540] Trial 21 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.014862867955656292, 'alpha': 6.079673423700758e-06}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:49,691] Trial 22 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.05423965506156445, 'alpha': 2.610788375415324e-05}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:49,847] Trial 23 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.0037054478319997874, 'alpha': 9.313301376903118e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:50,004] Trial 24 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.013077716353095644, 'alpha': 0.0001938901084895494}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:50,161] Trial 25 finished with value: 0.6866442995243842 and parameters: {'booster': 'gblinear', 'lambda': 0.0004420369597016715, 'alpha': 1.2731284518119177e-07}. Best is trial 7 with value: 0.7581635155117249.\n",
      "[I 2024-12-07 22:13:50,313] Trial 26 finished with value: 0.7711411011566484 and parameters: {'booster': 'gblinear', 'lambda': 1.2262735180246439e-05, 'alpha': 8.475640911381701e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:50,468] Trial 27 finished with value: 0.7711411011566484 and parameters: {'booster': 'gblinear', 'lambda': 1.4135081395376663e-05, 'alpha': 1.1563926508242148e-06}. Best is trial 26 with value: 0.7711411011566484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.771\n",
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.042\n",
      "F1 Macro: 0.886 ± 0.066\n",
      "F1 Weighted: 0.933 ± 0.042\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.9386666666666666)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:13:50,623] Trial 28 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 3.350536607793748e-06, 'alpha': 3.2035763706703653e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:50,887] Trial 29 finished with value: 0.5036718571125454 and parameters: {'booster': 'dart', 'lambda': 1.5758060649862608e-06, 'alpha': 0.0004912369589464265, 'max_depth': 5, 'eta': 1.2106786536757498e-05, 'gamma': 4.14912407900022e-06, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 5.7811939375725656e-05, 'skip_drop': 8.297862776170574e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,140] Trial 30 finished with value: 0.0013903478254046716 and parameters: {'booster': 'gbtree', 'lambda': 3.0365585290179303e-05, 'alpha': 4.111508629716555e-05, 'max_depth': 6, 'eta': 1.076350074564469e-08, 'gamma': 0.008554588664711845, 'grow_policy': 'lossguide'}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,293] Trial 31 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 3.870018742011498e-05, 'alpha': 3.634856746397091e-07}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,446] Trial 32 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 3.1004686968548875e-07, 'alpha': 1.1094863724086096e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,604] Trial 33 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 1.4815150569740173e-06, 'alpha': 4.577920328066477e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,757] Trial 34 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 6.626089294204635e-05, 'alpha': 2.155197687132637e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:51,956] Trial 35 finished with value: 0.4745604857943926 and parameters: {'booster': 'gbtree', 'lambda': 1.7665995027593697e-05, 'alpha': 7.63406523827475e-06, 'max_depth': 1, 'eta': 0.001474276335027977, 'gamma': 2.1376259786001327e-06, 'grow_policy': 'depthwise'}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:52,221] Trial 36 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 0.0009747254295113402, 'alpha': 3.378246174064343e-08, 'max_depth': 9, 'eta': 1.3079113569407137e-05, 'gamma': 1.9533631577304966e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 3.3991550386340196e-05, 'skip_drop': 0.9040128835138177}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:52,375] Trial 37 finished with value: 0.728115464597245 and parameters: {'booster': 'gblinear', 'lambda': 6.776145012061839e-07, 'alpha': 4.314142856789199e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:52,555] Trial 38 finished with value: 0.4745604857943926 and parameters: {'booster': 'gbtree', 'lambda': 0.0002432742505658179, 'alpha': 2.3801432822052206e-07, 'max_depth': 1, 'eta': 0.0107368331364312, 'gamma': 0.022787379409943985, 'grow_policy': 'depthwise'}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:52,711] Trial 39 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.23730426029756424, 'alpha': 1.9935379031971694e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:52,864] Trial 40 finished with value: 0.7711411011566484 and parameters: {'booster': 'gblinear', 'lambda': 5.716223135677081e-06, 'alpha': 1.118145806544913e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,020] Trial 41 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 5.8709402021725736e-06, 'alpha': 2.0969931750784874e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,176] Trial 42 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 4.359083117587659e-06, 'alpha': 6.267004708168965e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,331] Trial 43 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 2.4001524655940818e-08, 'alpha': 1.2690313756081556e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,484] Trial 44 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 8.470801279890884e-05, 'alpha': 4.689744684164238e-07}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,636] Trial 45 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 1.4999556872127326e-05, 'alpha': 0.00021999185223400288}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,828] Trial 46 finished with value: 0.5197072044575879 and parameters: {'booster': 'gbtree', 'lambda': 1.9182702681958624e-06, 'alpha': 5.848330302149202e-06, 'max_depth': 5, 'eta': 0.955019234001326, 'gamma': 0.00027329777757771693, 'grow_policy': 'depthwise'}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:53,980] Trial 47 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.11648332022531382, 'alpha': 2.3874771472025767e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:54,254] Trial 48 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 3.020275662298407e-07, 'alpha': 6.348314794268659e-05, 'max_depth': 4, 'eta': 1.1660117201951303e-05, 'gamma': 1.2781972363480358e-05, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.011225868180369082, 'skip_drop': 5.047194377212339e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:54,414] Trial 49 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 3.8395428664586e-05, 'alpha': 1.1851520679482207e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:54,601] Trial 50 finished with value: 0.569966410908675 and parameters: {'booster': 'gblinear', 'lambda': 0.0015090680120231265, 'alpha': 0.0024201799580097407}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:54,761] Trial 51 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.0035668899701831133, 'alpha': 1.088508818034931e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:54,922] Trial 52 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.07891168903208114, 'alpha': 1.4008206955047287e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,080] Trial 53 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.01456763878533596, 'alpha': 4.646102597698793e-06}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,238] Trial 54 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.007159478208017359, 'alpha': 5.733963809902395e-07}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,391] Trial 55 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 0.00040304262871270613, 'alpha': 0.1837867210737252}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,549] Trial 56 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.028398237505784317, 'alpha': 2.1289923329845535e-07}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,706] Trial 57 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.8093883537332066, 'alpha': 1.0362263241217852e-05}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:55,981] Trial 58 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 1.0589752245932181e-05, 'alpha': 2.096072517874231e-05, 'max_depth': 7, 'eta': 1.8814524736469185e-08, 'gamma': 0.0016767522629014516, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 5.405310037569975e-07, 'skip_drop': 1.5186367657236732e-08}. Best is trial 26 with value: 0.7711411011566484.\n",
      "[I 2024-12-07 22:13:56,136] Trial 59 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.007404187410433044, 'alpha': 6.809134825433926e-05}. Best is trial 26 with value: 0.7711411011566484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.042\n",
      "F1 Macro: 0.886 ± 0.066\n",
      "F1 Weighted: 0.933 ± 0.042\n",
      "Best hyperparameters:\n",
      "{'booster': 'gblinear', 'lambda': 1.2262735180246439e-05, 'alpha': 8.475640911381701e-06}\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.print_best_parameters()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 22:14:40,960] A new study created in memory with name: no-name-24dbe65a-b4d8-4225-bf99-1e730ca6bee0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:42,474] Trial 0 finished with value: 0.4915300435158549 and parameters: {'lr': 0.0003772768129925274, 'dropout': 0.3815303768398889}. Best is trial 0 with value: 0.4915300435158549.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.492\n",
      "Best model performance:\n",
      "Accuracy: 0.878 ± 0.028\n",
      "F1 Macro: 0.662 ± 0.171\n",
      "F1 Weighted: 0.846 ± 0.049\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.4642857142857143), 'f1_weighted': np.float64(0.8047619047619048)}, {'acc': 0.8571428571428571, 'f1_macro': np.float64(0.46153846153846156), 'f1_weighted': np.float64(0.7912087912087912)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:43,961] Trial 1 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0013871291539731146, 'dropout': 0.14389250000144463}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:45,463] Trial 2 finished with value: 0.7581635155117249 and parameters: {'lr': 0.0031490379565270424, 'dropout': 0.5097806824707785}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:46,981] Trial 3 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0004106582731755948, 'dropout': 0.13036297106240527}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:48,430] Trial 4 finished with value: 0.5716516161494184 and parameters: {'lr': 0.00027732323435588713, 'dropout': 0.21284038012894255}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:49,917] Trial 5 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0009888287594642475, 'dropout': 0.5932169170365355}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:51,385] Trial 6 finished with value: 0.6297654796220318 and parameters: {'lr': 0.00880650014158617, 'dropout': 0.5685498752415934}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:52,893] Trial 7 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00010363496238632984, 'dropout': 0.20937347770348655}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:54,360] Trial 8 finished with value: 0.6631836987350632 and parameters: {'lr': 0.004834492456425044, 'dropout': 0.10828158474179014}. Best is trial 2 with value: 0.7581635155117249.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-07 22:14:55,848] Trial 9 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00011825206121159824, 'dropout': 0.3966485221143118}. Best is trial 2 with value: 0.7581635155117249.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.849 ± 0.038\n",
      "F1 Weighted: 0.916 ± 0.025\n",
      "Best hyperparameters:\n",
      "{'lr': 0.006410579595352722, 'dropout': 0.12366664616715325}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([200, 200, 200], ['mrna', 'mirna', 'te'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.in_channels, mogonet_eval.omic_names\n",
    "# mogonet_eval.evaluate()\n",
    "# mogonet_eval.print_best_results()\n",
    "# mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mrna': {'ENSG00000181826': 0, 'ENSG00000278588': 0, 'ENSG00000120594': 0, 'ENSG00000121797': 0, 'ENSG00000140398': 0, 'ENSG00000168062': 0, 'ENSG00000174307': 0, 'ENSG00000184897': 0, 'ENSG00000105497': 0, 'ENSG00000113552': 0, 'ENSG00000188536': 0, 'ENSG00000181004': 0, 'ENSG00000143590': 0, 'ENSG00000006534': 0, 'ENSG00000184792': 0, 'ENSG00000114737': 0, 'ENSG00000130518': 0, 'ENSG00000133561': 0, 'ENSG00000179820': 0, 'ENSG00000196329': 0, 'ENSG00000164626': 0, 'ENSG00000206172': 0, 'ENSG00000196866': 0, 'ENSG00000160013': 0, 'ENSG00000164938': 0, 'ENSG00000260729': 0, 'ENSG00000204161': 0, 'ENSG00000062282': 0, 'ENSG00000087903': 0, 'ENSG00000176641': 0, 'ENSG00000136603': 0, 'ENSG00000113369': 0, 'ENSG00000148935': 0, 'ENSG00000153071': 0, 'ENSG00000174130': 0, 'ENSG00000131016': 0, 'ENSG00000144959': 0, 'ENSG00000203883': 0, 'ENSG00000116574': 0, 'ENSG00000078804': 0, 'ENSG00000120217': 0, 'ENSG00000172667': 0, 'ENSG00000161544': 0, 'ENSG00000121966': 0, 'ENSG00000197646': 0, 'ENSG00000271303': 0, 'ENSG00000125846': 0, 'ENSG00000119801': 0, 'ENSG00000179144': 0, 'ENSG00000196549': 0, 'ENSG00000196230': 0, 'ENSG00000184588': 0, 'ENSG00000026103': 0, 'ENSG00000158578': 0, 'ENSG00000130052': 0, 'ENSG00000119686': 0, 'ENSG00000132475': 0, 'ENSG00000172995': 0, 'ENSG00000022567': 0, 'ENSG00000272398': 0, 'ENSG00000228727': 0, 'ENSG00000171680': 0, 'ENSG00000175538': 0, 'ENSG00000178752': 0, 'ENSG00000119508': 0, 'ENSG00000187837': 0, 'ENSG00000185614': 0, 'ENSG00000178573': 0, 'ENSG00000177191': 0, 'ENSG00000073737': 0, 'ENSG00000092295': 0, 'ENSG00000138623': 0, 'ENSG00000135926': 0, 'ENSG00000185669': 0, 'ENSG00000189283': 0, 'ENSG00000026025': 0, 'ENSG00000168298': 0, 'ENSG00000164330': 0, 'ENSG00000081320': 0, 'ENSG00000174944': 0, 'ENSG00000175449': 0, 'ENSG00000181790': 0, 'ENSG00000260314': 0, 'ENSG00000177455': 0, 'ENSG00000142961': 0, 'ENSG00000073754': 0, 'ENSG00000124575': 0, 'ENSG00000109321': 0, 'ENSG00000175097': 0, 'ENSG00000127364': 0, 'ENSG00000155659': 0, 'ENSG00000110777': 0, 'ENSG00000185730': 0, 'ENSG00000244734': 0, 'ENSG00000196092': 0, 'ENSG00000185304': 0, 'ENSG00000130021': 0, 'ENSG00000182584': 0, 'ENSG00000196517': 0, 'ENSG00000284931': 0, 'ENSG00000084764': 0, 'ENSG00000128322': 0, 'ENSG00000068976': 0, 'ENSG00000166349': 0, 'ENSG00000146592': 0, 'ENSG00000005238': 0, 'ENSG00000111424': 0, 'ENSG00000116096': 0, 'ENSG00000065534': 0, 'ENSG00000181444': 0, 'ENSG00000130787': 0, 'ENSG00000137310': 0, 'ENSG00000159958': 0, 'ENSG00000155090': 0, 'ENSG00000263155': 0, 'ENSG00000166289': 0, 'ENSG00000176845': 0, 'ENSG00000121858': 0, 'ENSG00000136541': 0, 'ENSG00000144749': 0, 'ENSG00000158373': 0, 'ENSG00000163491': 0, 'ENSG00000165272': 0, 'ENSG00000004939': 0, 'ENSG00000171729': 0, 'ENSG00000161835': 0, 'ENSG00000137834': 0, 'ENSG00000129116': 0, 'ENSG00000105369': 0, 'ENSG00000127366': 0, 'ENSG00000174600': 0, 'ENSG00000204186': 0, 'ENSG00000180155': 0, 'ENSG00000131080': 0, 'ENSG00000139174': 0, 'ENSG00000168785': 0, 'ENSG00000105409': 0, 'ENSG00000176125': 0, 'ENSG00000007312': 0, 'ENSG00000171115': 0, 'ENSG00000107796': 0, 'ENSG00000196628': 0, 'ENSG00000151491': 0, 'ENSG00000102554': 0, 'ENSG00000196565': 0, 'ENSG00000132334': 0, 'ENSG00000120318': 0, 'ENSG00000120889': 0, 'ENSG00000124882': 0, 'ENSG00000188153': 0, 'ENSG00000198805': 0, 'ENSG00000185745': 0, 'ENSG00000127362': 0, 'ENSG00000147454': 0, 'ENSG00000137193': 0, 'ENSG00000180061': 0, 'ENSG00000075213': 0, 'ENSG00000169575': 0, 'ENSG00000258674': 0, 'ENSG00000213934': 0, 'ENSG00000135898': 0, 'ENSG00000101276': 0, 'ENSG00000065833': 0, 'ENSG00000180340': 0, 'ENSG00000128274': 0, 'ENSG00000171621': 0, 'ENSG00000080823': 0, 'ENSG00000185090': 0, 'ENSG00000077044': 0, 'ENSG00000090776': 0, 'ENSG00000171860': 0, 'ENSG00000161513': 0, 'ENSG00000054654': 0, 'ENSG00000158555': 0, 'ENSG00000138795': 0, 'ENSG00000160321': 0, 'ENSG00000179163': 0, 'ENSG00000169242': 0, 'ENSG00000099377': 0, 'ENSG00000170412': 0, 'ENSG00000112182': 0, 'ENSG00000170180': 0, 'ENSG00000108219': 0, 'ENSG00000159899': 0, 'ENSG00000154640': 0, 'ENSG00000185015': 0, 'ENSG00000076864': 0, 'ENSG00000171208': 0, 'ENSG00000173369': 0, 'ENSG00000143147': 0, 'ENSG00000165323': 0, 'ENSG00000087589': 0, 'ENSG00000080819': 0, 'ENSG00000214940': 0, 'ENSG00000168671': 0, 'ENSG00000250722': 0, 'ENSG00000165886': 0, 'ENSG00000088899': 0, 'ENSG00000130821': 0, 'ENSG00000172159': 0}, 'mirna': {'ENSG00000221771': 0, 'ENSG00000277402': 0, 'ENSG00000264781': 0, 'ENSG00000274034': 0, 'ENSG00000207975': 0, 'ENSG00000275458': 0, 'ENSG00000284387': 0, 'ENSG00000208037': 0, 'ENSG00000264773': 0, 'ENSG00000276961': 0, 'ENSG00000207808': 0, 'ENSG00000265612': 0, 'ENSG00000274494': 0, 'ENSG00000274552': 0, 'ENSG00000284375': 0, 'ENSG00000263413': 0, 'ENSG00000283929': 0, 'ENSG00000207980': 0, 'ENSG00000266751': 0, 'ENSG00000221445': 0, 'ENSG00000266643': 0, 'ENSG00000264354': 0, 'ENSG00000199090': 0, 'ENSG00000207622': 0, 'ENSG00000263790': 0, 'ENSG00000283200': 0, 'ENSG00000265321': 0, 'ENSG00000221680': 0, 'ENSG00000277942': 0, 'ENSG00000208023': 0, 'ENSG00000265134': 0, 'ENSG00000266017': 0, 'ENSG00000263409': 0, 'ENSG00000275022': 0, 'ENSG00000266594': 0, 'ENSG00000275967': 0, 'ENSG00000274620': 0, 'ENSG00000266325': 0, 'ENSG00000283172': 0, 'ENSG00000264357': 0, 'ENSG00000264102': 0, 'ENSG00000284031': 0, 'ENSG00000207983': 0, 'ENSG00000275101': 0, 'ENSG00000221091': 0, 'ENSG00000264500': 0, 'ENSG00000273776': 0, 'ENSG00000275789': 0, 'ENSG00000266297': 0, 'ENSG00000207776': 0, 'ENSG00000265879': 0, 'ENSG00000274060': 0, 'ENSG00000263857': 0, 'ENSG00000265623': 0, 'ENSG00000264160': 0, 'ENSG00000198974': 0, 'ENSG00000271886': 0, 'ENSG00000267959': 0, 'ENSG00000221063': 0, 'ENSG00000274822': 0, 'ENSG00000275692': 0, 'ENSG00000284229': 0, 'ENSG00000266698': 0, 'ENSG00000266124': 0, 'ENSG00000207650': 0, 'ENSG00000207815': 0, 'ENSG00000276869': 0, 'ENSG00000278420': 0, 'ENSG00000274986': 0, 'ENSG00000274380': 0, 'ENSG00000265333': 0, 'ENSG00000221176': 0, 'ENSG00000264572': 0, 'ENSG00000221603': 0, 'ENSG00000264610': 0, 'ENSG00000207839': 0, 'ENSG00000276926': 0, 'ENSG00000264292': 0, 'ENSG00000265657': 0, 'ENSG00000264864': 0, 'ENSG00000221190': 0, 'ENSG00000276908': 0, 'ENSG00000221214': 0, 'ENSG00000266407': 0, 'ENSG00000266320': 0, 'ENSG00000263527': 0, 'ENSG00000221792': 0, 'ENSG00000221411': 0, 'ENSG00000264796': 0, 'ENSG00000199161': 0, 'ENSG00000266619': 0, 'ENSG00000264947': 0, 'ENSG00000266235': 0, 'ENSG00000207988': 0, 'ENSG00000266533': 0, 'ENSG00000221333': 0, 'ENSG00000284378': 0, 'ENSG00000264585': 0, 'ENSG00000207654': 0, 'ENSG00000274314': 0, 'ENSG00000276753': 0, 'ENSG00000265867': 0, 'ENSG00000264931': 0, 'ENSG00000265137': 0, 'ENSG00000211575': 0, 'ENSG00000275207': 0, 'ENSG00000266139': 0, 'ENSG00000263584': 0, 'ENSG00000207741': 0, 'ENSG00000266270': 0, 'ENSG00000264049': 0, 'ENSG00000264616': 0, 'ENSG00000275651': 0, 'ENSG00000275067': 0, 'ENSG00000221533': 0, 'ENSG00000263361': 0, 'ENSG00000265874': 0, 'ENSG00000265102': 0, 'ENSG00000275667': 0, 'ENSG00000278549': 0, 'ENSG00000207583': 0, 'ENSG00000274466': 0, 'ENSG00000263828': 0, 'ENSG00000264814': 0, 'ENSG00000283204': 0, 'ENSG00000265660': 0, 'ENSG00000222071': 0, 'ENSG00000265996': 0, 'ENSG00000273836': 0, 'ENSG00000211513': 0, 'ENSG00000215973': 0, 'ENSG00000221760': 0, 'ENSG00000274417': 0, 'ENSG00000208002': 0, 'ENSG00000266146': 0, 'ENSG00000266245': 0, 'ENSG00000221406': 0, 'ENSG00000283971': 0, 'ENSG00000274111': 0, 'ENSG00000264349': 0, 'ENSG00000266589': 0, 'ENSG00000266758': 0, 'ENSG00000278658': 0, 'ENSG00000276641': 0, 'ENSG00000276102': 0, 'ENSG00000275950': 0, 'ENSG00000274134': 0, 'ENSG00000263439': 0, 'ENSG00000277379': 0, 'ENSG00000266668': 0, 'ENSG00000212017': 0, 'ENSG00000284154': 0, 'ENSG00000273874': 0, 'ENSG00000263381': 0, 'ENSG00000283498': 0, 'ENSG00000207864': 0, 'ENSG00000264201': 0, 'ENSG00000283532': 0, 'ENSG00000275449': 0, 'ENSG00000265539': 0, 'ENSG00000221394': 0, 'ENSG00000212024': 0, 'ENSG00000202566': 0, 'ENSG00000274705': 0, 'ENSG00000278449': 0, 'ENSG00000278349': 0, 'ENSG00000283514': 0, 'ENSG00000207779': 0, 'ENSG00000266618': 0, 'ENSG00000263813': 0, 'ENSG00000221585': 0, 'ENSG00000266063': 0, 'ENSG00000265201': 0, 'ENSG00000265565': 0, 'ENSG00000264536': 0, 'ENSG00000284186': 0, 'ENSG00000283676': 0, 'ENSG00000263675': 0, 'ENSG00000278447': 0, 'ENSG00000284419': 0, 'ENSG00000198995': 0, 'ENSG00000265112': 0, 'ENSG00000199024': 0, 'ENSG00000283441': 0, 'ENSG00000263963': 0, 'ENSG00000265820': 0, 'ENSG00000283206': 0, 'ENSG00000221325': 0, 'ENSG00000264653': 0, 'ENSG00000283429': 0, 'ENSG00000284224': 0, 'ENSG00000265396': 0, 'ENSG00000283475': 0, 'ENSG00000266038': 0, 'ENSG00000207588': 0, 'ENSG00000281842': 0, 'ENSG00000276404': 0, 'ENSG00000207820': 0, 'ENSG00000252695': 0, 'ENSG00000263831': 0}, 'te': {'HERV-Fc1': 0, 'LTR10C': 0, 'L1M3C_5': 0, 'LTR27E': 0, 'L2': 0, 'L1P4b_5end': 0, 'LTR18A': 0, 'MER87': 0, 'MER57E3': 0, 'THER2': 0, 'MLT1K': 0, 'LTR70': 0, 'LTR1F1': 0, 'MER54B': 0, 'LTR28': 0, 'LTR21A': 0, 'MIR3': 0, 'LTR2': 0, 'MER51E': 0, 'LTR27B': 0, 'L1M3D_5': 0, 'HERVE_a': 0, 'LTR34': 0, 'MER57F': 0, 'LTR38C': 0, 'LTR3': 0, 'L1MA9_5': 0, 'L1P4c_5end': 0, 'HARLEQUIN': 0, 'LTR1C1': 0, 'LTR36': 0, 'LTR60B': 0, 'L1ME3C_3end': 0, 'LOR1b_LTR': 0, 'HERVS71': 0, 'LTR1C': 0, 'LTR47A2': 0, 'MER66A': 0, 'LTR16A1': 0, 'LTR24': 0, 'HERVL66I': 0, 'L1ME4': 0, 'MER57E1': 0, 'MER68B': 0, 'MER70A': 0, 'LTR26E': 0, 'LTR57': 0, 'MER61B': 0, 'AluYf5': 0, 'MLT1G3': 0, 'MER9B': 0, 'L1ME5_3end': 0, 'MER66D': 0, 'LTR58': 0, 'LTR25': 0, 'LTR75_1': 0, 'MER65C': 0, 'LTR1B1': 0, 'LTR71A': 0, 'MER83C': 0, 'HERV-K14CI': 0, 'LTR2B': 0, 'MER52A': 0, 'LTR15': 0, 'MER101': 0, 'MER34A': 0, 'SVA_D': 0, 'HERV1_LTRb': 0, 'LTR25-int': 0, 'LTR72': 0, 'MER66B': 0, 'FRAM': 0, 'HERVE': 0, 'LTR53': 0, 'MLT2D': 0, 'LTR2C': 0, 'AluY': 0, 'LTR64': 0, 'LTR1D1': 0, 'LTR38A1': 0, 'L2B': 0, 'MER74A': 0, 'LTR9B': 0, 'LTR24B': 0, 'MER57C1': 0, 'L1ME2': 0, 'LTR9A1': 0, 'LTR14C': 0, 'LTR62': 0, 'LTR44': 0, 'LTR60': 0, 'MER31B': 0, 'LTR77': 0, 'L1PA14_5': 0, 'MER34D': 0, 'MER57B2': 0, 'LTR37B': 0, 'HERV17': 0, 'MER9': 0, 'LTR26': 0, 'LTR1F': 0, 'LTR12E': 0, 'HERVK11DI': 0, 'LTR2752': 0, 'L1PBA1_5': 0, 'MER67C': 0, 'MIRc': 0, 'MER41E': 0, 'PrimLTR79': 0, 'PABL_A': 0, 'LTR46': 0, 'LOR1I': 0, 'LTR12B': 0, 'AluYa1': 0, 'MLT1J': 0, 'L1MC4': 0, 'L1M1B_5': 0, 'LTR40A': 0, 'HERVI': 0, 'MER57D': 0, 'MLT1H': 0, 'MER34-int': 0, 'LTR26B': 0, 'MER34C2': 0, 'MER67A': 0, 'LTR17': 0, 'MER52C': 0, 'L1ME3A': 0, 'AluYd8': 0, 'MER66C': 0, 'HERV-K14I': 0, 'MER21B': 0, 'MLT1_I': 0, 'MER4D_LTR': 0, 'AluYd3': 0, 'LTR51': 0, 'LTR72B': 0, 'MER88': 0, 'LTR59': 0, 'LTR22C0': 0, 'ALU': 0, 'L1PA7_5': 0, 'HERVK': 0, 'L1P4d_5end': 0, 'L1MD2': 0, 'MER50B': 0, 'L1MB4_5': 0, 'HUERS-P1': 0, 'AluJo': 0, 'LTR6A': 0, 'HERVL': 0, 'IN25': 0, 'MER50C': 0, 'MER21C': 0, 'LTR12': 0, 'AluYb3a2': 0, 'MLT1H1': 0, 'MER4CL34': 0, 'MER67D': 0, 'LTR12C': 0, 'LTR71B': 0, 'LTR39': 0, 'LTR41': 0, 'LTR54B': 0, 'L1ME3E_3end': 0, 'FAM': 0, 'LTR7C': 0, 'LTR8B': 0, 'PABL_B': 0, 'L1PA17_5': 0, 'MLT1C1': 0, 'MER57A1': 0, 'L1HS': 0, 'MER51B': 0, 'LTR19A': 0, 'MER41D': 0, 'LTR53B': 0, 'AluJr4': 0, 'HERVH': 0, 'MER68A': 0, 'L1MCA_5': 0, 'LTR1B': 0, 'MER11D': 0, 'LTR48B': 0, 'HERV35I': 0, 'L1PBB_5': 0, 'L1PA13_5': 0, 'ERVL': 0, 'HERVIP10FH': 0, 'LTR9D': 0, 'ERV3-16A3_I': 0, 'LTR13A': 0, 'L1PREC1': 0, 'L1PBA_5': 0, 'MER92B': 0, 'LTR54': 0, 'L1M3DE_5': 0, 'HERV19I': 0, 'LTR40C': 0, 'L1MA8': 0}}\n"
     ]
    }
   ],
   "source": [
    "mogonet_eval.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HERV-Fc1',\n",
       " 'LTR10C',\n",
       " 'L1M3C_5',\n",
       " 'LTR27E',\n",
       " 'L2',\n",
       " 'L1P4b_5end',\n",
       " 'LTR18A',\n",
       " 'MER87',\n",
       " 'MER57E3',\n",
       " 'THER2',\n",
       " 'MLT1K',\n",
       " 'LTR70',\n",
       " 'LTR1F1',\n",
       " 'MER54B',\n",
       " 'LTR28',\n",
       " 'LTR21A',\n",
       " 'MIR3',\n",
       " 'LTR2',\n",
       " 'MER51E',\n",
       " 'LTR27B',\n",
       " 'L1M3D_5',\n",
       " 'HERVE_a',\n",
       " 'LTR34',\n",
       " 'MER57F',\n",
       " 'LTR38C',\n",
       " 'LTR3',\n",
       " 'L1MA9_5',\n",
       " 'L1P4c_5end',\n",
       " 'HARLEQUIN',\n",
       " 'LTR1C1',\n",
       " 'LTR36',\n",
       " 'LTR60B',\n",
       " 'L1ME3C_3end',\n",
       " 'LOR1b_LTR',\n",
       " 'HERVS71',\n",
       " 'LTR1C',\n",
       " 'LTR47A2',\n",
       " 'MER66A',\n",
       " 'LTR16A1',\n",
       " 'LTR24',\n",
       " 'HERVL66I',\n",
       " 'L1ME4',\n",
       " 'MER57E1',\n",
       " 'MER68B',\n",
       " 'MER70A',\n",
       " 'LTR26E',\n",
       " 'LTR57',\n",
       " 'MER61B',\n",
       " 'AluYf5',\n",
       " 'MLT1G3',\n",
       " 'MER9B',\n",
       " 'L1ME5_3end',\n",
       " 'MER66D',\n",
       " 'LTR58',\n",
       " 'LTR25',\n",
       " 'LTR75_1',\n",
       " 'MER65C',\n",
       " 'LTR1B1',\n",
       " 'LTR71A',\n",
       " 'MER83C',\n",
       " 'HERV-K14CI',\n",
       " 'LTR2B',\n",
       " 'MER52A',\n",
       " 'LTR15',\n",
       " 'MER101',\n",
       " 'MER34A',\n",
       " 'SVA_D',\n",
       " 'HERV1_LTRb',\n",
       " 'LTR25-int',\n",
       " 'LTR72',\n",
       " 'MER66B',\n",
       " 'FRAM',\n",
       " 'HERVE',\n",
       " 'LTR53',\n",
       " 'MLT2D',\n",
       " 'LTR2C',\n",
       " 'AluY',\n",
       " 'LTR64',\n",
       " 'LTR1D1',\n",
       " 'LTR38A1',\n",
       " 'L2B',\n",
       " 'MER74A',\n",
       " 'LTR9B',\n",
       " 'LTR24B',\n",
       " 'MER57C1',\n",
       " 'L1ME2',\n",
       " 'LTR9A1',\n",
       " 'LTR14C',\n",
       " 'LTR62',\n",
       " 'LTR44',\n",
       " 'LTR60',\n",
       " 'MER31B',\n",
       " 'LTR77',\n",
       " 'L1PA14_5',\n",
       " 'MER34D',\n",
       " 'MER57B2',\n",
       " 'LTR37B',\n",
       " 'HERV17',\n",
       " 'MER9',\n",
       " 'LTR26',\n",
       " 'LTR1F',\n",
       " 'LTR12E',\n",
       " 'HERVK11DI',\n",
       " 'LTR2752',\n",
       " 'L1PBA1_5',\n",
       " 'MER67C',\n",
       " 'MIRc',\n",
       " 'MER41E',\n",
       " 'PrimLTR79',\n",
       " 'PABL_A',\n",
       " 'LTR46',\n",
       " 'LOR1I',\n",
       " 'LTR12B',\n",
       " 'AluYa1',\n",
       " 'MLT1J',\n",
       " 'L1MC4',\n",
       " 'L1M1B_5',\n",
       " 'LTR40A',\n",
       " 'HERVI',\n",
       " 'MER57D',\n",
       " 'MLT1H',\n",
       " 'MER34-int',\n",
       " 'LTR26B',\n",
       " 'MER34C2',\n",
       " 'MER67A',\n",
       " 'LTR17',\n",
       " 'MER52C',\n",
       " 'L1ME3A',\n",
       " 'AluYd8',\n",
       " 'MER66C',\n",
       " 'HERV-K14I',\n",
       " 'MER21B',\n",
       " 'MLT1_I',\n",
       " 'MER4D_LTR',\n",
       " 'AluYd3',\n",
       " 'LTR51',\n",
       " 'LTR72B',\n",
       " 'MER88',\n",
       " 'LTR59',\n",
       " 'LTR22C0',\n",
       " 'ALU',\n",
       " 'L1PA7_5',\n",
       " 'HERVK',\n",
       " 'L1P4d_5end',\n",
       " 'L1MD2',\n",
       " 'MER50B',\n",
       " 'L1MB4_5',\n",
       " 'HUERS-P1',\n",
       " 'AluJo',\n",
       " 'LTR6A',\n",
       " 'HERVL',\n",
       " 'IN25',\n",
       " 'MER50C',\n",
       " 'MER21C',\n",
       " 'LTR12',\n",
       " 'AluYb3a2',\n",
       " 'MLT1H1',\n",
       " 'MER4CL34',\n",
       " 'MER67D',\n",
       " 'LTR12C',\n",
       " 'LTR71B',\n",
       " 'LTR39',\n",
       " 'LTR41',\n",
       " 'LTR54B',\n",
       " 'L1ME3E_3end',\n",
       " 'FAM',\n",
       " 'LTR7C',\n",
       " 'LTR8B',\n",
       " 'PABL_B',\n",
       " 'L1PA17_5',\n",
       " 'MLT1C1',\n",
       " 'MER57A1',\n",
       " 'L1HS',\n",
       " 'MER51B',\n",
       " 'LTR19A',\n",
       " 'MER41D',\n",
       " 'LTR53B',\n",
       " 'AluJr4',\n",
       " 'HERVH',\n",
       " 'MER68A',\n",
       " 'L1MCA_5',\n",
       " 'LTR1B',\n",
       " 'MER11D',\n",
       " 'LTR48B',\n",
       " 'HERV35I',\n",
       " 'L1PBB_5',\n",
       " 'L1PA13_5',\n",
       " 'ERVL',\n",
       " 'HERVIP10FH',\n",
       " 'LTR9D',\n",
       " 'ERV3-16A3_I',\n",
       " 'LTR13A',\n",
       " 'L1PREC1',\n",
       " 'L1PBA_5',\n",
       " 'MER92B',\n",
       " 'LTR54',\n",
       " 'L1M3DE_5',\n",
       " 'HERV19I',\n",
       " 'LTR40C',\n",
       " 'L1MA8']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mogonet_eval.data_manager.get_split(0)\n",
    "mogonet_eval.data_manager.feature_names\n",
    "# data['mrna']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:38:30,432] A new study created in memory with name: no-name-53d82c88-b37a-43ac-a498-90cf04fcfd67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4404, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1544, Train Acc: 0.9153, Train F1 Macro: 0.8282, Train F1 Weighted: 0.9090\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1415, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9067\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0740, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0696, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0755, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0174, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3212, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1876, Train Acc: 0.9322, Train F1 Macro: 0.8689, Train F1 Weighted: 0.9291\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1684, Train Acc: 0.8983, Train F1 Macro: 0.7569, Train F1 Weighted: 0.8794\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1281, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0852, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7545, Val Geometric Mean: 0.7145\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7545\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0536, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0803, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.4054)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3104, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1878, Train Acc: 0.8644, Train F1 Macro: 0.6758, Train F1 Weighted: 0.8393\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2349, Train Acc: 0.8983, Train F1 Macro: 0.8324, Train F1 Weighted: 0.9019\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2634, Train Acc: 0.8475, Train F1 Macro: 0.6563, Train F1 Weighted: 0.8257\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1047, Train Acc: 0.9661, Train F1 Macro: 0.9398, Train F1 Weighted: 0.9661\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1049, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1526, Train Acc: 0.9322, Train F1 Macro: 0.8796, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.8108)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(16.0270)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3331, Train Acc: 0.8305, Train F1 Macro: 0.6385, Train F1 Weighted: 0.8037\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2197, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9039\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1594, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1526, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1313, Train Acc: 0.9322, Train F1 Macro: 0.8883, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1331, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8115\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0485, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7432)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(15.5135)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2777, Train Acc: 0.8333, Train F1 Macro: 0.5370, Train F1 Weighted: 0.7716\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1101, Train Acc: 0.9667, Train F1 Macro: 0.9443, Train F1 Weighted: 0.9667\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1249, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1722, Train Acc: 0.8833, Train F1 Macro: 0.8117, Train F1 Weighted: 0.8853\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0373, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0350, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:40:32,404] Trial 0 finished with value: 0.9491196586666667 and parameters: {}. Best is trial 0 with value: 0.9491196586666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0700, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.7879, Val F1 Weighted: 0.8745, Val Geometric Mean: 0.8390\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.7879, Test F1 Weighted: 0.8745\n",
      "##################################################\n",
      "New best score: 0.949\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "three_layers = True\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": three_layers,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT 3L\" if three_layers else \"BiRGAT 2L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrna, mirna, circrna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, circrna, 2L no interactions\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.918 ± 0.113\n",
    "F1 Weighted: 0.953 ± 0.064\n",
    "---\n",
    "Accuracy: 0.946 ± 0.027\n",
    "F1 Macro: 0.904 ± 0.048\n",
    "F1 Weighted: 0.944 ± 0.028\n",
    "# mrna, mirna, circrna 3L, interactions, degree ~20 in diff exp graphs, larger degree shows degraded performance\n",
    "# making the avg degree to high shows large jumps on the validation set during training\n",
    "Accuracy: 0.945 ± 0.053\n",
    "F1 Macro: 0.910 ± 0.081\n",
    "F1 Weighted: 0.946 ± 0.048\n",
    "# mrna, mirna, circrna 2L, interactions, 64 cap\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053\n",
    "# mrna, mirna, circrna 3L, interactions, 64 cap\n",
    "Accuracy: 0.891 ± 0.054\n",
    "F1 Macro: 0.801 ± 0.088\n",
    "F1 Weighted: 0.888 ± 0.056\n",
    "# mrna, mirna, circrna 3L\n",
    "Accuracy: 0.920 ± 0.050\n",
    "F1 Macro: 0.829 ± 0.112\n",
    "F1 Weighted: 0.907 ± 0.062\n",
    "# mrna, mirna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, 3L\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.897 ± 0.089\n",
    "F1 Weighted: 0.944 ± 0.051"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
