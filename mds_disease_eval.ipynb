{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "  \n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  1\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  2\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  3\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 48    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 13    │\n",
      "└───────┴───────┘\n",
      "fold:  4\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "for fold_idx in range(5):\n",
    "    train_df, test_df = mrna_loader.get_fold(fold_idx)\n",
    "\n",
    "    print(\"fold: \", fold_idx)\n",
    "    print(train_df[\"class\"].value_counts(), test_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.feature_dim, odm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,063] A new study created in memory with name: no-name-487dc805-fc43-418d-8a2d-a156c69f7612\n",
      "[I 2024-11-10 19:15:05,137] Trial 0 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,204] Trial 1 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,288] Trial 2 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,353] Trial 3 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,421] Trial 4 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,489] Trial 5 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,552] Trial 6 finished with value: 0.58024233114353 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,615] Trial 7 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,682] Trial 8 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,747] Trial 9 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,818] Trial 10 finished with value: 0.6723834634892419 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,886] Trial 11 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,955] Trial 12 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,042] Trial 13 finished with value: 0.5846632831891617 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,113] Trial 14 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,182] Trial 15 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,251] Trial 16 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,322] Trial 17 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,392] Trial 18 finished with value: 0.5888211266094818 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,458] Trial 19 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'n_neighbors': 16}\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,374] A new study created in memory with name: no-name-17b65a7d-695e-4e4c-9283-d9a803e51944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,465] Trial 0 finished with value: 0.6497992012967035 and parameters: {'C': 0.5912776084859577, 'class_weight': None, 'rfe_step': 0.18135798798684793, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,574] Trial 1 finished with value: 0.6497992012967035 and parameters: {'C': 1.92761937700467, 'class_weight': 'balanced', 'rfe_step': 0.12740681956658, 'rfe_n_features': 111}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,656] Trial 2 finished with value: 0.6477728936997709 and parameters: {'C': 0.04431683340011391, 'class_weight': 'balanced', 'rfe_step': 0.0561795144594292, 'rfe_n_features': 170}. Best is trial 0 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,742] Trial 3 finished with value: 0.5947745825537253 and parameters: {'C': 6.239037597620005, 'class_weight': None, 'rfe_step': 0.16137130043179504, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,804] Trial 4 finished with value: 0.6497992012967035 and parameters: {'C': 1.8024991990621209, 'class_weight': None, 'rfe_step': 0.16636180097742892, 'rfe_n_features': 200}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,914] Trial 5 finished with value: 0.5947745825537253 and parameters: {'C': 5.247154408538435, 'class_weight': None, 'rfe_step': 0.06264027199095756, 'rfe_n_features': 156}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,008] Trial 6 finished with value: 0.5947745825537253 and parameters: {'C': 9.321484902226024, 'class_weight': None, 'rfe_step': 0.16378265009504533, 'rfe_n_features': 154}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,119] Trial 7 finished with value: 0.5947745825537253 and parameters: {'C': 8.202967718129962, 'class_weight': None, 'rfe_step': 0.05914431146688243, 'rfe_n_features': 158}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,184] Trial 8 finished with value: 0.6477728936997709 and parameters: {'C': 0.04494677478861271, 'class_weight': 'balanced', 'rfe_step': 0.16643442620663507, 'rfe_n_features': 172}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,248] Trial 9 finished with value: 0.6477728936997709 and parameters: {'C': 0.019873129915673807, 'class_weight': 'balanced', 'rfe_step': 0.13255506502150974, 'rfe_n_features': 180}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,334] Trial 10 finished with value: 0.6288118975723532 and parameters: {'C': 0.2890553099749232, 'class_weight': None, 'rfe_step': 0.19984885818456594, 'rfe_n_features': 126}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,448] Trial 11 finished with value: 0.6497992012967035 and parameters: {'C': 0.7846558478244101, 'class_weight': 'balanced', 'rfe_step': 0.1008311361438369, 'rfe_n_features': 101}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,550] Trial 12 finished with value: 0.6787024344579722 and parameters: {'C': 0.25258158703203787, 'class_weight': 'balanced', 'rfe_step': 0.10914476888380255, 'rfe_n_features': 126}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,656] Trial 13 finished with value: 0.6787024344579722 and parameters: {'C': 0.21138530733394667, 'class_weight': 'balanced', 'rfe_step': 0.08776062841727579, 'rfe_n_features': 131}. Best is trial 12 with value: 0.6787024344579722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.679\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.905 ± 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:58,755] Trial 14 finished with value: 0.6787024344579722 and parameters: {'C': 0.15592096415861068, 'class_weight': 'balanced', 'rfe_step': 0.0936740719697733, 'rfe_n_features': 129}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,870] Trial 15 finished with value: 0.7331794285989254 and parameters: {'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:58,960] Trial 16 finished with value: 0.6809510924096367 and parameters: {'C': 0.050080573100068776, 'class_weight': 'balanced', 'rfe_step': 0.1057608612630386, 'rfe_n_features': 120}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,068] Trial 17 finished with value: 0.6422617001076846 and parameters: {'C': 0.07668997260540834, 'class_weight': 'balanced', 'rfe_step': 0.0832177373698388, 'rfe_n_features': 114}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.733\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:59,147] Trial 18 finished with value: 0.6477728936997709 and parameters: {'C': 0.010268450364919926, 'class_weight': 'balanced', 'rfe_step': 0.11357879132217108, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,238] Trial 19 finished with value: 0.6809510924096367 and parameters: {'C': 0.0644825018841236, 'class_weight': 'balanced', 'rfe_step': 0.14146501839591458, 'rfe_n_features': 116}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,340] Trial 20 finished with value: 0.728115464597245 and parameters: {'C': 0.10675039358903013, 'class_weight': 'balanced', 'rfe_step': 0.07651257359386716, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,445] Trial 21 finished with value: 0.728115464597245 and parameters: {'C': 0.1155640352564498, 'class_weight': 'balanced', 'rfe_step': 0.07635708884773021, 'rfe_n_features': 138}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,554] Trial 22 finished with value: 0.728115464597245 and parameters: {'C': 0.13297292036765632, 'class_weight': 'balanced', 'rfe_step': 0.0736767585819143, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,655] Trial 23 finished with value: 0.7331794285989254 and parameters: {'C': 0.0839042307438939, 'class_weight': 'balanced', 'rfe_step': 0.07312489903466748, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,748] Trial 24 finished with value: 0.6477728936997709 and parameters: {'C': 0.022068952997355576, 'class_weight': 'balanced', 'rfe_step': 0.06952300589961795, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,855] Trial 25 finished with value: 0.6497992012967035 and parameters: {'C': 0.4403890185742033, 'class_weight': 'balanced', 'rfe_step': 0.0517673718488775, 'rfe_n_features': 164}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,941] Trial 26 finished with value: 0.6477728936997709 and parameters: {'C': 0.023322190329971796, 'class_weight': 'balanced', 'rfe_step': 0.09350830825927489, 'rfe_n_features': 136}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,038] Trial 27 finished with value: 0.6926488504188454 and parameters: {'C': 0.09203815468971016, 'class_weight': 'balanced', 'rfe_step': 0.07812215855799436, 'rfe_n_features': 147}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,130] Trial 28 finished with value: 0.6809510924096367 and parameters: {'C': 0.03393875406287643, 'class_weight': 'balanced', 'rfe_step': 0.11620427238691917, 'rfe_n_features': 103}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,264] Trial 29 finished with value: 0.6497992012967035 and parameters: {'C': 0.5654000035290196, 'class_weight': 'balanced', 'rfe_step': 0.06790668927665994, 'rfe_n_features': 146}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.938 ± 0.058\n",
    "F1 Macro: 0.684 ± 0.258\n",
    "F1 Weighted: 0.924 ± 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:48,987] A new study created in memory with name: no-name-078ffa6d-2c7a-496a-b139-dae54416a30c\n",
      "[I 2024-11-12 23:57:49,181] Trial 0 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 0.2012630144698972, 'alpha': 4.341615091163527e-08, 'max_depth': 3, 'eta': 2.332756805332267e-06, 'gamma': 1.2412516726806618e-06, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5036718571125454.\n",
      "[I 2024-11-12 23:57:49,288] Trial 1 finished with value: 0.6219707163863691 and parameters: {'booster': 'gblinear', 'lambda': 8.646955492392989e-07, 'alpha': 1.5506307885311386e-07}. Best is trial 1 with value: 0.6219707163863691.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.504\n",
      "Best model performance:\n",
      "Accuracy: 0.825 ± 0.123\n",
      "F1 Macro: 0.729 ± 0.187\n",
      "F1 Weighted: 0.838 ± 0.104\n",
      "New best score: 0.622\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.049\n",
      "F1 Macro: 0.804 ± 0.088\n",
      "F1 Weighted: 0.880 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:49,497] Trial 2 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 2.5211337235291564e-07, 'alpha': 0.0007655003241959716, 'max_depth': 3, 'eta': 3.1708961722098076e-07, 'gamma': 0.6434137367112158, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.4656097396901352e-08, 'skip_drop': 1.394809407147541e-05}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,604] Trial 3 finished with value: 0.5652269064579075 and parameters: {'booster': 'gblinear', 'lambda': 9.852933249626684e-05, 'alpha': 0.0011541940286663782}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,784] Trial 4 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 6.894009654657362e-05, 'alpha': 0.013098965460206181, 'max_depth': 4, 'eta': 3.221944198197507e-05, 'gamma': 2.1300756208798475e-07, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,977] Trial 5 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.06827875535860767, 'alpha': 0.0013947811316405605, 'max_depth': 8, 'eta': 8.226228187567583e-07, 'gamma': 2.1496589291743587e-06, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,156] Trial 6 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 1.586758017056682e-08, 'alpha': 0.00027096780439986665, 'max_depth': 3, 'eta': 1.639910100371045e-05, 'gamma': 3.083038221040991e-05, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,335] Trial 7 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.01441133265718125, 'alpha': 1.517997363432716e-07, 'max_depth': 3, 'eta': 6.05186690982422e-06, 'gamma': 1.493397804642125e-05, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,444] Trial 8 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 5.154325644399204e-06, 'alpha': 7.218531093288851e-07}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,602] Trial 9 finished with value: 0.49872960448032255 and parameters: {'booster': 'dart', 'lambda': 0.012793603702235803, 'alpha': 2.1551367986267964e-07, 'max_depth': 1, 'eta': 7.765576996118632e-07, 'gamma': 0.07662326036468593, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.03260828154492218, 'skip_drop': 0.002823072031521814}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,713] Trial 10 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 2.42729620598781e-06, 'alpha': 7.409878421702675e-06}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,825] Trial 11 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 3.2574254119276433e-06, 'alpha': 1.7501251014935886e-06}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,933] Trial 12 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 3.031008780600074e-06, 'alpha': 0.9313765259674033}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:51,046] Trial 13 finished with value: 0.6522278748969422 and parameters: {'booster': 'gblinear', 'lambda': 6.837093128801486e-08, 'alpha': 1.0154079836126915e-08}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,160] Trial 14 finished with value: 0.5844131317615832 and parameters: {'booster': 'gblinear', 'lambda': 1.3457712092301079e-08, 'alpha': 1.3358060209942263e-05}. Best is trial 13 with value: 0.6522278748969422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.652\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.819 ± 0.098\n",
      "F1 Weighted: 0.892 ± 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:51,277] Trial 15 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 1.9861036182191637e-07, 'alpha': 1.5363778417735748e-08}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,498] Trial 16 finished with value: 0.5675043225090064 and parameters: {'booster': 'dart', 'lambda': 9.193201467048045e-08, 'alpha': 1.1675521747082847e-08, 'max_depth': 9, 'eta': 0.13753571594079478, 'gamma': 0.0009921462222416356, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.9079710789742155, 'skip_drop': 0.45513656784906026}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,609] Trial 17 finished with value: 0.6119104221152568 and parameters: {'booster': 'gblinear', 'lambda': 2.706796288224791e-05, 'alpha': 2.5731722360039313e-05}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,725] Trial 18 finished with value: 0.5575009739708995 and parameters: {'booster': 'gblinear', 'lambda': 1.2472205789904531e-07, 'alpha': 1.1038914553322574e-07}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,837] Trial 19 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0015864259951147818, 'alpha': 1.67898986538854e-06}. Best is trial 19 with value: 0.6760709742693581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.676\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.901 ± 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:52,063] Trial 20 finished with value: 0.48162878238196427 and parameters: {'booster': 'dart', 'lambda': 0.0013594253713011379, 'alpha': 1.0427442190553258e-06, 'max_depth': 7, 'eta': 0.008224588783981643, 'gamma': 0.002546554304199964, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 8.656537424825594e-07, 'skip_drop': 1.5164876825299935e-08}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,175] Trial 21 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0008033446327792338, 'alpha': 3.3395862525915787e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,290] Trial 22 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.00096864069113249, 'alpha': 3.8063602619995724e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,420] Trial 23 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0011847480928647528, 'alpha': 7.568955495245484e-05}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,534] Trial 24 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0008321191051562027, 'alpha': 4.150398302279131e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,645] Trial 25 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.00047924279684625316, 'alpha': 7.922261415903151e-05}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,757] Trial 26 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.006115702581713739, 'alpha': 2.403489779945619e-06}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:52,870] Trial 27 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.007068348306687501, 'alpha': 4.983613495089214e-07}. Best is trial 26 with value: 0.7172503758053912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.717\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.853 ± 0.097\n",
      "F1 Weighted: 0.914 ± 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:53,080] Trial 28 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.8723166811962394, 'alpha': 6.643695005025739e-07, 'max_depth': 7, 'eta': 0.0006642969678843825, 'gamma': 1.141093687755603e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0003347653951284447, 'skip_drop': 5.2559008104433786e-08}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,213] Trial 29 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.04065379049808371, 'alpha': 2.641676480443951e-05}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,349] Trial 30 finished with value: 0.5297617257950828 and parameters: {'booster': 'gbtree', 'lambda': 0.009681052052635123, 'alpha': 2.5948563926624134e-05, 'max_depth': 1, 'eta': 0.609774250373914, 'gamma': 0.004724513014616042, 'grow_policy': 'depthwise'}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,463] Trial 31 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.04696806092777378, 'alpha': 6.424328677092413e-07}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,577] Trial 32 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.09785783345067849, 'alpha': 2.787118060386497e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:53,689] Trial 33 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.3370343582549276, 'alpha': 7.704536972626795e-08}. Best is trial 32 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:53,807] Trial 34 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.29095253999994614, 'alpha': 5.459898539694326e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:53,920] Trial 35 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.5839108883101183, 'alpha': 2.947158047877257e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,056] Trial 36 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.19041924107375766, 'alpha': 5.650857249421197e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,251] Trial 37 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.1825115817608083, 'alpha': 5.490275888289266e-08, 'max_depth': 6, 'eta': 2.9138188559070594e-08, 'gamma': 3.665815759431605e-08, 'grow_policy': 'lossguide'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,364] Trial 38 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.23237081073936433, 'alpha': 5.200403554206324e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,578] Trial 39 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.20060425023437314, 'alpha': 2.2221898294984027e-07, 'max_depth': 5, 'eta': 0.0011024198828807463, 'gamma': 0.00017176625380473714, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.00011970591669768955, 'skip_drop': 0.8994962401548576}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,763] Trial 40 finished with value: 0.6122666381403188 and parameters: {'booster': 'gbtree', 'lambda': 0.08924863241922468, 'alpha': 3.959013021333259e-08, 'max_depth': 5, 'eta': 0.06359289930152, 'gamma': 0.035590451980013614, 'grow_policy': 'lossguide'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,875] Trial 41 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.393045881889716, 'alpha': 7.246586679112614e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,996] Trial 42 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.11429081190472298, 'alpha': 3.325976396678091e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,110] Trial 43 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.02895913419064581, 'alpha': 2.6466726982642934e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,242] Trial 44 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.33206185593540327, 'alpha': 1.0279611622279887e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,357] Trial 45 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.8084812424635968, 'alpha': 2.488301670965524e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,471] Trial 46 finished with value: 0.612600042864821 and parameters: {'booster': 'gblinear', 'lambda': 0.018410742614269726, 'alpha': 0.04114472206767246}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,587] Trial 47 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.1311768696826609, 'alpha': 3.155217924149402e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,790] Trial 48 finished with value: 0.0013903478254046716 and parameters: {'booster': 'gbtree', 'lambda': 0.003822733448220564, 'alpha': 0.003043334464338783, 'max_depth': 9, 'eta': 1.250294721559774e-08, 'gamma': 0.696011003606078, 'grow_policy': 'depthwise'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,903] Trial 49 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.33706584829752156, 'alpha': 1.0880696612540967e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,079] Trial 50 finished with value: 0.455156407362528 and parameters: {'booster': 'dart', 'lambda': 0.06496017185526001, 'alpha': 2.1804140721146447e-08, 'max_depth': 2, 'eta': 0.00017650468706026422, 'gamma': 0.0004694151091426447, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.029915736178884e-08, 'skip_drop': 2.1034039629328684e-05}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,192] Trial 51 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.11254929472527452, 'alpha': 5.765544771367046e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,331] Trial 52 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.02135396069531604, 'alpha': 3.50965584514197e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,449] Trial 53 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.2662621023740773, 'alpha': 1.114150190734843e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,562] Trial 54 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.07386493670993022, 'alpha': 1.768235462229777e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,677] Trial 55 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.5383927785796644, 'alpha': 5.753867875186309e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,789] Trial 56 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.15499852309094572, 'alpha': 4.1219665763946747e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,902] Trial 57 finished with value: 0.6219707163863691 and parameters: {'booster': 'gblinear', 'lambda': 1.980988818564679e-05, 'alpha': 1.0987664608778963e-06}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:57,017] Trial 58 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.003515674321615172, 'alpha': 2.1239611426827974e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:57,130] Trial 59 finished with value: 0.6466999579380653 and parameters: {'booster': 'gblinear', 'lambda': 0.00022066623709189373, 'alpha': 1.329806772968925e-07}. Best is trial 32 with value: 0.7581635155117249.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n"
     ]
    }
   ],
   "source": [
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 19:38:31,736] A new study created in memory with name: no-name-fb818b66-fdee-40b1-8f69-a558bed153e5\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:33,026] Trial 0 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00031835665015120484, 'dropout': 0.20654770214911006}. Best is trial 0 with value: 0.5814461133586074.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.581\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.032\n",
      "F1 Macro: 0.745 ± 0.154\n",
      "F1 Weighted: 0.874 ± 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:34,320] Trial 1 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0003994236638881323, 'dropout': 0.31297149922050155}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:35,641] Trial 2 finished with value: 0.5814461133586074 and parameters: {'lr': 0.0020550157776878125, 'dropout': 0.45506523137093347}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:36,932] Trial 3 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0023832168392858106, 'dropout': 0.44985818791023646}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:38,197] Trial 4 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00029998028591521244, 'dropout': 0.16352985288896082}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:39,449] Trial 5 finished with value: 0.7101929043348356 and parameters: {'lr': 0.003514759206662055, 'dropout': 0.3875206144282498}. Best is trial 5 with value: 0.7101929043348356.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.710\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:40,733] Trial 6 finished with value: 0.7290661114709509 and parameters: {'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:42,027] Trial 7 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0037000620150167396, 'dropout': 0.3974109441837972}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:43,296] Trial 8 finished with value: 0.6288118975723532 and parameters: {'lr': 0.001620528316220534, 'dropout': 0.1429762051271858}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:44,620] Trial 9 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0001195833284751336, 'dropout': 0.2314216389186731}. Best is trial 6 with value: 0.7290661114709509.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.5],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n",
      "Best hyperparameters:\n",
      "{'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"knn\")\n",
    "svm_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"svm\")\n",
    "xgb_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.923 ± 0.084\n",
      "F1 Macro: 0.825 ± 0.208\n",
      "F1 Weighted: 0.933 ± 0.072\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "knn_eval.print_best_results()\n",
    "svm_eval.print_best_results()\n",
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.235535  ┆ 0.200897  ┆ N54       ┆ 0     │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.461213  ┆ 0.061081  ┆ N58       ┆ 0     │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.114601  ┆ 0.226098  ┆ N82       ┆ 0     │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.0       ┆ 0.209648  ┆ N83       ┆ 0     │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.017143  ┆ 0.183437  ┆ N84       ┆ 0     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.390905  ┆ 0.471158  ┆ V777      ┆ 1     │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.733365  ┆ 0.766874  ┆ V806      ┆ 1     │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.093215  ┆ 0.636191  ┆ V839      ┆ 1     │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.630781  ┆ 0.495458  ┆ V883      ┆ 1     │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.637763  ┆ 0.268635  ┆ V888      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘,\n",
       " shape: (15, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.403793  ┆ 0.284774  ┆ N60       ┆ 0     │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.417659  ┆ 0.313869  ┆ N70       ┆ 0     │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.117834  ┆ 0.285621  ┆ N85       ┆ 0     │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.374325  ┆ 0.466707  ┆ V108      ┆ 1     │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 0.253248  ┆ 0.812754  ┆ V1297     ┆ 1     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.081684  ┆ 0.283214  ┆ V221      ┆ 1     │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.261181  ┆ 0.413476  ┆ V456      ┆ 1     │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.1762    ┆ 0.701686  ┆ V538      ┆ 1     │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.178761  ┆ 1.105654  ┆ V574      ┆ 1     │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.955394  ┆ 0.193979  ┆ V940      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrna_loader.get_fold(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.770044 ┆ 0.741842 ┆ 0.767158 ┆ 0.87709  │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.628628 ┆ 0.630266 ┆ 0.27247  ┆ 0.548661 │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.417187 ┆ 0.635949 ┆ 0.342004 ┆ 0.649414 │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.791224 ┆ 0.742146 ┆ 0.109209 ┆ 0.684224 │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.467998 ┆ 0.54089  ┆ 0.362278 ┆ 0.494964 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.942295 ┆ 0.816323 ┆ 0.630765 ┆ 0.78563  │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.574101 ┆ 0.724553 ┆ 0.044501 ┆ 0.687679 │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.987056 ┆ 0.689131 ┆ 0.576118 ┆ 0.892948 │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.594205 ┆ 0.57892  ┆ 0.349641 ┆ 0.600517 │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.541005 ┆ 0.44407  ┆ 0.433872 ┆ 0.336804 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " shape: (15, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.34427  ┆ 0.699313 ┆ 0.544814 ┆ 0.635649 │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.296186 ┆ 0.341609 ┆ 0.329612 ┆ 0.375117 │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.591366 ┆ 0.505929 ┆ 0.130364 ┆ 0.352736 │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.765963 ┆ 0.758159 ┆ 0.878573 ┆ 0.923286 │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 1.060318 ┆ 1.127576 ┆ 0.740619 ┆ 1.126258 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.910938 ┆ 0.703334 ┆ 0.682987 ┆ 0.731746 │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.647633 ┆ 1.039966 ┆ 0.69509  ┆ 1.039513 │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.983345 ┆ 0.899831 ┆ 0.183759 ┆ 0.94264  │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.767064 ┆ 0.753895 ┆ 0.422372 ┆ 0.772614 │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.249167 ┆ 0.635731 ┆ 0.254413 ┆ 0.622748 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.190054  ┆ 0.8589    ┆ 0.724728  ┆ 0.796341  ┆ … ┆ 0.792052  ┆ 0.724515  ┆ 0.670447  ┆ 0.877857 │\n",
       " │ 0.131677  ┆ 0.788182  ┆ 0.73847   ┆ 0.566229  ┆ … ┆ 0.570109  ┆ 0.788338  ┆ 0.25199   ┆ 0.445283 │\n",
       " │ 0.94058   ┆ 0.623225  ┆ 0.735662  ┆ 0.41357   ┆ … ┆ 0.39374   ┆ 0.692776  ┆ 0.478255  ┆ 0.630045 │\n",
       " │ 0.805491  ┆ 0.642498  ┆ 0.62087   ┆ 0.43411   ┆ … ┆ 0.656948  ┆ 0.800219  ┆ 0.68736   ┆ 0.821026 │\n",
       " │ 1.0       ┆ 0.789645  ┆ 0.52572   ┆ 0.214973  ┆ … ┆ 0.666039  ┆ 0.897311  ┆ 0.771568  ┆ 0.806792 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.226835  ┆ 0.480819  ┆ 1.0       ┆ 0.766894  ┆ … ┆ 0.52434   ┆ 0.73862   ┆ 0.457018  ┆ 0.690925 │\n",
       " │ 0.123242  ┆ 0.591142  ┆ 0.42922   ┆ 0.684059  ┆ … ┆ 0.802056  ┆ 0.921184  ┆ 0.314539  ┆ 0.94341  │\n",
       " │ 0.559905  ┆ 0.608163  ┆ 0.5361    ┆ 0.619954  ┆ … ┆ 0.674672  ┆ 0.955206  ┆ 0.787847  ┆ 0.672842 │\n",
       " │ 0.138803  ┆ 0.760565  ┆ 0.817056  ┆ 0.712027  ┆ … ┆ 0.848153  ┆ 0.876225  ┆ 0.836959  ┆ 1.0      │\n",
       " │ 0.219329  ┆ 0.813799  ┆ 0.562113  ┆ 0.573218  ┆ … ┆ 0.574968  ┆ 0.769111  ┆ 0.359642  ┆ 0.478356 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " shape: (15, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.20333   ┆ 0.738426  ┆ 0.846229  ┆ 0.688066  ┆ … ┆ 0.359969  ┆ 0.746973  ┆ 0.238121  ┆ 0.522262 │\n",
       " │ 0.191396  ┆ 0.819688  ┆ 0.91776   ┆ 0.82761   ┆ … ┆ 0.460708  ┆ 0.782432  ┆ 0.363974  ┆ 0.661165 │\n",
       " │ 0.949997  ┆ 0.784648  ┆ 0.654279  ┆ 0.11519   ┆ … ┆ 0.471986  ┆ 0.407116  ┆ 0.389313  ┆ 0.551279 │\n",
       " │ 0.047037  ┆ 0.460519  ┆ 0.601395  ┆ 0.754197  ┆ … ┆ 0.749248  ┆ 0.706021  ┆ 0.465903  ┆ 0.671848 │\n",
       " │ 0.560943  ┆ 0.237744  ┆ 0.38922   ┆ 0.4356    ┆ … ┆ 0.834129  ┆ 0.890805  ┆ 0.737512  ┆ 0.667177 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.456982  ┆ 0.455895  ┆ 0.55602   ┆ 0.528333  ┆ … ┆ 0.935791  ┆ 1.077982  ┆ 0.708828  ┆ 1.100682 │\n",
       " │ 0.015711  ┆ 0.132877  ┆ 1.11073   ┆ 0.720927  ┆ … ┆ 0.554755  ┆ 0.955612  ┆ 0.010688  ┆ 0.735628 │\n",
       " │ 0.504371  ┆ 0.511528  ┆ 0.618815  ┆ 0.492415  ┆ … ┆ 0.49578   ┆ 0.901123  ┆ 0.400956  ┆ 0.652603 │\n",
       " │ -0.288737 ┆ 0.580996  ┆ 0.852197  ┆ 0.760822  ┆ … ┆ 0.773837  ┆ 0.856068  ┆ 0.776993  ┆ 0.666524 │\n",
       " │ -0.041422 ┆ 0.525319  ┆ 0.513923  ┆ 0.752942  ┆ … ┆ 0.878347  ┆ 0.836865  ┆ 0.52754   ┆ 0.922727 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catodm = CatOmicDataManager(\n",
    "    omic_data_loaders={\n",
    "        \"mrna\": mrna_loader,\n",
    "        # \"meth\": meth_loader,\n",
    "        \"mirna\": mirna_loader,\n",
    "    },\n",
    "    n_splits=5,\n",
    ")\n",
    "\n",
    "catodm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:58:27,453] A new study created in memory with name: no-name-88f2a81c-17ae-464a-9309-9250ee9deaed\n",
      "[I 2024-11-12 23:58:31,864] Trial 0 finished with value: 0.8225991965179064 and parameters: {}. Best is trial 0 with value: 0.8225991965179064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.823\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.918 ± 0.070\n",
      "F1 Weighted: 0.947 ± 0.047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': np.float64(0.9466666666666667),\n",
       " 'f1_macro': np.float64(0.917909090909091),\n",
       " 'f1_weighted': np.float64(0.9466545454545454),\n",
       " 'acc_std': np.float64(0.04988876515698587),\n",
       " 'f1_macro_std': np.float64(0.06951282657072173),\n",
       " 'f1_weighted_std': np.float64(0.047331376205998074)}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders={\n",
    "            \"mrna\": mrna_loader,\n",
    "            \"mirna\": mirna_loader,\n",
    "        },\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"attention\",\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 100,\n",
    "        \"log_interval\": 101,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200]) torch.Size([200])\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(1) tensor(12.9189)\n",
      "torch.Size([200]) torch.Size([200])\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(12.7027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  feature_names=[2],\n",
       "  omics=[2],\n",
       "  num_relations=6,\n",
       "  y=[74],\n",
       "  train_mask=[74],\n",
       "  test_mask=[74],\n",
       "  val_mask=[74],\n",
       "  mrna={ x=[74, 200] },\n",
       "  mrna_feature={ x=[200, 200] },\n",
       "  mirna={ x=[74, 200] },\n",
       "  mirna_feature={ x=[200, 200] },\n",
       "  (mrna, diff_exp, mrna_feature)={ edge_index=[2, 956] },\n",
       "  (mirna, diff_exp, mirna_feature)={ edge_index=[2, 940] },\n",
       "  (mrna_feature, rev_diff_exp, mrna)={ edge_index=[2, 956] },\n",
       "  (mirna_feature, rev_diff_exp, mirna)={ edge_index=[2, 940] },\n",
       "  (mrna_feature, interacts, mrna_feature)={ edge_index=[2, 293] },\n",
       "  (mirna_feature, regulates, mrna_feature)={ edge_index=[2, 462] }\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "\n",
    "bpdm = BipartiteGraphDataManager(\n",
    "    omic_data_loaders={\n",
    "        \"mrna\": mrna_loader,\n",
    "        \"mirna\": mirna_loader,\n",
    "    },\n",
    "    n_splits=5,\n",
    "    params={\n",
    "        \"diff_exp_thresholds\" : {\n",
    "            \"mrna\": 1.8,\n",
    "            \"mirna\": 1.8,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "data, _, _, _ = bpdm.get_split(0)\n",
    "# params={\n",
    "#     \"graph_style\": \"threshold\",\n",
    "#     \"self_connections\": True,\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1996,  0.0200],\n",
       "        [ 0.1476, -0.0125],\n",
       "        [-0.0169, -0.3833],\n",
       "        [-0.2606, -0.1123],\n",
       "        [ 0.0988,  0.1072],\n",
       "        [ 0.2152,  0.1267],\n",
       "        [ 0.2539,  0.2399],\n",
       "        [-0.1327,  0.1547],\n",
       "        [ 0.1204,  0.0057],\n",
       "        [ 0.0787,  0.0047],\n",
       "        [-0.1699, -0.0654],\n",
       "        [-0.1015, -0.0169],\n",
       "        [-0.1148, -0.1564],\n",
       "        [-0.2891, -0.1228],\n",
       "        [-0.1221, -0.1194],\n",
       "        [-0.2548, -0.1509],\n",
       "        [-0.1392, -0.1450],\n",
       "        [-0.0354, -0.2231],\n",
       "        [-0.2383,  0.0189],\n",
       "        [-0.0243, -0.2815],\n",
       "        [-0.1663,  0.0082],\n",
       "        [ 0.0280, -0.0584],\n",
       "        [ 0.1171,  0.0122],\n",
       "        [ 0.0266, -0.0320],\n",
       "        [-0.1866,  0.0935],\n",
       "        [-0.1981, -0.1071],\n",
       "        [-0.1796, -0.3447],\n",
       "        [-0.0147, -0.2409],\n",
       "        [ 0.1196, -0.2709],\n",
       "        [ 0.0811, -0.2482],\n",
       "        [ 0.1268, -0.0737],\n",
       "        [-0.0107, -0.0359],\n",
       "        [-0.1744, -0.2606],\n",
       "        [-0.2738, -0.0500],\n",
       "        [-0.0365, -0.1182],\n",
       "        [ 0.0636, -0.2654],\n",
       "        [-0.0823, -0.1176],\n",
       "        [-0.1565, -0.2752],\n",
       "        [ 0.0837,  0.2634],\n",
       "        [-0.1940, -0.2055],\n",
       "        [-0.0094, -0.1054],\n",
       "        [-0.0698, -0.2017],\n",
       "        [-0.3152,  0.0972],\n",
       "        [-0.1653,  0.0656],\n",
       "        [ 0.1305,  0.0121],\n",
       "        [-0.0037, -0.4056],\n",
       "        [-0.2522,  0.1519],\n",
       "        [-0.0549, -0.1125],\n",
       "        [ 0.2435, -0.0827],\n",
       "        [-0.1944, -0.0879],\n",
       "        [ 0.0065,  0.0019],\n",
       "        [-0.0183, -0.3525],\n",
       "        [ 0.1464, -0.0199],\n",
       "        [-0.0770, -0.0364],\n",
       "        [-0.0929, -0.2363],\n",
       "        [-0.0214,  0.0484],\n",
       "        [-0.0698, -0.2347],\n",
       "        [ 0.0091, -0.1021],\n",
       "        [ 0.0795,  0.0861],\n",
       "        [-0.2792, -0.2338],\n",
       "        [-0.0985, -0.2449],\n",
       "        [-0.1814, -0.2986],\n",
       "        [ 0.0176,  0.1467],\n",
       "        [-0.1159, -0.0345],\n",
       "        [ 0.1978, -0.0071],\n",
       "        [ 0.2575,  0.2147],\n",
       "        [ 0.0825,  0.0425],\n",
       "        [-0.1977, -0.3105],\n",
       "        [-0.1683, -0.2478],\n",
       "        [-0.0759,  0.0594],\n",
       "        [-0.1281, -0.2853],\n",
       "        [-0.2448, -0.0294],\n",
       "        [-0.0968,  0.0006],\n",
       "        [ 0.0042, -0.1574]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.models.birgat import BiRGAT\n",
    "\n",
    "params = {\n",
    "    \"hidden_channels\": [200, 32, 32, 32, 32],\n",
    "    \"heads\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"attention_dropout\": 0.2,\n",
    "    \"use_proj_module\": False,\n",
    "    \"integrator_type\": \"attention\",\n",
    "    \"proj_dim\" : 64,\n",
    "    \"three_layers\": False\n",
    "}\n",
    "\n",
    "model = BiRGAT(\n",
    "    omic_channels=data.omics,\n",
    "    feature_names=data.feature_names,\n",
    "    relations=list(data.edge_index_dict.keys()),\n",
    "    input_dims={\n",
    "        omic: data.x_dict[omic].shape[1] for omic in data.x_dict.keys()\n",
    "    },\n",
    "    proj_dim=params[\"proj_dim\"],\n",
    "    hidden_channels=params[\"hidden_channels\"],\n",
    "    num_classes=len(torch.unique(data.y)),\n",
    "    heads=params[\"heads\"],\n",
    "    dropout=params[\"dropout\"],\n",
    "    attention_dropout=params[\"attention_dropout\"],\n",
    "    use_proj_module=params[\"use_proj_module\"],\n",
    "    integrator_type=params[\"integrator_type\"],\n",
    "    three_layers=params[\"three_layers\"],\n",
    ")\n",
    "model.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('mrna_feature',\n",
       "  'interacts',\n",
       "  'mrna_feature'): tensor([[  0,   1,   2,   2,   3,   4,   4,   4,   5,   6,   7,   8,   9,  10,\n",
       "           10,  10,  10,  10,  10,  11,  12,  12,  12,  12,  12,  13,  13,  14,\n",
       "           15,  15,  16,  17,  17,  17,  17,  18,  19,  19,  19,  19,  20,  20,\n",
       "           21,  21,  21,  21,  21,  21,  21,  22,  23,  23,  23,  24,  25,  26,\n",
       "           27,  27,  28,  29,  30,  30,  31,  31,  32,  33,  33,  34,  35,  36,\n",
       "           36,  36,  37,  38,  38,  38,  38,  38,  38,  38,  38,  38,  39,  40,\n",
       "           40,  40,  40,  40,  40,  40,  40,  41,  41,  42,  43,  43,  43,  43,\n",
       "           43,  44,  44,  45,  45,  46,  47,  48,  48,  48,  48,  49,  49,  49,\n",
       "           49,  49,  50,  50,  50,  51,  51,  51,  52,  52,  52,  52,  52,  52,\n",
       "           53,  53,  53,  53,  53,  53,  53,  53,  54,  55,  56,  57,  58,  59,\n",
       "           59,  60,  61,  62,  62,  62,  62,  62,  63,  64,  65,  65,  65,  65,\n",
       "           66,  67,  67,  67,  68,  69,  70,  71,  72,  73,  74,  74,  75,  75,\n",
       "           75,  75,  75,  76,  76,  76,  76,  77,  77,  77,  77,  77,  77,  77,\n",
       "           78,  79,  80,  81,  82,  83,  83,  83,  83,  83,  83,  83,  83,  83,\n",
       "           83,  83,  83,  83,  83,  84,  85,  86,  86,  86,  87,  87,  88,  88,\n",
       "           89,  89,  90,  90,  90,  90,  91,  91,  91,  91,  91,  91,  92,  93,\n",
       "           93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
       "           95,  96,  96,  97,  98,  99, 100, 101, 101, 101, 101, 101, 101, 102,\n",
       "          102, 103, 103, 103, 103, 103, 104, 104, 105, 106, 106, 107, 108, 108,\n",
       "          109, 110, 110, 111, 112, 112, 112, 113, 114, 115, 116, 117, 117, 117,\n",
       "          118, 119, 120, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124,\n",
       "          125, 126, 126, 127, 127, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "          128, 128, 128, 129, 130, 131, 132, 133, 133, 134, 135, 135, 136, 137,\n",
       "          138, 138, 138, 138, 138, 138, 138, 138, 138, 139, 139, 139, 139, 140,\n",
       "          140, 140, 141, 141, 141, 142, 142, 143, 143, 144, 144, 144, 144, 144,\n",
       "          145, 146, 147, 147, 147, 147, 148, 148, 149, 150, 150, 151, 152, 152,\n",
       "          153, 153, 153, 153, 153, 153, 154, 155, 156, 157, 157, 157, 157, 157,\n",
       "          158, 159, 159, 159, 159, 159, 159, 159, 160, 160, 160, 161, 162, 163,\n",
       "          164, 165, 165, 166, 166, 167, 168, 168, 168, 169, 169, 170, 170, 171,\n",
       "          171, 172, 173, 173, 173, 174, 174, 174, 174, 174, 175, 176, 177, 177,\n",
       "          177, 177, 178, 178, 178, 178, 178, 179, 180, 180, 180, 180, 181, 181,\n",
       "          181, 181, 181, 182, 183, 184, 185, 186, 187, 188, 188, 188, 189, 190,\n",
       "          191, 192, 192, 192, 193, 194, 195, 196, 197, 197, 198, 199, 199],\n",
       "         [  0,   1,   2, 167,   3,   4,  65,  67,   5,   6,   7,   8,   9,  10,\n",
       "           21,  93, 144, 153, 159,  11,  12,  50,  75, 169, 177,  13,  52,  14,\n",
       "           15, 165,  16,  17,  19,  48, 139,  18,  17,  19,  48, 139,  20,  62,\n",
       "           10,  21,  53,  93, 144, 153, 159,  22,  23, 163, 198,  24,  25,  26,\n",
       "           27, 168,  28,  29,  30, 126,  31, 147,  32,  33, 110,  34,  35,  36,\n",
       "          136, 194,  37,   0,  33,  35,  38, 147, 169, 179, 187, 198,  39,  40,\n",
       "           43,  44,  50,  52,  70,  83, 110,  41, 140,  42,  40,  43,  83, 136,\n",
       "          169,  40,  44,  45, 173,  46,  47,  17,  19,  48, 139,  49,  83,  94,\n",
       "          128, 167,  50,  52, 146,  51,  76,  93,  40,  50,  52,  83, 117, 147,\n",
       "           21,  53,  93, 123, 153, 159, 181, 192,  54,  55,  56,  57,  58,  59,\n",
       "           83,  60,  61,  18,  20,  62, 134, 147,  63,  64,   7,  22,  65, 192,\n",
       "           66,   4,  67, 180,  68,  69,  70,  71,  72,  73,  74, 171,  14,  50,\n",
       "           75, 140, 199,  65,  76,  86, 102,  77,  83,  91,  94, 103, 128, 174,\n",
       "           78,  79,  80,  81,  82,  40,  43,  49,  52,  59,  77,  83,  91,  94,\n",
       "          101, 112, 128, 138, 181,  84,  85,   7,  65,  86,  87, 148,  88, 103,\n",
       "           89, 152,  18,  90, 147, 170,  77,  83,  91,  94, 128, 138,  92,  10,\n",
       "           21,  53,  93, 144, 159,  49,  77,  83,  91,  94, 128, 138, 174, 180,\n",
       "           95,  96, 102,  97,  98,  99, 100,  83, 101, 103, 128, 138, 157,  96,\n",
       "          102,  77,  88, 101, 103, 157, 104, 137, 105,   2, 106, 107, 108, 140,\n",
       "          109,  33, 110, 111,  83, 112, 138, 113, 114, 115, 116,  52, 117, 147,\n",
       "          118, 119, 120, 121,  19,  50, 110, 122,  21,  53, 123, 181, 124, 142,\n",
       "          125,  30, 126, 127, 142,  18,  49,  77,  83,  91,  94, 101, 122, 128,\n",
       "          138, 149, 157, 129, 130, 131, 132,  19, 133, 134, 135, 169, 136, 137,\n",
       "           43,  83,  91,  94, 101, 112, 128, 138, 157,  17,  19,  48, 139,  75,\n",
       "          108, 140, 141, 174, 192, 127, 142, 140, 143,  10,  21,  93, 144, 159,\n",
       "          145, 146,  43,  52, 117, 147,  87, 148, 149,   9, 150, 151,  89, 152,\n",
       "           10,  21,  53, 153, 159, 192, 154, 155, 156, 101, 103, 128, 138, 157,\n",
       "          158,  10,  21,  53,  93, 144, 153, 159,  50, 136, 160, 161, 162, 163,\n",
       "          164,  15, 165,  86, 166, 167,  27, 166, 168,  12, 169,  90, 170,  74,\n",
       "          171, 172,  45, 173, 179,  74,  77,  94, 141, 174, 175, 176,  50,  75,\n",
       "          140, 177,  64, 104, 172, 178, 198, 179,  67,  94, 104, 180,  53,  83,\n",
       "          110, 123, 181, 182, 183, 184, 185, 186, 187, 116, 133, 188, 189, 190,\n",
       "          191,  53, 153, 192, 193, 194, 195, 196, 120, 197, 198,  75, 199]]),\n",
       " ('mirna_feature',\n",
       "  'regulates',\n",
       "  'mrna_feature'): tensor([[  1,   5,   5,   5,   5,   5,   5,   5,   5,   6,   6,   6,   6,   6,\n",
       "            7,   7,   7,   7,   7,   7,   7,   7,   7,   8,   8,  10,  10,  10,\n",
       "           10,  10,  10,  12,  12,  12,  13,  13,  16,  16,  17,  17,  17,  17,\n",
       "           17,  17,  17,  17,  17,  17,  17,  19,  19,  19,  19,  19,  19,  22,\n",
       "           22,  22,  26,  26,  26,  27,  27,  27,  28,  28,  29,  29,  29,  29,\n",
       "           29,  29,  29,  30,  30,  30,  31,  31,  32,  32,  34,  34,  34,  34,\n",
       "           34,  34,  35,  35,  35,  35,  35,  40,  41,  42,  42,  42,  42,  42,\n",
       "           43,  43,  45,  46,  46,  47,  47,  52,  52,  52,  52,  52,  52,  54,\n",
       "           54,  54,  54,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,\n",
       "           55,  56,  56,  56,  56,  56,  56,  57,  57,  59,  59,  59,  61,  61,\n",
       "           61,  63,  64,  64,  64,  64,  64,  64,  64,  64,  65,  66,  66,  67,\n",
       "           68,  69,  69,  69,  70,  70,  71,  71,  71,  71,  71,  71,  71,  74,\n",
       "           74,  74,  74,  74,  75,  75,  75,  76,  76,  76,  77,  78,  79,  79,\n",
       "           79,  79,  79,  79,  79,  79,  79,  79,  79,  79,  79,  81,  81,  81,\n",
       "           81,  81,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,\n",
       "           82,  83,  85,  87,  87,  87,  89,  89,  89,  90,  93,  93,  93,  93,\n",
       "           93,  93,  94,  94,  95,  95,  96,  98, 101, 101, 101, 101, 101, 101,\n",
       "          101, 103, 104, 105, 105, 105, 105, 105, 105, 106, 108, 108, 108, 108,\n",
       "          108, 108, 108, 108, 108, 111, 111, 111, 112, 113, 116, 116, 117, 117,\n",
       "          117, 118, 118, 119, 120, 121, 121, 121, 121, 122, 124, 124, 124, 124,\n",
       "          124, 126, 126, 127, 127, 127, 127, 127, 127, 128, 128, 128, 128, 128,\n",
       "          128, 128, 128, 129, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
       "          131, 131, 132, 132, 133, 133, 135, 135, 135, 137, 138, 138, 141, 141,\n",
       "          141, 141, 141, 141, 142, 143, 143, 143, 146, 147, 147, 147, 147, 147,\n",
       "          147, 150, 151, 151, 151, 151, 152, 152, 153, 153, 153, 153, 155, 155,\n",
       "          155, 155, 155, 155, 156, 160, 160, 162, 163, 163, 163, 164, 165, 167,\n",
       "          167, 167, 167, 167, 167, 167, 168, 168, 168, 168, 168, 168, 168, 168,\n",
       "          168, 169, 169, 169, 169, 169, 169, 170, 171, 172, 172, 172, 172, 172,\n",
       "          173, 173, 173, 173, 173, 173, 176, 177, 177, 177, 177, 177, 178, 180,\n",
       "          180, 180, 180, 180, 180, 180, 180, 180, 180, 181, 183, 183, 183, 185,\n",
       "          185, 187, 187, 188, 190, 191, 193, 193, 193, 193, 193, 193, 193, 193,\n",
       "          193, 193, 193, 193, 193, 193, 194, 194, 194, 194, 195, 195, 195, 195,\n",
       "          196, 196, 197, 197, 197, 197, 197, 197, 198, 198, 198, 198, 198, 198],\n",
       "         [154,   8,  67, 104, 110, 142, 169, 175, 190,  52,  84, 133, 154, 182,\n",
       "            0,  40, 104, 118, 127, 143, 156, 167, 197,  29, 192,  20,  41,  71,\n",
       "          130, 140, 142,   0,  24, 104,  81, 127,   2,  31,  20,  35,  48,  51,\n",
       "           52,  82, 119, 131, 149, 175, 180,   0,  30,  45,  64, 141, 180,  12,\n",
       "           84, 130,  41,  64, 169,   2,  39, 127, 170, 182,  81,  94, 135, 136,\n",
       "          150, 154, 198,   4, 119, 189,  83, 141,  18,  91,  27,  51,  64, 131,\n",
       "          139, 156,   8,  47, 106, 132, 151,  27,  39,  41, 104, 108, 180, 187,\n",
       "           94, 174,  80,  31,  57,  35, 189,   8,  18,  27, 134, 143, 190,   9,\n",
       "          113, 175, 190,   9,  12,  24,  47,  55,  75,  78, 113, 134, 142, 156,\n",
       "          162,  30,  35,  64, 145, 169, 190,  45, 197,  28,  94, 136,  14, 113,\n",
       "          147, 117,  18,  24,  79, 103, 114, 145, 180, 190, 101, 123, 153, 119,\n",
       "           93,  58,  71, 117,  67, 134,  58,  98, 104, 107, 135, 145, 186,  15,\n",
       "          104, 106, 168, 189,  77, 154, 156,  11,  39,  91,  45,  59,   0,  24,\n",
       "           30,  41,  51,  64,  77, 103, 131, 143, 147, 148, 190,  36,  91, 104,\n",
       "          180, 197,  24,  30,  41,  43,  49,  75,  94, 104, 108, 131, 169, 174,\n",
       "          180, 145,  24,  14,  94, 180,  77, 133, 148,  30,  30,  32,  36,  41,\n",
       "           64, 113, 117, 154,  30, 113, 182,  49,  41,  46,  67,  77,  94, 104,\n",
       "          176,  59,  35,  79, 117, 143, 167, 170, 181, 135,   2,  24,  30,  35,\n",
       "           41,  74, 131, 182, 190,   2,  50, 104, 180, 180,  33,  39,  44, 140,\n",
       "          180,  59, 141, 135, 141,  33,  36,  85, 142,  33,  41,  46,  67,  77,\n",
       "          104, 128, 165,  47,  64,  94, 103, 104, 156,  14,  81, 124, 163, 169,\n",
       "          174, 179, 197,  33,  24,  30,  43,  49,  67,  94, 113, 122, 131, 148,\n",
       "          162, 184,  39,  50,   2, 148,  81, 136, 154, 189,  64, 190,  33, 127,\n",
       "          145, 162, 180, 190, 154, 148, 149, 198,   2,  55,  67,  94, 114, 145,\n",
       "          163,  64,  20,  42,  52, 156,  18,  39,  41,  78, 104, 149,  20,  41,\n",
       "           71, 130, 140, 142, 173,  77, 131, 180,  50,  87, 128,  31, 134,   2,\n",
       "           41, 108, 119, 153, 156, 162,   2,  51,  77, 104, 107, 126, 131, 156,\n",
       "          174,  13,  34,  85, 117, 153, 180,   0, 142,  35,  51,  64,  94, 127,\n",
       "            0,   2,  57,  78,  92, 142, 127,  77, 145, 156, 169, 172, 190,   2,\n",
       "           30,  51,  64,  74,  78, 117, 127, 145, 181, 197,  39,  67, 141,  24,\n",
       "          175,  64, 180, 197,  78,  31,   2,   9,  30,  55,  67,  77,  78,  88,\n",
       "          117, 131, 135, 156, 180, 199,  29,  42,  95, 165,  90, 128, 132, 154,\n",
       "           64, 104,  31,  58,  67, 108, 119, 127,  37,  39,  64,  84, 118, 180]])}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,   2,   3,   4,   4,   4,   5,   6,   7,   8,   9,  10,\n",
       "          10,  10,  10,  10,  10,  11,  12,  12,  12,  12,  12,  13,  13,  14,\n",
       "          15,  15,  16,  17,  17,  17,  17,  18,  19,  19,  19,  19,  20,  20,\n",
       "          21,  21,  21,  21,  21,  21,  21,  22,  23,  23,  23,  24,  25,  26,\n",
       "          27,  27,  28,  29,  30,  30,  31,  31,  32,  33,  33,  34,  35,  36,\n",
       "          36,  36,  37,  38,  38,  38,  38,  38,  38,  38,  38,  38,  39,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40,  41,  41,  42,  43,  43,  43,  43,\n",
       "          43,  44,  44,  45,  45,  46,  47,  48,  48,  48,  48,  49,  49,  49,\n",
       "          49,  49,  50,  50,  50,  51,  51,  51,  52,  52,  52,  52,  52,  52,\n",
       "          53,  53,  53,  53,  53,  53,  53,  53,  54,  55,  56,  57,  58,  59,\n",
       "          59,  60,  61,  62,  62,  62,  62,  62,  63,  64,  65,  65,  65,  65,\n",
       "          66,  67,  67,  67,  68,  69,  70,  71,  72,  73,  74,  74,  75,  75,\n",
       "          75,  75,  75,  76,  76,  76,  76,  77,  77,  77,  77,  77,  77,  77,\n",
       "          78,  79,  80,  81,  82,  83,  83,  83,  83,  83,  83,  83,  83,  83,\n",
       "          83,  83,  83,  83,  83,  84,  85,  86,  86,  86,  87,  87,  88,  88,\n",
       "          89,  89,  90,  90,  90,  90,  91,  91,  91,  91,  91,  91,  92,  93,\n",
       "          93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
       "          95,  96,  96,  97,  98,  99, 100, 101, 101, 101, 101, 101, 101, 102,\n",
       "         102, 103, 103, 103, 103, 103, 104, 104, 105, 106, 106, 107, 108, 108,\n",
       "         109, 110, 110, 111, 112, 112, 112, 113, 114, 115, 116, 117, 117, 117,\n",
       "         118, 119, 120, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124,\n",
       "         125, 126, 126, 127, 127, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "         128, 128, 128, 129, 130, 131, 132, 133, 133, 134, 135, 135, 136, 137,\n",
       "         138, 138, 138, 138, 138, 138, 138, 138, 138, 139, 139, 139, 139, 140,\n",
       "         140, 140, 141, 141, 141, 142, 142, 143, 143, 144, 144, 144, 144, 144,\n",
       "         145, 146, 147, 147, 147, 147, 148, 148, 149, 150, 150, 151, 152, 152,\n",
       "         153, 153, 153, 153, 153, 153, 154, 155, 156, 157, 157, 157, 157, 157,\n",
       "         158, 159, 159, 159, 159, 159, 159, 159, 160, 160, 160, 161, 162, 163,\n",
       "         164, 165, 165, 166, 166, 167, 168, 168, 168, 169, 169, 170, 170, 171,\n",
       "         171, 172, 173, 173, 173, 174, 174, 174, 174, 174, 175, 176, 177, 177,\n",
       "         177, 177, 178, 178, 178, 178, 178, 179, 180, 180, 180, 180, 181, 181,\n",
       "         181, 181, 181, 182, 183, 184, 185, 186, 187, 188, 188, 188, 189, 190,\n",
       "         191, 192, 192, 192, 193, 194, 195, 196, 197, 197, 198, 199, 199],\n",
       "        [  0,   1,   2, 167,   3,   4,  65,  67,   5,   6,   7,   8,   9,  10,\n",
       "          21,  93, 144, 153, 159,  11,  12,  50,  75, 169, 177,  13,  52,  14,\n",
       "          15, 165,  16,  17,  19,  48, 139,  18,  17,  19,  48, 139,  20,  62,\n",
       "          10,  21,  53,  93, 144, 153, 159,  22,  23, 163, 198,  24,  25,  26,\n",
       "          27, 168,  28,  29,  30, 126,  31, 147,  32,  33, 110,  34,  35,  36,\n",
       "         136, 194,  37,   0,  33,  35,  38, 147, 169, 179, 187, 198,  39,  40,\n",
       "          43,  44,  50,  52,  70,  83, 110,  41, 140,  42,  40,  43,  83, 136,\n",
       "         169,  40,  44,  45, 173,  46,  47,  17,  19,  48, 139,  49,  83,  94,\n",
       "         128, 167,  50,  52, 146,  51,  76,  93,  40,  50,  52,  83, 117, 147,\n",
       "          21,  53,  93, 123, 153, 159, 181, 192,  54,  55,  56,  57,  58,  59,\n",
       "          83,  60,  61,  18,  20,  62, 134, 147,  63,  64,   7,  22,  65, 192,\n",
       "          66,   4,  67, 180,  68,  69,  70,  71,  72,  73,  74, 171,  14,  50,\n",
       "          75, 140, 199,  65,  76,  86, 102,  77,  83,  91,  94, 103, 128, 174,\n",
       "          78,  79,  80,  81,  82,  40,  43,  49,  52,  59,  77,  83,  91,  94,\n",
       "         101, 112, 128, 138, 181,  84,  85,   7,  65,  86,  87, 148,  88, 103,\n",
       "          89, 152,  18,  90, 147, 170,  77,  83,  91,  94, 128, 138,  92,  10,\n",
       "          21,  53,  93, 144, 159,  49,  77,  83,  91,  94, 128, 138, 174, 180,\n",
       "          95,  96, 102,  97,  98,  99, 100,  83, 101, 103, 128, 138, 157,  96,\n",
       "         102,  77,  88, 101, 103, 157, 104, 137, 105,   2, 106, 107, 108, 140,\n",
       "         109,  33, 110, 111,  83, 112, 138, 113, 114, 115, 116,  52, 117, 147,\n",
       "         118, 119, 120, 121,  19,  50, 110, 122,  21,  53, 123, 181, 124, 142,\n",
       "         125,  30, 126, 127, 142,  18,  49,  77,  83,  91,  94, 101, 122, 128,\n",
       "         138, 149, 157, 129, 130, 131, 132,  19, 133, 134, 135, 169, 136, 137,\n",
       "          43,  83,  91,  94, 101, 112, 128, 138, 157,  17,  19,  48, 139,  75,\n",
       "         108, 140, 141, 174, 192, 127, 142, 140, 143,  10,  21,  93, 144, 159,\n",
       "         145, 146,  43,  52, 117, 147,  87, 148, 149,   9, 150, 151,  89, 152,\n",
       "          10,  21,  53, 153, 159, 192, 154, 155, 156, 101, 103, 128, 138, 157,\n",
       "         158,  10,  21,  53,  93, 144, 153, 159,  50, 136, 160, 161, 162, 163,\n",
       "         164,  15, 165,  86, 166, 167,  27, 166, 168,  12, 169,  90, 170,  74,\n",
       "         171, 172,  45, 173, 179,  74,  77,  94, 141, 174, 175, 176,  50,  75,\n",
       "         140, 177,  64, 104, 172, 178, 198, 179,  67,  94, 104, 180,  53,  83,\n",
       "         110, 123, 181, 182, 183, 184, 185, 186, 187, 116, 133, 188, 189, 190,\n",
       "         191,  53, 153, 192, 193, 194, 195, 196, 120, 197, 198,  75, 199]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"mrna_feature\", \"interacts\", \"mrna_feature\"].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrna': tensor([[ 0.1901,  0.8589,  0.7247,  ...,  0.4696,  0.2355,  0.2009],\n",
       "         [ 0.1317,  0.7882,  0.7385,  ...,  0.8404,  0.4612,  0.0611],\n",
       "         [ 0.9406,  0.6232,  0.7357,  ...,  0.8423,  0.1146,  0.2261],\n",
       "         ...,\n",
       "         [ 0.5044,  0.5115,  0.6188,  ...,  0.3666,  0.1762,  0.7017],\n",
       "         [-0.2887,  0.5810,  0.8522,  ...,  0.3074,  0.1788,  1.1057],\n",
       "         [-0.0414,  0.5253,  0.5139,  ...,  0.1934,  0.9554,  0.1940]]),\n",
       " 'mirna': tensor([[ 0.8021,  0.7240,  0.8759,  ...,  0.7245,  0.6704,  0.8779],\n",
       "         [ 0.6343,  0.6804,  0.6110,  ...,  0.7883,  0.2520,  0.4453],\n",
       "         [ 0.4353,  0.6257,  0.7742,  ...,  0.6928,  0.4783,  0.6300],\n",
       "         ...,\n",
       "         [ 0.6901,  0.5982,  0.8047,  ...,  0.9011,  0.4010,  0.6526],\n",
       "         [ 0.7930,  0.8460,  0.6499,  ...,  0.8561,  0.7770,  0.6665],\n",
       "         [ 0.9770, -0.0268,  0.6509,  ...,  0.8369,  0.5275,  0.9227]])}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_mrna_interactions_db = pl.read_csv(\"interaction_data/mirna_mrna_interactions_DB.csv\")\n",
    "mmirnas = mirna_mrna_interactions_db[\"mirna\"].to_list()\n",
    "mirna_gene_names = [\"\".join(mirna.split(\"-\")[1:3]).upper() for mirna in mmirnas]\n",
    "mirna_mrna_interactions_db.with_columns(\n",
    "    pl.Series(\"mirna\", mirna_gene_names)\n",
    ").select(\"mirna\", \"gene\").write_csv(\"interaction_data/mirna_genes_mrna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
