{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-11-21 00:37:21,081] A new study created in memory with name: no-name-94094ee1-ba90-428c-976e-5b569c19bcc2\n",
      "[I 2024-11-21 00:37:21,266] Trial 0 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.545253957603188.\n",
      "[I 2024-11-21 00:37:21,408] Trial 1 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.545\n",
      "Best model performance:\n",
      "Accuracy: 0.880 ± 0.065\n",
      "F1 Macro: 0.721 ± 0.176\n",
      "F1 Weighted: 0.859 ± 0.076\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8, 'f1_macro': np.float64(0.64), 'f1_weighted': np.float64(0.7840000000000001)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.4642857142857143), 'f1_weighted': np.float64(0.8047619047619048)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:21,547] Trial 2 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,685] Trial 3 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 12}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,822] Trial 4 finished with value: 0.6024938699160863 and parameters: {'n_neighbors': 17}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,958] Trial 5 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,099] Trial 6 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,235] Trial 7 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 12}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,373] Trial 8 finished with value: 0.5716516161494184 and parameters: {'n_neighbors': 16}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,512] Trial 9 finished with value: 0.40415587923136775 and parameters: {'n_neighbors': 2}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,651] Trial 10 finished with value: 0.40415587923136775 and parameters: {'n_neighbors': 2}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,789] Trial 11 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,927] Trial 12 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,068] Trial 13 finished with value: 0.4728693782897456 and parameters: {'n_neighbors': 20}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,206] Trial 14 finished with value: 0.6181128117837651 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,363] Trial 15 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 9}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,503] Trial 16 finished with value: 0.5114959395093938 and parameters: {'n_neighbors': 4}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,643] Trial 17 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 10}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,782] Trial 18 finished with value: 0.6181128117837651 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,922] Trial 19 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 10}. Best is trial 1 with value: 0.6631836987350632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()\n",
    "# knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:23,944] A new study created in memory with name: no-name-fc278aab-919a-4906-b8b0-a7f9d091292e\n",
      "[I 2024-11-21 00:37:24,100] Trial 0 finished with value: 0.6812403790638734 and parameters: {'C': 0.016675213956196765, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6812403790638734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.681\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.053\n",
      "F1 Macro: 0.835 ± 0.098\n",
      "F1 Weighted: 0.901 ± 0.059\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:24,240] Trial 1 finished with value: 0.6812403790638734 and parameters: {'C': 0.02849950426647128, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6812403790638734.\n",
      "[I 2024-11-21 00:37:24,380] Trial 2 finished with value: 0.7039337563024542 and parameters: {'C': 0.16072762000291213, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,523] Trial 3 finished with value: 0.6682316917536709 and parameters: {'C': 0.8456778159715628, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7039337563024542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:24,645] Trial 4 finished with value: 0.7039337563024542 and parameters: {'C': 0.01642746925590834, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,767] Trial 5 finished with value: 0.6192439604955923 and parameters: {'C': 0.011471521424410965, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,914] Trial 6 finished with value: 0.6682316917536709 and parameters: {'C': 2.0583204175797025, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:25,041] Trial 7 finished with value: 0.7039337563024542 and parameters: {'C': 0.03561454122625204, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:25,189] Trial 8 finished with value: 0.7172503758053912 and parameters: {'C': 0.16498184393238047, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,338] Trial 9 finished with value: 0.6682316917536709 and parameters: {'C': 0.8051797736707758, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.717\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.853 ± 0.097\n",
      "F1 Weighted: 0.914 ± 0.057\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.9386666666666666)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:25,489] Trial 10 finished with value: 0.6682316917536709 and parameters: {'C': 6.8692960511070815, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,628] Trial 11 finished with value: 0.7039337563024542 and parameters: {'C': 0.12368378886541774, 'class_weight': None}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,774] Trial 12 finished with value: 0.7172503758053912 and parameters: {'C': 0.14675039305810655, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,935] Trial 13 finished with value: 0.7172503758053912 and parameters: {'C': 0.06619879441586395, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,080] Trial 14 finished with value: 0.7172503758053912 and parameters: {'C': 0.33863485362653023, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,228] Trial 15 finished with value: 0.7172503758053912 and parameters: {'C': 0.2738085590206223, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,372] Trial 16 finished with value: 0.7172503758053912 and parameters: {'C': 0.08140982990470529, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,519] Trial 17 finished with value: 0.6682316917536709 and parameters: {'C': 0.6466183948186608, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,670] Trial 18 finished with value: 0.6682316917536709 and parameters: {'C': 3.2695257580816848, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,812] Trial 19 finished with value: 0.7172503758053912 and parameters: {'C': 0.32396317734373475, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,959] Trial 20 finished with value: 0.7172503758053912 and parameters: {'C': 0.1773168287534853, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,124] Trial 21 finished with value: 0.6812403790638734 and parameters: {'C': 0.05509301175131984, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,268] Trial 22 finished with value: 0.7172503758053912 and parameters: {'C': 0.07423983074574342, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,411] Trial 23 finished with value: 0.6812403790638734 and parameters: {'C': 0.04166154432477624, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,555] Trial 24 finished with value: 0.7172503758053912 and parameters: {'C': 0.10242699193831091, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,700] Trial 25 finished with value: 0.7172503758053912 and parameters: {'C': 0.20537916953085308, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,846] Trial 26 finished with value: 0.6682316917536709 and parameters: {'C': 0.4921568656578518, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,992] Trial 27 finished with value: 0.6682316917536709 and parameters: {'C': 1.4224972711541313, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:28,136] Trial 28 finished with value: 0.7581635155117249 and parameters: {'C': 0.06285217601548478, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,269] Trial 29 finished with value: 0.6812403790638734 and parameters: {'C': 0.018593943994173228, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:28,416] Trial 30 finished with value: 0.7172503758053912 and parameters: {'C': 0.12500130112484123, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,554] Trial 31 finished with value: 0.7581635155117249 and parameters: {'C': 0.06105273633037403, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,693] Trial 32 finished with value: 0.6812403790638734 and parameters: {'C': 0.027728082850775628, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,839] Trial 33 finished with value: 0.6812403790638734 and parameters: {'C': 0.05589008364042942, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,000] Trial 34 finished with value: 0.7172503758053912 and parameters: {'C': 0.2011691749306423, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,136] Trial 35 finished with value: 0.6812403790638734 and parameters: {'C': 0.027343074353882547, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,278] Trial 36 finished with value: 0.7039337563024542 and parameters: {'C': 0.12093639130266076, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,419] Trial 37 finished with value: 0.6812403790638734 and parameters: {'C': 0.04601041828230094, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,547] Trial 38 finished with value: 0.7039337563024542 and parameters: {'C': 0.019383673971694864, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,690] Trial 39 finished with value: 0.7172503758053912 and parameters: {'C': 0.08622059323109191, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,833] Trial 40 finished with value: 0.6682316917536709 and parameters: {'C': 0.4499493036333314, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,975] Trial 41 finished with value: 0.7172503758053912 and parameters: {'C': 0.06928414273712898, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,119] Trial 42 finished with value: 0.7172503758053912 and parameters: {'C': 0.16527710413971605, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,252] Trial 43 finished with value: 0.6812403790638734 and parameters: {'C': 0.012212028485024657, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,393] Trial 44 finished with value: 0.6812403790638734 and parameters: {'C': 0.033225608242794136, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,544] Trial 45 finished with value: 0.7172503758053912 and parameters: {'C': 0.25708380463560365, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,681] Trial 46 finished with value: 0.7039337563024542 and parameters: {'C': 0.1337825759029321, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,823] Trial 47 finished with value: 0.6812403790638734 and parameters: {'C': 0.05651684844138084, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,989] Trial 48 finished with value: 0.7172503758053912 and parameters: {'C': 0.07768576960502818, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:31,136] Trial 49 finished with value: 0.7172503758053912 and parameters: {'C': 0.09745557521539507, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_disease/mrna_mirna_te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:31,173] A new study created in memory with name: no-name-f5967f9c-326a-4aee-bb2e-645187e52939\n",
      "[I 2024-11-21 00:37:31,455] Trial 0 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 0.777042272438913, 'alpha': 0.007012163504194617, 'max_depth': 5, 'eta': 3.366922110430363e-05, 'gamma': 8.229368747919227e-07, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5844534524426352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.584\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.065\n",
      "F1 Macro: 0.761 ± 0.170\n",
      "F1 Weighted: 0.874 ± 0.070\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8, 'f1_macro': np.float64(0.7204968944099379), 'f1_weighted': np.float64(0.8099378881987577)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8, 'f1_macro': np.float64(0.4444444444444444), 'f1_weighted': np.float64(0.7703703703703704)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:31,700] Trial 1 finished with value: 0.43246561009234025 and parameters: {'booster': 'gbtree', 'lambda': 6.907136835959009e-06, 'alpha': 1.0143730016435205e-08, 'max_depth': 2, 'eta': 0.002099548308135269, 'gamma': 0.29869310861813714, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5844534524426352.\n",
      "[I 2024-11-21 00:37:31,956] Trial 2 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 0.0004247114677277087, 'alpha': 9.511835775344175e-07, 'max_depth': 4, 'eta': 2.5091288695504218e-08, 'gamma': 0.007636488371795125, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.6728264260126283, 'skip_drop': 0.04105023768061634}. Best is trial 0 with value: 0.5844534524426352.\n",
      "[I 2024-11-21 00:37:32,153] Trial 3 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.016289389067311696, 'alpha': 1.379929016068266e-06}. Best is trial 3 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:32,454] Trial 4 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 6.427644198938857e-07, 'alpha': 0.3135826508026116, 'max_depth': 3, 'eta': 1.7716798332590427e-06, 'gamma': 0.0007902184802045573, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:32,736] Trial 5 finished with value: 0.48162878238196427 and parameters: {'booster': 'dart', 'lambda': 0.1313406697070976, 'alpha': 1.638006863412106e-05, 'max_depth': 7, 'eta': 0.025799967421881054, 'gamma': 2.1564641889698135e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.945806173024392e-06, 'skip_drop': 1.1842930673809874e-06}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:32,974] Trial 6 finished with value: 0.3829003597127381 and parameters: {'booster': 'dart', 'lambda': 0.0005164639182912427, 'alpha': 0.15150398599441617, 'max_depth': 2, 'eta': 6.88180581792351e-07, 'gamma': 2.9300678019603802e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.2592507872069602e-05, 'skip_drop': 0.012425098872306271}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,247] Trial 7 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.0021990430148398615, 'alpha': 6.785036790894485e-08, 'max_depth': 8, 'eta': 0.15488864925113413, 'gamma': 1.8650462303090628e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.7394278999267716, 'skip_drop': 0.012013624858241284}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,432] Trial 8 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.0013531430853130294, 'alpha': 9.926188862054352e-05}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,683] Trial 9 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.2471937292414054, 'alpha': 4.5028907372294134e-08, 'max_depth': 9, 'eta': 0.0016584275770170109, 'gamma': 4.6121080626099866e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.01649900332753479, 'skip_drop': 0.13924496883626883}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,910] Trial 10 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.0925298540779154e-07, 'alpha': 1.7653402312068486e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,111] Trial 11 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 3.728824666665131e-08, 'alpha': 1.6807039964666046e-06}. Best is trial 10 with value: 0.7766534866310313.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.777\n",
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.060\n",
      "F1 Macro: 0.892 ± 0.093\n",
      "F1 Weighted: 0.933 ± 0.060\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:34,300] Trial 12 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.176720274615829e-08, 'alpha': 4.04284334741871e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,538] Trial 13 finished with value: 0.6013127244639374 and parameters: {'booster': 'gblinear', 'lambda': 1.9535657640285294e-08, 'alpha': 0.0009778525679663705}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,705] Trial 14 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.2804431138058544e-07, 'alpha': 3.1156566564358095e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,949] Trial 15 finished with value: 0.6119104221152568 and parameters: {'booster': 'gblinear', 'lambda': 1.2334231778132507e-05, 'alpha': 3.04085084646386e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,133] Trial 16 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.7714031111206863e-07, 'alpha': 0.0006308075490266575}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,360] Trial 17 finished with value: 0.7264254257106565 and parameters: {'booster': 'gblinear', 'lambda': 8.122705212395857e-06, 'alpha': 7.349808410166707e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,601] Trial 18 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 6.077244040722739e-08, 'alpha': 2.2773194926380306e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,814] Trial 19 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.6131875038114226e-06, 'alpha': 0.00030970623691254197}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,073] Trial 20 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 3.620622275794472e-05, 'alpha': 1.983031001800808e-06, 'max_depth': 6, 'eta': 2.204014653967302e-08, 'gamma': 0.6964502540891363, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,273] Trial 21 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.175520283658004e-07, 'alpha': 3.3561015437436135e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,464] Trial 22 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.1335878433150491e-06, 'alpha': 1.7759347340935225e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,663] Trial 23 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 6.28491510005258e-08, 'alpha': 1.2281821436667312e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,827] Trial 24 finished with value: 0.643037106032834 and parameters: {'booster': 'gblinear', 'lambda': 2.7992144006805795e-07, 'alpha': 4.1935593704490814e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,990] Trial 25 finished with value: 0.7580962043744017 and parameters: {'booster': 'gblinear', 'lambda': 4.433572626082405e-08, 'alpha': 7.83185887778193e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,180] Trial 26 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.9887479450260334e-06, 'alpha': 6.082027733282836e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,370] Trial 27 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 6.528422466079318e-05, 'alpha': 5.8426795296331335e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,558] Trial 28 finished with value: 0.4745604857943926 and parameters: {'booster': 'gbtree', 'lambda': 1.1973695217119165e-07, 'alpha': 3.252554884290843e-06, 'max_depth': 1, 'eta': 2.851217189448492e-05, 'gamma': 0.01092396566733931, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,753] Trial 29 finished with value: 0.5350135409063966 and parameters: {'booster': 'gblinear', 'lambda': 1.0577115996797466e-08, 'alpha': 0.016288192694524445}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,024] Trial 30 finished with value: 0.5052171643290275 and parameters: {'booster': 'gbtree', 'lambda': 5.800270817487896e-07, 'alpha': 0.0030527715355163853, 'max_depth': 9, 'eta': 0.6832464382719352, 'gamma': 1.0287459993048287e-05, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,206] Trial 31 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.3370056135249796e-08, 'alpha': 2.2102834525156941e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,407] Trial 32 finished with value: 0.7393866992524499 and parameters: {'booster': 'gblinear', 'lambda': 7.364211905515403e-08, 'alpha': 1.3991698679819874e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,625] Trial 33 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 2.8502650681579386e-08, 'alpha': 1.6524907413237376e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,782] Trial 34 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.064955076031104e-06, 'alpha': 9.567492716913095e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,027] Trial 35 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.885541595444948e-07, 'alpha': 4.88186700879917e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,288] Trial 36 finished with value: 0.5591177623513526 and parameters: {'booster': 'gbtree', 'lambda': 8.255756802988455e-07, 'alpha': 1.6575023223409445e-06, 'max_depth': 5, 'eta': 0.0009433909430132146, 'gamma': 0.0005349263805792408, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,551] Trial 37 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 3.4389612745015565e-06, 'alpha': 1.2911099179660089e-05, 'max_depth': 7, 'eta': 8.881353218400541e-07, 'gamma': 2.7008096642432587e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.4745418109879485e-08, 'skip_drop': 5.992526928180549e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,771] Trial 38 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.765248582391878e-07, 'alpha': 5.130314045457026e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,014] Trial 39 finished with value: 0.4745604857943926 and parameters: {'booster': 'dart', 'lambda': 2.523868730808593e-05, 'alpha': 0.00012412878476261187, 'max_depth': 1, 'eta': 0.025109640289980736, 'gamma': 0.05422171499705667, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.512262965444561e-08, 'skip_drop': 2.8831336612931595e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,196] Trial 40 finished with value: 0.7171808453237203 and parameters: {'booster': 'gblinear', 'lambda': 0.0002458025491219182, 'alpha': 2.562020279089221e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,411] Trial 41 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.3917221062872473e-07, 'alpha': 3.0425242954594615e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,607] Trial 42 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.0475340992571034e-07, 'alpha': 1.2918998678319232e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,832] Trial 43 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.015539334718207728, 'alpha': 6.815424094420351e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,995] Trial 44 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.9317625367510702e-08, 'alpha': 3.635120096875507e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,273] Trial 45 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 2.717301141452891e-07, 'alpha': 3.7474244708240846e-06, 'max_depth': 4, 'eta': 0.00023586718986498095, 'gamma': 0.00031029094636290635, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0007997919101984602, 'skip_drop': 1.0884770015755012e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,474] Trial 46 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.748596059327255e-08, 'alpha': 1.179149728539407e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,682] Trial 47 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 4.361861022961585e-07, 'alpha': 9.195175740480457e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,920] Trial 48 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 1.0192216342389192e-08, 'alpha': 4.5886407853737526e-05, 'max_depth': 7, 'eta': 6.369657741505739e-06, 'gamma': 2.0958019445412326e-05, 'grow_policy': 'depthwise'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,088] Trial 49 finished with value: 0.7580962043744017 and parameters: {'booster': 'gblinear', 'lambda': 7.172111160302295e-08, 'alpha': 1.0357555640614724e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,302] Trial 50 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.067053824150643e-06, 'alpha': 3.05650357812132e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,462] Trial 51 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.767276339177455e-07, 'alpha': 6.130570105270266e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,657] Trial 52 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.988971773010852e-06, 'alpha': 2.1943683208676023e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,819] Trial 53 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.6187073619549733e-05, 'alpha': 3.464831177625243e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:43,006] Trial 54 finished with value: 0.8139129397754031 and parameters: {'booster': 'gblinear', 'lambda': 4.381163187421666e-06, 'alpha': 6.047147950187142e-06}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,167] Trial 55 finished with value: 0.8139129397754031 and parameters: {'booster': 'gblinear', 'lambda': 5.5772634756779776e-06, 'alpha': 5.602300832663326e-06}. Best is trial 54 with value: 0.8139129397754031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.814\n",
      "Best model performance:\n",
      "Accuracy: 0.946 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.946 ± 0.050\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:43,473] Trial 56 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 5.26701895563825e-06, 'alpha': 5.120405999096948e-06, 'max_depth': 3, 'eta': 2.7433820281214906e-07, 'gamma': 0.004473086396723472, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 9.235469129389463e-07, 'skip_drop': 0.00022812065498343997}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,666] Trial 57 finished with value: 0.7171808453237203 and parameters: {'booster': 'gblinear', 'lambda': 9.4994711111478e-06, 'alpha': 1.934607037399446e-05}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,859] Trial 58 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 0.00011825017071948709, 'alpha': 0.7820437516582137}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:44,021] Trial 59 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.0218539314838976e-06, 'alpha': 0.00010208036194228354}. Best is trial 54 with value: 0.8139129397754031.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "# xgb_eval.print_best_results()\n",
    "# xgb_eval.print_best_parameters()\n",
    "# xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:46,083] A new study created in memory with name: no-name-ddd7999b-136f-4033-9f00-4e92a3b21339\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:47,546] Trial 0 finished with value: 0.669993140387569 and parameters: {'lr': 0.007622039663011948, 'dropout': 0.4585845501943143}. Best is trial 0 with value: 0.669993140387569.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.670\n",
      "Best model performance:\n",
      "Accuracy: 0.907 ± 0.053\n",
      "F1 Macro: 0.822 ± 0.096\n",
      "F1 Weighted: 0.899 ± 0.058\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:49,005] Trial 1 finished with value: 0.7147297129153442 and parameters: {'lr': 0.006410579595352722, 'dropout': 0.12366664616715325}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.715\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.849 ± 0.038\n",
      "F1 Weighted: 0.916 ± 0.025\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:50,494] Trial 2 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00012686027549121097, 'dropout': 0.5749879061567991}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:51,959] Trial 3 finished with value: 0.49172948755219825 and parameters: {'lr': 0.000264893796838899, 'dropout': 0.4631781594647826}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:53,409] Trial 4 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0004784206952250331, 'dropout': 0.3596304409529879}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:54,863] Trial 5 finished with value: 0.6192439604955923 and parameters: {'lr': 0.00033733559620486577, 'dropout': 0.4455335469905525}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:56,364] Trial 6 finished with value: 0.6305302169311957 and parameters: {'lr': 0.00838500747394, 'dropout': 0.17526326839457254}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:57,864] Trial 7 finished with value: 0.6305302169311957 and parameters: {'lr': 0.0009276867308827282, 'dropout': 0.17028304177971718}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:59,381] Trial 8 finished with value: 0.6812403790638734 and parameters: {'lr': 0.001062617455357245, 'dropout': 0.5396150459529041}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:38:00,887] Trial 9 finished with value: 0.3786465143290708 and parameters: {'lr': 0.00025910701283476294, 'dropout': 0.5224525566025218}. Best is trial 1 with value: 0.7147297129153442.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.849 ± 0.038\n",
      "F1 Weighted: 0.916 ± 0.025\n",
      "Best hyperparameters:\n",
      "{'lr': 0.006410579595352722, 'dropout': 0.12366664616715325}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:38:01,189] A new study created in memory with name: no-name-1e4f9227-6f35-44bd-8676-43b458665935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:38:30,050] Trial 0 finished with value: 0.8690169726942147 and parameters: {}. Best is trial 0 with value: 0.8690169726942147.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.869\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.942 ± 0.073\n",
      "F1 Weighted: 0.961 ± 0.050\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.8295454545454546), 'f1_weighted': np.float64(0.8772727272727272)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.942 ± 0.073\n",
      "F1 Weighted: 0.961 ± 0.050\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:38:30,432] A new study created in memory with name: no-name-53d82c88-b37a-43ac-a498-90cf04fcfd67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4404, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1544, Train Acc: 0.9153, Train F1 Macro: 0.8282, Train F1 Weighted: 0.9090\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1415, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9067\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0740, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0696, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0755, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0174, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3212, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1876, Train Acc: 0.9322, Train F1 Macro: 0.8689, Train F1 Weighted: 0.9291\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1684, Train Acc: 0.8983, Train F1 Macro: 0.7569, Train F1 Weighted: 0.8794\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1281, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0852, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7545, Val Geometric Mean: 0.7145\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7545\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0536, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0803, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.4054)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3104, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1878, Train Acc: 0.8644, Train F1 Macro: 0.6758, Train F1 Weighted: 0.8393\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2349, Train Acc: 0.8983, Train F1 Macro: 0.8324, Train F1 Weighted: 0.9019\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2634, Train Acc: 0.8475, Train F1 Macro: 0.6563, Train F1 Weighted: 0.8257\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1047, Train Acc: 0.9661, Train F1 Macro: 0.9398, Train F1 Weighted: 0.9661\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1049, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1526, Train Acc: 0.9322, Train F1 Macro: 0.8796, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.8108)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(16.0270)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3331, Train Acc: 0.8305, Train F1 Macro: 0.6385, Train F1 Weighted: 0.8037\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2197, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9039\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1594, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1526, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1313, Train Acc: 0.9322, Train F1 Macro: 0.8883, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1331, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8115\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0485, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7432)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(15.5135)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2777, Train Acc: 0.8333, Train F1 Macro: 0.5370, Train F1 Weighted: 0.7716\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1101, Train Acc: 0.9667, Train F1 Macro: 0.9443, Train F1 Weighted: 0.9667\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1249, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1722, Train Acc: 0.8833, Train F1 Macro: 0.8117, Train F1 Weighted: 0.8853\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0373, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0350, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:40:32,404] Trial 0 finished with value: 0.9491196586666667 and parameters: {}. Best is trial 0 with value: 0.9491196586666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0700, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.7879, Val F1 Weighted: 0.8745, Val Geometric Mean: 0.8390\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.7879, Test F1 Weighted: 0.8745\n",
      "##################################################\n",
      "New best score: 0.949\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "three_layers = True\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": three_layers,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT 3L\" if three_layers else \"BiRGAT 2L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrna, mirna, circrna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, circrna, 2L no interactions\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.918 ± 0.113\n",
    "F1 Weighted: 0.953 ± 0.064\n",
    "---\n",
    "Accuracy: 0.946 ± 0.027\n",
    "F1 Macro: 0.904 ± 0.048\n",
    "F1 Weighted: 0.944 ± 0.028\n",
    "# mrna, mirna, circrna 3L, interactions, degree ~20 in diff exp graphs, larger degree shows degraded performance\n",
    "# making the avg degree to high shows large jumps on the validation set during training\n",
    "Accuracy: 0.945 ± 0.053\n",
    "F1 Macro: 0.910 ± 0.081\n",
    "F1 Weighted: 0.946 ± 0.048\n",
    "# mrna, mirna, circrna 2L, interactions, 64 cap\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053\n",
    "# mrna, mirna, circrna 3L, interactions, 64 cap\n",
    "Accuracy: 0.891 ± 0.054\n",
    "F1 Macro: 0.801 ± 0.088\n",
    "F1 Weighted: 0.888 ± 0.056\n",
    "# mrna, mirna, circrna 3L\n",
    "Accuracy: 0.920 ± 0.050\n",
    "F1 Macro: 0.829 ± 0.112\n",
    "F1 Weighted: 0.907 ± 0.062\n",
    "# mrna, mirna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, 3L\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.897 ± 0.089\n",
    "F1 Weighted: 0.944 ± 0.051"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
