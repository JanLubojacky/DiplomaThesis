{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_disease/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_disease/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-11-21 00:37:21,081] A new study created in memory with name: no-name-94094ee1-ba90-428c-976e-5b569c19bcc2\n",
      "[I 2024-11-21 00:37:21,266] Trial 0 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.545253957603188.\n",
      "[I 2024-11-21 00:37:21,408] Trial 1 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.545\n",
      "Best model performance:\n",
      "Accuracy: 0.880 ± 0.065\n",
      "F1 Macro: 0.721 ± 0.176\n",
      "F1 Weighted: 0.859 ± 0.076\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8, 'f1_macro': np.float64(0.64), 'f1_weighted': np.float64(0.7840000000000001)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.4642857142857143), 'f1_weighted': np.float64(0.8047619047619048)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:21,547] Trial 2 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,685] Trial 3 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 12}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,822] Trial 4 finished with value: 0.6024938699160863 and parameters: {'n_neighbors': 17}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:21,958] Trial 5 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 8}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,099] Trial 6 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,235] Trial 7 finished with value: 0.545253957603188 and parameters: {'n_neighbors': 12}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,373] Trial 8 finished with value: 0.5716516161494184 and parameters: {'n_neighbors': 16}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,512] Trial 9 finished with value: 0.40415587923136775 and parameters: {'n_neighbors': 2}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,651] Trial 10 finished with value: 0.40415587923136775 and parameters: {'n_neighbors': 2}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,789] Trial 11 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:22,927] Trial 12 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,068] Trial 13 finished with value: 0.4728693782897456 and parameters: {'n_neighbors': 20}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,206] Trial 14 finished with value: 0.6181128117837651 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,363] Trial 15 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 9}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,503] Trial 16 finished with value: 0.5114959395093938 and parameters: {'n_neighbors': 4}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,643] Trial 17 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 10}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,782] Trial 18 finished with value: 0.6181128117837651 and parameters: {'n_neighbors': 5}. Best is trial 1 with value: 0.6631836987350632.\n",
      "[I 2024-11-21 00:37:23,922] Trial 19 finished with value: 0.6516600083300729 and parameters: {'n_neighbors': 10}. Best is trial 1 with value: 0.6631836987350632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()\n",
    "# knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:23,944] A new study created in memory with name: no-name-fc278aab-919a-4906-b8b0-a7f9d091292e\n",
      "[I 2024-11-21 00:37:24,100] Trial 0 finished with value: 0.6812403790638734 and parameters: {'C': 0.016675213956196765, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6812403790638734.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.681\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.053\n",
      "F1 Macro: 0.835 ± 0.098\n",
      "F1 Weighted: 0.901 ± 0.059\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:24,240] Trial 1 finished with value: 0.6812403790638734 and parameters: {'C': 0.02849950426647128, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6812403790638734.\n",
      "[I 2024-11-21 00:37:24,380] Trial 2 finished with value: 0.7039337563024542 and parameters: {'C': 0.16072762000291213, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,523] Trial 3 finished with value: 0.6682316917536709 and parameters: {'C': 0.8456778159715628, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.7039337563024542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:24,645] Trial 4 finished with value: 0.7039337563024542 and parameters: {'C': 0.01642746925590834, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,767] Trial 5 finished with value: 0.6192439604955923 and parameters: {'C': 0.011471521424410965, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:24,914] Trial 6 finished with value: 0.6682316917536709 and parameters: {'C': 2.0583204175797025, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:25,041] Trial 7 finished with value: 0.7039337563024542 and parameters: {'C': 0.03561454122625204, 'class_weight': None}. Best is trial 2 with value: 0.7039337563024542.\n",
      "[I 2024-11-21 00:37:25,189] Trial 8 finished with value: 0.7172503758053912 and parameters: {'C': 0.16498184393238047, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,338] Trial 9 finished with value: 0.6682316917536709 and parameters: {'C': 0.8051797736707758, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.717\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.853 ± 0.097\n",
      "F1 Weighted: 0.914 ± 0.057\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.9386666666666666)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:25,489] Trial 10 finished with value: 0.6682316917536709 and parameters: {'C': 6.8692960511070815, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,628] Trial 11 finished with value: 0.7039337563024542 and parameters: {'C': 0.12368378886541774, 'class_weight': None}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,774] Trial 12 finished with value: 0.7172503758053912 and parameters: {'C': 0.14675039305810655, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:25,935] Trial 13 finished with value: 0.7172503758053912 and parameters: {'C': 0.06619879441586395, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,080] Trial 14 finished with value: 0.7172503758053912 and parameters: {'C': 0.33863485362653023, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,228] Trial 15 finished with value: 0.7172503758053912 and parameters: {'C': 0.2738085590206223, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,372] Trial 16 finished with value: 0.7172503758053912 and parameters: {'C': 0.08140982990470529, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,519] Trial 17 finished with value: 0.6682316917536709 and parameters: {'C': 0.6466183948186608, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,670] Trial 18 finished with value: 0.6682316917536709 and parameters: {'C': 3.2695257580816848, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,812] Trial 19 finished with value: 0.7172503758053912 and parameters: {'C': 0.32396317734373475, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:26,959] Trial 20 finished with value: 0.7172503758053912 and parameters: {'C': 0.1773168287534853, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,124] Trial 21 finished with value: 0.6812403790638734 and parameters: {'C': 0.05509301175131984, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,268] Trial 22 finished with value: 0.7172503758053912 and parameters: {'C': 0.07423983074574342, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,411] Trial 23 finished with value: 0.6812403790638734 and parameters: {'C': 0.04166154432477624, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,555] Trial 24 finished with value: 0.7172503758053912 and parameters: {'C': 0.10242699193831091, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,700] Trial 25 finished with value: 0.7172503758053912 and parameters: {'C': 0.20537916953085308, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,846] Trial 26 finished with value: 0.6682316917536709 and parameters: {'C': 0.4921568656578518, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:27,992] Trial 27 finished with value: 0.6682316917536709 and parameters: {'C': 1.4224972711541313, 'class_weight': 'balanced'}. Best is trial 8 with value: 0.7172503758053912.\n",
      "[I 2024-11-21 00:37:28,136] Trial 28 finished with value: 0.7581635155117249 and parameters: {'C': 0.06285217601548478, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,269] Trial 29 finished with value: 0.6812403790638734 and parameters: {'C': 0.018593943994173228, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:28,416] Trial 30 finished with value: 0.7172503758053912 and parameters: {'C': 0.12500130112484123, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,554] Trial 31 finished with value: 0.7581635155117249 and parameters: {'C': 0.06105273633037403, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,693] Trial 32 finished with value: 0.6812403790638734 and parameters: {'C': 0.027728082850775628, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:28,839] Trial 33 finished with value: 0.6812403790638734 and parameters: {'C': 0.05589008364042942, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,000] Trial 34 finished with value: 0.7172503758053912 and parameters: {'C': 0.2011691749306423, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,136] Trial 35 finished with value: 0.6812403790638734 and parameters: {'C': 0.027343074353882547, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,278] Trial 36 finished with value: 0.7039337563024542 and parameters: {'C': 0.12093639130266076, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,419] Trial 37 finished with value: 0.6812403790638734 and parameters: {'C': 0.04601041828230094, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,547] Trial 38 finished with value: 0.7039337563024542 and parameters: {'C': 0.019383673971694864, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,690] Trial 39 finished with value: 0.7172503758053912 and parameters: {'C': 0.08622059323109191, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,833] Trial 40 finished with value: 0.6682316917536709 and parameters: {'C': 0.4499493036333314, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:29,975] Trial 41 finished with value: 0.7172503758053912 and parameters: {'C': 0.06928414273712898, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,119] Trial 42 finished with value: 0.7172503758053912 and parameters: {'C': 0.16527710413971605, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,252] Trial 43 finished with value: 0.6812403790638734 and parameters: {'C': 0.012212028485024657, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,393] Trial 44 finished with value: 0.6812403790638734 and parameters: {'C': 0.033225608242794136, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,544] Trial 45 finished with value: 0.7172503758053912 and parameters: {'C': 0.25708380463560365, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,681] Trial 46 finished with value: 0.7039337563024542 and parameters: {'C': 0.1337825759029321, 'class_weight': None}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,823] Trial 47 finished with value: 0.6812403790638734 and parameters: {'C': 0.05651684844138084, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:30,989] Trial 48 finished with value: 0.7172503758053912 and parameters: {'C': 0.07768576960502818, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:31,136] Trial 49 finished with value: 0.7172503758053912 and parameters: {'C': 0.09745557521539507, 'class_weight': 'balanced'}. Best is trial 28 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_disease/mrna_mirna_te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:31,173] A new study created in memory with name: no-name-f5967f9c-326a-4aee-bb2e-645187e52939\n",
      "[I 2024-11-21 00:37:31,455] Trial 0 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 0.777042272438913, 'alpha': 0.007012163504194617, 'max_depth': 5, 'eta': 3.366922110430363e-05, 'gamma': 8.229368747919227e-07, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5844534524426352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.584\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.065\n",
      "F1 Macro: 0.761 ± 0.170\n",
      "F1 Weighted: 0.874 ± 0.070\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8, 'f1_macro': np.float64(0.7204968944099379), 'f1_weighted': np.float64(0.8099378881987577)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8, 'f1_macro': np.float64(0.4444444444444444), 'f1_weighted': np.float64(0.7703703703703704)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:31,700] Trial 1 finished with value: 0.43246561009234025 and parameters: {'booster': 'gbtree', 'lambda': 6.907136835959009e-06, 'alpha': 1.0143730016435205e-08, 'max_depth': 2, 'eta': 0.002099548308135269, 'gamma': 0.29869310861813714, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5844534524426352.\n",
      "[I 2024-11-21 00:37:31,956] Trial 2 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 0.0004247114677277087, 'alpha': 9.511835775344175e-07, 'max_depth': 4, 'eta': 2.5091288695504218e-08, 'gamma': 0.007636488371795125, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.6728264260126283, 'skip_drop': 0.04105023768061634}. Best is trial 0 with value: 0.5844534524426352.\n",
      "[I 2024-11-21 00:37:32,153] Trial 3 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.016289389067311696, 'alpha': 1.379929016068266e-06}. Best is trial 3 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:32,454] Trial 4 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 6.427644198938857e-07, 'alpha': 0.3135826508026116, 'max_depth': 3, 'eta': 1.7716798332590427e-06, 'gamma': 0.0007902184802045573, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:32,736] Trial 5 finished with value: 0.48162878238196427 and parameters: {'booster': 'dart', 'lambda': 0.1313406697070976, 'alpha': 1.638006863412106e-05, 'max_depth': 7, 'eta': 0.025799967421881054, 'gamma': 2.1564641889698135e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 2.945806173024392e-06, 'skip_drop': 1.1842930673809874e-06}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:32,974] Trial 6 finished with value: 0.3829003597127381 and parameters: {'booster': 'dart', 'lambda': 0.0005164639182912427, 'alpha': 0.15150398599441617, 'max_depth': 2, 'eta': 6.88180581792351e-07, 'gamma': 2.9300678019603802e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.2592507872069602e-05, 'skip_drop': 0.012425098872306271}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,247] Trial 7 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.0021990430148398615, 'alpha': 6.785036790894485e-08, 'max_depth': 8, 'eta': 0.15488864925113413, 'gamma': 1.8650462303090628e-06, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.7394278999267716, 'skip_drop': 0.012013624858241284}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,432] Trial 8 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.0013531430853130294, 'alpha': 9.926188862054352e-05}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,683] Trial 9 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.2471937292414054, 'alpha': 4.5028907372294134e-08, 'max_depth': 9, 'eta': 0.0016584275770170109, 'gamma': 4.6121080626099866e-05, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.01649900332753479, 'skip_drop': 0.13924496883626883}. Best is trial 3 with value: 0.7581635155117249.\n",
      "[I 2024-11-21 00:37:33,910] Trial 10 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.0925298540779154e-07, 'alpha': 1.7653402312068486e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,111] Trial 11 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 3.728824666665131e-08, 'alpha': 1.6807039964666046e-06}. Best is trial 10 with value: 0.7766534866310313.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.777\n",
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.060\n",
      "F1 Macro: 0.892 ± 0.093\n",
      "F1 Weighted: 0.933 ± 0.060\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:34,300] Trial 12 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.176720274615829e-08, 'alpha': 4.04284334741871e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,538] Trial 13 finished with value: 0.6013127244639374 and parameters: {'booster': 'gblinear', 'lambda': 1.9535657640285294e-08, 'alpha': 0.0009778525679663705}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,705] Trial 14 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.2804431138058544e-07, 'alpha': 3.1156566564358095e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:34,949] Trial 15 finished with value: 0.6119104221152568 and parameters: {'booster': 'gblinear', 'lambda': 1.2334231778132507e-05, 'alpha': 3.04085084646386e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,133] Trial 16 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.7714031111206863e-07, 'alpha': 0.0006308075490266575}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,360] Trial 17 finished with value: 0.7264254257106565 and parameters: {'booster': 'gblinear', 'lambda': 8.122705212395857e-06, 'alpha': 7.349808410166707e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,601] Trial 18 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 6.077244040722739e-08, 'alpha': 2.2773194926380306e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:35,814] Trial 19 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.6131875038114226e-06, 'alpha': 0.00030970623691254197}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,073] Trial 20 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 3.620622275794472e-05, 'alpha': 1.983031001800808e-06, 'max_depth': 6, 'eta': 2.204014653967302e-08, 'gamma': 0.6964502540891363, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,273] Trial 21 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.175520283658004e-07, 'alpha': 3.3561015437436135e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,464] Trial 22 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.1335878433150491e-06, 'alpha': 1.7759347340935225e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,663] Trial 23 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 6.28491510005258e-08, 'alpha': 1.2281821436667312e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,827] Trial 24 finished with value: 0.643037106032834 and parameters: {'booster': 'gblinear', 'lambda': 2.7992144006805795e-07, 'alpha': 4.1935593704490814e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:36,990] Trial 25 finished with value: 0.7580962043744017 and parameters: {'booster': 'gblinear', 'lambda': 4.433572626082405e-08, 'alpha': 7.83185887778193e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,180] Trial 26 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.9887479450260334e-06, 'alpha': 6.082027733282836e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,370] Trial 27 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 6.528422466079318e-05, 'alpha': 5.8426795296331335e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,558] Trial 28 finished with value: 0.4745604857943926 and parameters: {'booster': 'gbtree', 'lambda': 1.1973695217119165e-07, 'alpha': 3.252554884290843e-06, 'max_depth': 1, 'eta': 2.851217189448492e-05, 'gamma': 0.01092396566733931, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:37,753] Trial 29 finished with value: 0.5350135409063966 and parameters: {'booster': 'gblinear', 'lambda': 1.0577115996797466e-08, 'alpha': 0.016288192694524445}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,024] Trial 30 finished with value: 0.5052171643290275 and parameters: {'booster': 'gbtree', 'lambda': 5.800270817487896e-07, 'alpha': 0.0030527715355163853, 'max_depth': 9, 'eta': 0.6832464382719352, 'gamma': 1.0287459993048287e-05, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,206] Trial 31 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.3370056135249796e-08, 'alpha': 2.2102834525156941e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,407] Trial 32 finished with value: 0.7393866992524499 and parameters: {'booster': 'gblinear', 'lambda': 7.364211905515403e-08, 'alpha': 1.3991698679819874e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,625] Trial 33 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 2.8502650681579386e-08, 'alpha': 1.6524907413237376e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:38,782] Trial 34 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.064955076031104e-06, 'alpha': 9.567492716913095e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,027] Trial 35 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.885541595444948e-07, 'alpha': 4.88186700879917e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,288] Trial 36 finished with value: 0.5591177623513526 and parameters: {'booster': 'gbtree', 'lambda': 8.255756802988455e-07, 'alpha': 1.6575023223409445e-06, 'max_depth': 5, 'eta': 0.0009433909430132146, 'gamma': 0.0005349263805792408, 'grow_policy': 'lossguide'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,551] Trial 37 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 3.4389612745015565e-06, 'alpha': 1.2911099179660089e-05, 'max_depth': 7, 'eta': 8.881353218400541e-07, 'gamma': 2.7008096642432587e-07, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.4745418109879485e-08, 'skip_drop': 5.992526928180549e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:39,771] Trial 38 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 4.765248582391878e-07, 'alpha': 5.130314045457026e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,014] Trial 39 finished with value: 0.4745604857943926 and parameters: {'booster': 'dart', 'lambda': 2.523868730808593e-05, 'alpha': 0.00012412878476261187, 'max_depth': 1, 'eta': 0.025109640289980736, 'gamma': 0.05422171499705667, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.512262965444561e-08, 'skip_drop': 2.8831336612931595e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,196] Trial 40 finished with value: 0.7171808453237203 and parameters: {'booster': 'gblinear', 'lambda': 0.0002458025491219182, 'alpha': 2.562020279089221e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,411] Trial 41 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.3917221062872473e-07, 'alpha': 3.0425242954594615e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,607] Trial 42 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.0475340992571034e-07, 'alpha': 1.2918998678319232e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,832] Trial 43 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.015539334718207728, 'alpha': 6.815424094420351e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:40,995] Trial 44 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.9317625367510702e-08, 'alpha': 3.635120096875507e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,273] Trial 45 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 2.717301141452891e-07, 'alpha': 3.7474244708240846e-06, 'max_depth': 4, 'eta': 0.00023586718986498095, 'gamma': 0.00031029094636290635, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0007997919101984602, 'skip_drop': 1.0884770015755012e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,474] Trial 46 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.748596059327255e-08, 'alpha': 1.179149728539407e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,682] Trial 47 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 4.361861022961585e-07, 'alpha': 9.195175740480457e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:41,920] Trial 48 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 1.0192216342389192e-08, 'alpha': 4.5886407853737526e-05, 'max_depth': 7, 'eta': 6.369657741505739e-06, 'gamma': 2.0958019445412326e-05, 'grow_policy': 'depthwise'}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,088] Trial 49 finished with value: 0.7580962043744017 and parameters: {'booster': 'gblinear', 'lambda': 7.172111160302295e-08, 'alpha': 1.0357555640614724e-05}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,302] Trial 50 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 2.067053824150643e-06, 'alpha': 3.05650357812132e-08}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,462] Trial 51 finished with value: 0.7766534866310313 and parameters: {'booster': 'gblinear', 'lambda': 1.767276339177455e-07, 'alpha': 6.130570105270266e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,657] Trial 52 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.988971773010852e-06, 'alpha': 2.1943683208676023e-06}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:42,819] Trial 53 finished with value: 0.7351676603683842 and parameters: {'booster': 'gblinear', 'lambda': 1.6187073619549733e-05, 'alpha': 3.464831177625243e-07}. Best is trial 10 with value: 0.7766534866310313.\n",
      "[I 2024-11-21 00:37:43,006] Trial 54 finished with value: 0.8139129397754031 and parameters: {'booster': 'gblinear', 'lambda': 4.381163187421666e-06, 'alpha': 6.047147950187142e-06}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,167] Trial 55 finished with value: 0.8139129397754031 and parameters: {'booster': 'gblinear', 'lambda': 5.5772634756779776e-06, 'alpha': 5.602300832663326e-06}. Best is trial 54 with value: 0.8139129397754031.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.814\n",
      "Best model performance:\n",
      "Accuracy: 0.946 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.946 ± 0.050\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:43,473] Trial 56 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 5.26701895563825e-06, 'alpha': 5.120405999096948e-06, 'max_depth': 3, 'eta': 2.7433820281214906e-07, 'gamma': 0.004473086396723472, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 9.235469129389463e-07, 'skip_drop': 0.00022812065498343997}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,666] Trial 57 finished with value: 0.7171808453237203 and parameters: {'booster': 'gblinear', 'lambda': 9.4994711111478e-06, 'alpha': 1.934607037399446e-05}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:43,859] Trial 58 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 0.00011825017071948709, 'alpha': 0.7820437516582137}. Best is trial 54 with value: 0.8139129397754031.\n",
      "[I 2024-11-21 00:37:44,021] Trial 59 finished with value: 0.643905171245766 and parameters: {'booster': 'gblinear', 'lambda': 1.0218539314838976e-06, 'alpha': 0.00010208036194228354}. Best is trial 54 with value: 0.8139129397754031.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "# xgb_eval.print_best_results()\n",
    "# xgb_eval.print_best_parameters()\n",
    "# xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:37:46,083] A new study created in memory with name: no-name-ddd7999b-136f-4033-9f00-4e92a3b21339\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:47,546] Trial 0 finished with value: 0.669993140387569 and parameters: {'lr': 0.007622039663011948, 'dropout': 0.4585845501943143}. Best is trial 0 with value: 0.669993140387569.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.670\n",
      "Best model performance:\n",
      "Accuracy: 0.907 ± 0.053\n",
      "F1 Macro: 0.822 ± 0.096\n",
      "F1 Weighted: 0.899 ± 0.058\n",
      "[{'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7115384615384616), 'f1_weighted': np.float64(0.8384615384615385)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:49,005] Trial 1 finished with value: 0.7147297129153442 and parameters: {'lr': 0.006410579595352722, 'dropout': 0.12366664616715325}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.715\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.849 ± 0.038\n",
      "F1 Weighted: 0.916 ± 0.025\n",
      "[{'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 0.8666666666666667, 'f1_macro': np.float64(0.7916666666666666), 'f1_weighted': np.float64(0.8666666666666667)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.8148148148148148), 'f1_weighted': np.float64(0.9234567901234567)}, {'acc': 0.9285714285714286, 'f1_macro': np.float64(0.8782608695652174), 'f1_weighted': np.float64(0.9341614906832298)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:50,494] Trial 2 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00012686027549121097, 'dropout': 0.5749879061567991}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:51,959] Trial 3 finished with value: 0.49172948755219825 and parameters: {'lr': 0.000264893796838899, 'dropout': 0.4631781594647826}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:53,409] Trial 4 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0004784206952250331, 'dropout': 0.3596304409529879}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:54,863] Trial 5 finished with value: 0.6192439604955923 and parameters: {'lr': 0.00033733559620486577, 'dropout': 0.4455335469905525}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:56,364] Trial 6 finished with value: 0.6305302169311957 and parameters: {'lr': 0.00838500747394, 'dropout': 0.17526326839457254}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:57,864] Trial 7 finished with value: 0.6305302169311957 and parameters: {'lr': 0.0009276867308827282, 'dropout': 0.17028304177971718}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:37:59,381] Trial 8 finished with value: 0.6812403790638734 and parameters: {'lr': 0.001062617455357245, 'dropout': 0.5396150459529041}. Best is trial 1 with value: 0.7147297129153442.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:38:00,887] Trial 9 finished with value: 0.3786465143290708 and parameters: {'lr': 0.00025910701283476294, 'dropout': 0.5224525566025218}. Best is trial 1 with value: 0.7147297129153442.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.849 ± 0.038\n",
      "F1 Weighted: 0.916 ± 0.025\n",
      "Best hyperparameters:\n",
      "{'lr': 0.006410579595352722, 'dropout': 0.12366664616715325}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([200, 200, 200], ['mrna', 'mirna', 'te'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.in_channels, mogonet_eval.omic_names\n",
    "# mogonet_eval.evaluate()\n",
    "# mogonet_eval.print_best_results()\n",
    "# mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mrna': {'ENSG00000181826': 0, 'ENSG00000278588': 0, 'ENSG00000120594': 0, 'ENSG00000121797': 0, 'ENSG00000140398': 0, 'ENSG00000168062': 0, 'ENSG00000174307': 0, 'ENSG00000184897': 0, 'ENSG00000105497': 0, 'ENSG00000113552': 0, 'ENSG00000188536': 0, 'ENSG00000181004': 0, 'ENSG00000143590': 0, 'ENSG00000006534': 0, 'ENSG00000184792': 0, 'ENSG00000114737': 0, 'ENSG00000130518': 0, 'ENSG00000133561': 0, 'ENSG00000179820': 0, 'ENSG00000196329': 0, 'ENSG00000164626': 0, 'ENSG00000206172': 0, 'ENSG00000196866': 0, 'ENSG00000160013': 0, 'ENSG00000164938': 0, 'ENSG00000260729': 0, 'ENSG00000204161': 0, 'ENSG00000062282': 0, 'ENSG00000087903': 0, 'ENSG00000176641': 0, 'ENSG00000136603': 0, 'ENSG00000113369': 0, 'ENSG00000148935': 0, 'ENSG00000153071': 0, 'ENSG00000174130': 0, 'ENSG00000131016': 0, 'ENSG00000144959': 0, 'ENSG00000203883': 0, 'ENSG00000116574': 0, 'ENSG00000078804': 0, 'ENSG00000120217': 0, 'ENSG00000172667': 0, 'ENSG00000161544': 0, 'ENSG00000121966': 0, 'ENSG00000197646': 0, 'ENSG00000271303': 0, 'ENSG00000125846': 0, 'ENSG00000119801': 0, 'ENSG00000179144': 0, 'ENSG00000196549': 0, 'ENSG00000196230': 0, 'ENSG00000184588': 0, 'ENSG00000026103': 0, 'ENSG00000158578': 0, 'ENSG00000130052': 0, 'ENSG00000119686': 0, 'ENSG00000132475': 0, 'ENSG00000172995': 0, 'ENSG00000022567': 0, 'ENSG00000272398': 0, 'ENSG00000228727': 0, 'ENSG00000171680': 0, 'ENSG00000175538': 0, 'ENSG00000178752': 0, 'ENSG00000119508': 0, 'ENSG00000187837': 0, 'ENSG00000185614': 0, 'ENSG00000178573': 0, 'ENSG00000177191': 0, 'ENSG00000073737': 0, 'ENSG00000092295': 0, 'ENSG00000138623': 0, 'ENSG00000135926': 0, 'ENSG00000185669': 0, 'ENSG00000189283': 0, 'ENSG00000026025': 0, 'ENSG00000168298': 0, 'ENSG00000164330': 0, 'ENSG00000081320': 0, 'ENSG00000174944': 0, 'ENSG00000175449': 0, 'ENSG00000181790': 0, 'ENSG00000260314': 0, 'ENSG00000177455': 0, 'ENSG00000142961': 0, 'ENSG00000073754': 0, 'ENSG00000124575': 0, 'ENSG00000109321': 0, 'ENSG00000175097': 0, 'ENSG00000127364': 0, 'ENSG00000155659': 0, 'ENSG00000110777': 0, 'ENSG00000185730': 0, 'ENSG00000244734': 0, 'ENSG00000196092': 0, 'ENSG00000185304': 0, 'ENSG00000130021': 0, 'ENSG00000182584': 0, 'ENSG00000196517': 0, 'ENSG00000284931': 0, 'ENSG00000084764': 0, 'ENSG00000128322': 0, 'ENSG00000068976': 0, 'ENSG00000166349': 0, 'ENSG00000146592': 0, 'ENSG00000005238': 0, 'ENSG00000111424': 0, 'ENSG00000116096': 0, 'ENSG00000065534': 0, 'ENSG00000181444': 0, 'ENSG00000130787': 0, 'ENSG00000137310': 0, 'ENSG00000159958': 0, 'ENSG00000155090': 0, 'ENSG00000263155': 0, 'ENSG00000166289': 0, 'ENSG00000176845': 0, 'ENSG00000121858': 0, 'ENSG00000136541': 0, 'ENSG00000144749': 0, 'ENSG00000158373': 0, 'ENSG00000163491': 0, 'ENSG00000165272': 0, 'ENSG00000004939': 0, 'ENSG00000171729': 0, 'ENSG00000161835': 0, 'ENSG00000137834': 0, 'ENSG00000129116': 0, 'ENSG00000105369': 0, 'ENSG00000127366': 0, 'ENSG00000174600': 0, 'ENSG00000204186': 0, 'ENSG00000180155': 0, 'ENSG00000131080': 0, 'ENSG00000139174': 0, 'ENSG00000168785': 0, 'ENSG00000105409': 0, 'ENSG00000176125': 0, 'ENSG00000007312': 0, 'ENSG00000171115': 0, 'ENSG00000107796': 0, 'ENSG00000196628': 0, 'ENSG00000151491': 0, 'ENSG00000102554': 0, 'ENSG00000196565': 0, 'ENSG00000132334': 0, 'ENSG00000120318': 0, 'ENSG00000120889': 0, 'ENSG00000124882': 0, 'ENSG00000188153': 0, 'ENSG00000198805': 0, 'ENSG00000185745': 0, 'ENSG00000127362': 0, 'ENSG00000147454': 0, 'ENSG00000137193': 0, 'ENSG00000180061': 0, 'ENSG00000075213': 0, 'ENSG00000169575': 0, 'ENSG00000258674': 0, 'ENSG00000213934': 0, 'ENSG00000135898': 0, 'ENSG00000101276': 0, 'ENSG00000065833': 0, 'ENSG00000180340': 0, 'ENSG00000128274': 0, 'ENSG00000171621': 0, 'ENSG00000080823': 0, 'ENSG00000185090': 0, 'ENSG00000077044': 0, 'ENSG00000090776': 0, 'ENSG00000171860': 0, 'ENSG00000161513': 0, 'ENSG00000054654': 0, 'ENSG00000158555': 0, 'ENSG00000138795': 0, 'ENSG00000160321': 0, 'ENSG00000179163': 0, 'ENSG00000169242': 0, 'ENSG00000099377': 0, 'ENSG00000170412': 0, 'ENSG00000112182': 0, 'ENSG00000170180': 0, 'ENSG00000108219': 0, 'ENSG00000159899': 0, 'ENSG00000154640': 0, 'ENSG00000185015': 0, 'ENSG00000076864': 0, 'ENSG00000171208': 0, 'ENSG00000173369': 0, 'ENSG00000143147': 0, 'ENSG00000165323': 0, 'ENSG00000087589': 0, 'ENSG00000080819': 0, 'ENSG00000214940': 0, 'ENSG00000168671': 0, 'ENSG00000250722': 0, 'ENSG00000165886': 0, 'ENSG00000088899': 0, 'ENSG00000130821': 0, 'ENSG00000172159': 0}, 'mirna': {'ENSG00000221771': 0, 'ENSG00000277402': 0, 'ENSG00000264781': 0, 'ENSG00000274034': 0, 'ENSG00000207975': 0, 'ENSG00000275458': 0, 'ENSG00000284387': 0, 'ENSG00000208037': 0, 'ENSG00000264773': 0, 'ENSG00000276961': 0, 'ENSG00000207808': 0, 'ENSG00000265612': 0, 'ENSG00000274494': 0, 'ENSG00000274552': 0, 'ENSG00000284375': 0, 'ENSG00000263413': 0, 'ENSG00000283929': 0, 'ENSG00000207980': 0, 'ENSG00000266751': 0, 'ENSG00000221445': 0, 'ENSG00000266643': 0, 'ENSG00000264354': 0, 'ENSG00000199090': 0, 'ENSG00000207622': 0, 'ENSG00000263790': 0, 'ENSG00000283200': 0, 'ENSG00000265321': 0, 'ENSG00000221680': 0, 'ENSG00000277942': 0, 'ENSG00000208023': 0, 'ENSG00000265134': 0, 'ENSG00000266017': 0, 'ENSG00000263409': 0, 'ENSG00000275022': 0, 'ENSG00000266594': 0, 'ENSG00000275967': 0, 'ENSG00000274620': 0, 'ENSG00000266325': 0, 'ENSG00000283172': 0, 'ENSG00000264357': 0, 'ENSG00000264102': 0, 'ENSG00000284031': 0, 'ENSG00000207983': 0, 'ENSG00000275101': 0, 'ENSG00000221091': 0, 'ENSG00000264500': 0, 'ENSG00000273776': 0, 'ENSG00000275789': 0, 'ENSG00000266297': 0, 'ENSG00000207776': 0, 'ENSG00000265879': 0, 'ENSG00000274060': 0, 'ENSG00000263857': 0, 'ENSG00000265623': 0, 'ENSG00000264160': 0, 'ENSG00000198974': 0, 'ENSG00000271886': 0, 'ENSG00000267959': 0, 'ENSG00000221063': 0, 'ENSG00000274822': 0, 'ENSG00000275692': 0, 'ENSG00000284229': 0, 'ENSG00000266698': 0, 'ENSG00000266124': 0, 'ENSG00000207650': 0, 'ENSG00000207815': 0, 'ENSG00000276869': 0, 'ENSG00000278420': 0, 'ENSG00000274986': 0, 'ENSG00000274380': 0, 'ENSG00000265333': 0, 'ENSG00000221176': 0, 'ENSG00000264572': 0, 'ENSG00000221603': 0, 'ENSG00000264610': 0, 'ENSG00000207839': 0, 'ENSG00000276926': 0, 'ENSG00000264292': 0, 'ENSG00000265657': 0, 'ENSG00000264864': 0, 'ENSG00000221190': 0, 'ENSG00000276908': 0, 'ENSG00000221214': 0, 'ENSG00000266407': 0, 'ENSG00000266320': 0, 'ENSG00000263527': 0, 'ENSG00000221792': 0, 'ENSG00000221411': 0, 'ENSG00000264796': 0, 'ENSG00000199161': 0, 'ENSG00000266619': 0, 'ENSG00000264947': 0, 'ENSG00000266235': 0, 'ENSG00000207988': 0, 'ENSG00000266533': 0, 'ENSG00000221333': 0, 'ENSG00000284378': 0, 'ENSG00000264585': 0, 'ENSG00000207654': 0, 'ENSG00000274314': 0, 'ENSG00000276753': 0, 'ENSG00000265867': 0, 'ENSG00000264931': 0, 'ENSG00000265137': 0, 'ENSG00000211575': 0, 'ENSG00000275207': 0, 'ENSG00000266139': 0, 'ENSG00000263584': 0, 'ENSG00000207741': 0, 'ENSG00000266270': 0, 'ENSG00000264049': 0, 'ENSG00000264616': 0, 'ENSG00000275651': 0, 'ENSG00000275067': 0, 'ENSG00000221533': 0, 'ENSG00000263361': 0, 'ENSG00000265874': 0, 'ENSG00000265102': 0, 'ENSG00000275667': 0, 'ENSG00000278549': 0, 'ENSG00000207583': 0, 'ENSG00000274466': 0, 'ENSG00000263828': 0, 'ENSG00000264814': 0, 'ENSG00000283204': 0, 'ENSG00000265660': 0, 'ENSG00000222071': 0, 'ENSG00000265996': 0, 'ENSG00000273836': 0, 'ENSG00000211513': 0, 'ENSG00000215973': 0, 'ENSG00000221760': 0, 'ENSG00000274417': 0, 'ENSG00000208002': 0, 'ENSG00000266146': 0, 'ENSG00000266245': 0, 'ENSG00000221406': 0, 'ENSG00000283971': 0, 'ENSG00000274111': 0, 'ENSG00000264349': 0, 'ENSG00000266589': 0, 'ENSG00000266758': 0, 'ENSG00000278658': 0, 'ENSG00000276641': 0, 'ENSG00000276102': 0, 'ENSG00000275950': 0, 'ENSG00000274134': 0, 'ENSG00000263439': 0, 'ENSG00000277379': 0, 'ENSG00000266668': 0, 'ENSG00000212017': 0, 'ENSG00000284154': 0, 'ENSG00000273874': 0, 'ENSG00000263381': 0, 'ENSG00000283498': 0, 'ENSG00000207864': 0, 'ENSG00000264201': 0, 'ENSG00000283532': 0, 'ENSG00000275449': 0, 'ENSG00000265539': 0, 'ENSG00000221394': 0, 'ENSG00000212024': 0, 'ENSG00000202566': 0, 'ENSG00000274705': 0, 'ENSG00000278449': 0, 'ENSG00000278349': 0, 'ENSG00000283514': 0, 'ENSG00000207779': 0, 'ENSG00000266618': 0, 'ENSG00000263813': 0, 'ENSG00000221585': 0, 'ENSG00000266063': 0, 'ENSG00000265201': 0, 'ENSG00000265565': 0, 'ENSG00000264536': 0, 'ENSG00000284186': 0, 'ENSG00000283676': 0, 'ENSG00000263675': 0, 'ENSG00000278447': 0, 'ENSG00000284419': 0, 'ENSG00000198995': 0, 'ENSG00000265112': 0, 'ENSG00000199024': 0, 'ENSG00000283441': 0, 'ENSG00000263963': 0, 'ENSG00000265820': 0, 'ENSG00000283206': 0, 'ENSG00000221325': 0, 'ENSG00000264653': 0, 'ENSG00000283429': 0, 'ENSG00000284224': 0, 'ENSG00000265396': 0, 'ENSG00000283475': 0, 'ENSG00000266038': 0, 'ENSG00000207588': 0, 'ENSG00000281842': 0, 'ENSG00000276404': 0, 'ENSG00000207820': 0, 'ENSG00000252695': 0, 'ENSG00000263831': 0}, 'te': {'HERV-Fc1': 0, 'LTR10C': 0, 'L1M3C_5': 0, 'LTR27E': 0, 'L2': 0, 'L1P4b_5end': 0, 'LTR18A': 0, 'MER87': 0, 'MER57E3': 0, 'THER2': 0, 'MLT1K': 0, 'LTR70': 0, 'LTR1F1': 0, 'MER54B': 0, 'LTR28': 0, 'LTR21A': 0, 'MIR3': 0, 'LTR2': 0, 'MER51E': 0, 'LTR27B': 0, 'L1M3D_5': 0, 'HERVE_a': 0, 'LTR34': 0, 'MER57F': 0, 'LTR38C': 0, 'LTR3': 0, 'L1MA9_5': 0, 'L1P4c_5end': 0, 'HARLEQUIN': 0, 'LTR1C1': 0, 'LTR36': 0, 'LTR60B': 0, 'L1ME3C_3end': 0, 'LOR1b_LTR': 0, 'HERVS71': 0, 'LTR1C': 0, 'LTR47A2': 0, 'MER66A': 0, 'LTR16A1': 0, 'LTR24': 0, 'HERVL66I': 0, 'L1ME4': 0, 'MER57E1': 0, 'MER68B': 0, 'MER70A': 0, 'LTR26E': 0, 'LTR57': 0, 'MER61B': 0, 'AluYf5': 0, 'MLT1G3': 0, 'MER9B': 0, 'L1ME5_3end': 0, 'MER66D': 0, 'LTR58': 0, 'LTR25': 0, 'LTR75_1': 0, 'MER65C': 0, 'LTR1B1': 0, 'LTR71A': 0, 'MER83C': 0, 'HERV-K14CI': 0, 'LTR2B': 0, 'MER52A': 0, 'LTR15': 0, 'MER101': 0, 'MER34A': 0, 'SVA_D': 0, 'HERV1_LTRb': 0, 'LTR25-int': 0, 'LTR72': 0, 'MER66B': 0, 'FRAM': 0, 'HERVE': 0, 'LTR53': 0, 'MLT2D': 0, 'LTR2C': 0, 'AluY': 0, 'LTR64': 0, 'LTR1D1': 0, 'LTR38A1': 0, 'L2B': 0, 'MER74A': 0, 'LTR9B': 0, 'LTR24B': 0, 'MER57C1': 0, 'L1ME2': 0, 'LTR9A1': 0, 'LTR14C': 0, 'LTR62': 0, 'LTR44': 0, 'LTR60': 0, 'MER31B': 0, 'LTR77': 0, 'L1PA14_5': 0, 'MER34D': 0, 'MER57B2': 0, 'LTR37B': 0, 'HERV17': 0, 'MER9': 0, 'LTR26': 0, 'LTR1F': 0, 'LTR12E': 0, 'HERVK11DI': 0, 'LTR2752': 0, 'L1PBA1_5': 0, 'MER67C': 0, 'MIRc': 0, 'MER41E': 0, 'PrimLTR79': 0, 'PABL_A': 0, 'LTR46': 0, 'LOR1I': 0, 'LTR12B': 0, 'AluYa1': 0, 'MLT1J': 0, 'L1MC4': 0, 'L1M1B_5': 0, 'LTR40A': 0, 'HERVI': 0, 'MER57D': 0, 'MLT1H': 0, 'MER34-int': 0, 'LTR26B': 0, 'MER34C2': 0, 'MER67A': 0, 'LTR17': 0, 'MER52C': 0, 'L1ME3A': 0, 'AluYd8': 0, 'MER66C': 0, 'HERV-K14I': 0, 'MER21B': 0, 'MLT1_I': 0, 'MER4D_LTR': 0, 'AluYd3': 0, 'LTR51': 0, 'LTR72B': 0, 'MER88': 0, 'LTR59': 0, 'LTR22C0': 0, 'ALU': 0, 'L1PA7_5': 0, 'HERVK': 0, 'L1P4d_5end': 0, 'L1MD2': 0, 'MER50B': 0, 'L1MB4_5': 0, 'HUERS-P1': 0, 'AluJo': 0, 'LTR6A': 0, 'HERVL': 0, 'IN25': 0, 'MER50C': 0, 'MER21C': 0, 'LTR12': 0, 'AluYb3a2': 0, 'MLT1H1': 0, 'MER4CL34': 0, 'MER67D': 0, 'LTR12C': 0, 'LTR71B': 0, 'LTR39': 0, 'LTR41': 0, 'LTR54B': 0, 'L1ME3E_3end': 0, 'FAM': 0, 'LTR7C': 0, 'LTR8B': 0, 'PABL_B': 0, 'L1PA17_5': 0, 'MLT1C1': 0, 'MER57A1': 0, 'L1HS': 0, 'MER51B': 0, 'LTR19A': 0, 'MER41D': 0, 'LTR53B': 0, 'AluJr4': 0, 'HERVH': 0, 'MER68A': 0, 'L1MCA_5': 0, 'LTR1B': 0, 'MER11D': 0, 'LTR48B': 0, 'HERV35I': 0, 'L1PBB_5': 0, 'L1PA13_5': 0, 'ERVL': 0, 'HERVIP10FH': 0, 'LTR9D': 0, 'ERV3-16A3_I': 0, 'LTR13A': 0, 'L1PREC1': 0, 'L1PBA_5': 0, 'MER92B': 0, 'LTR54': 0, 'L1M3DE_5': 0, 'HERV19I': 0, 'LTR40C': 0, 'L1MA8': 0}}\n"
     ]
    }
   ],
   "source": [
    "mogonet_eval.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HERV-Fc1',\n",
       " 'LTR10C',\n",
       " 'L1M3C_5',\n",
       " 'LTR27E',\n",
       " 'L2',\n",
       " 'L1P4b_5end',\n",
       " 'LTR18A',\n",
       " 'MER87',\n",
       " 'MER57E3',\n",
       " 'THER2',\n",
       " 'MLT1K',\n",
       " 'LTR70',\n",
       " 'LTR1F1',\n",
       " 'MER54B',\n",
       " 'LTR28',\n",
       " 'LTR21A',\n",
       " 'MIR3',\n",
       " 'LTR2',\n",
       " 'MER51E',\n",
       " 'LTR27B',\n",
       " 'L1M3D_5',\n",
       " 'HERVE_a',\n",
       " 'LTR34',\n",
       " 'MER57F',\n",
       " 'LTR38C',\n",
       " 'LTR3',\n",
       " 'L1MA9_5',\n",
       " 'L1P4c_5end',\n",
       " 'HARLEQUIN',\n",
       " 'LTR1C1',\n",
       " 'LTR36',\n",
       " 'LTR60B',\n",
       " 'L1ME3C_3end',\n",
       " 'LOR1b_LTR',\n",
       " 'HERVS71',\n",
       " 'LTR1C',\n",
       " 'LTR47A2',\n",
       " 'MER66A',\n",
       " 'LTR16A1',\n",
       " 'LTR24',\n",
       " 'HERVL66I',\n",
       " 'L1ME4',\n",
       " 'MER57E1',\n",
       " 'MER68B',\n",
       " 'MER70A',\n",
       " 'LTR26E',\n",
       " 'LTR57',\n",
       " 'MER61B',\n",
       " 'AluYf5',\n",
       " 'MLT1G3',\n",
       " 'MER9B',\n",
       " 'L1ME5_3end',\n",
       " 'MER66D',\n",
       " 'LTR58',\n",
       " 'LTR25',\n",
       " 'LTR75_1',\n",
       " 'MER65C',\n",
       " 'LTR1B1',\n",
       " 'LTR71A',\n",
       " 'MER83C',\n",
       " 'HERV-K14CI',\n",
       " 'LTR2B',\n",
       " 'MER52A',\n",
       " 'LTR15',\n",
       " 'MER101',\n",
       " 'MER34A',\n",
       " 'SVA_D',\n",
       " 'HERV1_LTRb',\n",
       " 'LTR25-int',\n",
       " 'LTR72',\n",
       " 'MER66B',\n",
       " 'FRAM',\n",
       " 'HERVE',\n",
       " 'LTR53',\n",
       " 'MLT2D',\n",
       " 'LTR2C',\n",
       " 'AluY',\n",
       " 'LTR64',\n",
       " 'LTR1D1',\n",
       " 'LTR38A1',\n",
       " 'L2B',\n",
       " 'MER74A',\n",
       " 'LTR9B',\n",
       " 'LTR24B',\n",
       " 'MER57C1',\n",
       " 'L1ME2',\n",
       " 'LTR9A1',\n",
       " 'LTR14C',\n",
       " 'LTR62',\n",
       " 'LTR44',\n",
       " 'LTR60',\n",
       " 'MER31B',\n",
       " 'LTR77',\n",
       " 'L1PA14_5',\n",
       " 'MER34D',\n",
       " 'MER57B2',\n",
       " 'LTR37B',\n",
       " 'HERV17',\n",
       " 'MER9',\n",
       " 'LTR26',\n",
       " 'LTR1F',\n",
       " 'LTR12E',\n",
       " 'HERVK11DI',\n",
       " 'LTR2752',\n",
       " 'L1PBA1_5',\n",
       " 'MER67C',\n",
       " 'MIRc',\n",
       " 'MER41E',\n",
       " 'PrimLTR79',\n",
       " 'PABL_A',\n",
       " 'LTR46',\n",
       " 'LOR1I',\n",
       " 'LTR12B',\n",
       " 'AluYa1',\n",
       " 'MLT1J',\n",
       " 'L1MC4',\n",
       " 'L1M1B_5',\n",
       " 'LTR40A',\n",
       " 'HERVI',\n",
       " 'MER57D',\n",
       " 'MLT1H',\n",
       " 'MER34-int',\n",
       " 'LTR26B',\n",
       " 'MER34C2',\n",
       " 'MER67A',\n",
       " 'LTR17',\n",
       " 'MER52C',\n",
       " 'L1ME3A',\n",
       " 'AluYd8',\n",
       " 'MER66C',\n",
       " 'HERV-K14I',\n",
       " 'MER21B',\n",
       " 'MLT1_I',\n",
       " 'MER4D_LTR',\n",
       " 'AluYd3',\n",
       " 'LTR51',\n",
       " 'LTR72B',\n",
       " 'MER88',\n",
       " 'LTR59',\n",
       " 'LTR22C0',\n",
       " 'ALU',\n",
       " 'L1PA7_5',\n",
       " 'HERVK',\n",
       " 'L1P4d_5end',\n",
       " 'L1MD2',\n",
       " 'MER50B',\n",
       " 'L1MB4_5',\n",
       " 'HUERS-P1',\n",
       " 'AluJo',\n",
       " 'LTR6A',\n",
       " 'HERVL',\n",
       " 'IN25',\n",
       " 'MER50C',\n",
       " 'MER21C',\n",
       " 'LTR12',\n",
       " 'AluYb3a2',\n",
       " 'MLT1H1',\n",
       " 'MER4CL34',\n",
       " 'MER67D',\n",
       " 'LTR12C',\n",
       " 'LTR71B',\n",
       " 'LTR39',\n",
       " 'LTR41',\n",
       " 'LTR54B',\n",
       " 'L1ME3E_3end',\n",
       " 'FAM',\n",
       " 'LTR7C',\n",
       " 'LTR8B',\n",
       " 'PABL_B',\n",
       " 'L1PA17_5',\n",
       " 'MLT1C1',\n",
       " 'MER57A1',\n",
       " 'L1HS',\n",
       " 'MER51B',\n",
       " 'LTR19A',\n",
       " 'MER41D',\n",
       " 'LTR53B',\n",
       " 'AluJr4',\n",
       " 'HERVH',\n",
       " 'MER68A',\n",
       " 'L1MCA_5',\n",
       " 'LTR1B',\n",
       " 'MER11D',\n",
       " 'LTR48B',\n",
       " 'HERV35I',\n",
       " 'L1PBB_5',\n",
       " 'L1PA13_5',\n",
       " 'ERVL',\n",
       " 'HERVIP10FH',\n",
       " 'LTR9D',\n",
       " 'ERV3-16A3_I',\n",
       " 'LTR13A',\n",
       " 'L1PREC1',\n",
       " 'L1PBA_5',\n",
       " 'MER92B',\n",
       " 'LTR54',\n",
       " 'L1M3DE_5',\n",
       " 'HERV19I',\n",
       " 'LTR40C',\n",
       " 'L1MA8']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mogonet_eval.data_manager.get_split(0)\n",
    "mogonet_eval.data_manager.feature_names\n",
    "# data['mrna']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:38:30,432] A new study created in memory with name: no-name-53d82c88-b37a-43ac-a498-90cf04fcfd67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.5676)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4404, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1544, Train Acc: 0.9153, Train F1 Macro: 0.8282, Train F1 Weighted: 0.9090\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1415, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9067\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0740, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0696, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0755, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0174, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3212, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1876, Train Acc: 0.9322, Train F1 Macro: 0.8689, Train F1 Weighted: 0.9291\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1684, Train Acc: 0.8983, Train F1 Macro: 0.7569, Train F1 Weighted: 0.8794\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1281, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0852, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7545, Val Geometric Mean: 0.7145\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7545\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0536, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0803, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9501\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.4054)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6216)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3104, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1878, Train Acc: 0.8644, Train F1 Macro: 0.6758, Train F1 Weighted: 0.8393\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2349, Train Acc: 0.8983, Train F1 Macro: 0.8324, Train F1 Weighted: 0.9019\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2634, Train Acc: 0.8475, Train F1 Macro: 0.6563, Train F1 Weighted: 0.8257\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1047, Train Acc: 0.9661, Train F1 Macro: 0.9398, Train F1 Weighted: 0.9661\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1049, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1526, Train Acc: 0.9322, Train F1 Macro: 0.8796, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.8108)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(16.0270)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3331, Train Acc: 0.8305, Train F1 Macro: 0.6385, Train F1 Weighted: 0.8037\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2197, Train Acc: 0.8983, Train F1 Macro: 0.8520, Train F1 Weighted: 0.9039\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1594, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1526, Train Acc: 0.9492, Train F1 Macro: 0.9190, Train F1 Weighted: 0.9500\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1313, Train Acc: 0.9322, Train F1 Macro: 0.8883, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.6591, Val F1 Weighted: 0.7758, Val Geometric Mean: 0.7211\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.6591, Test F1 Weighted: 0.7758\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1331, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8115\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0485, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8833, Val Geometric Mean: 0.8463\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8833\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7432)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(6) tensor(0) tensor(15.5135)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2777, Train Acc: 0.8333, Train F1 Macro: 0.5370, Train F1 Weighted: 0.7716\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1101, Train Acc: 0.9667, Train F1 Macro: 0.9443, Train F1 Weighted: 0.9667\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1249, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1722, Train Acc: 0.8833, Train F1 Macro: 0.8117, Train F1 Weighted: 0.8853\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.6500, Val F1 Weighted: 0.7571, Val Geometric Mean: 0.7058\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.6500, Test F1 Weighted: 0.7571\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0373, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0350, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7857, Val F1 Macro: 0.7143, Val F1 Weighted: 0.8163, Val Geometric Mean: 0.7709\n",
      "Test Acc: 0.7857, Test F1 Macro: 0.7143, Test F1 Weighted: 0.8163\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:40:32,404] Trial 0 finished with value: 0.9491196586666667 and parameters: {}. Best is trial 0 with value: 0.9491196586666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0700, Train Acc: 0.9833, Train F1 Macro: 0.9731, Train F1 Weighted: 0.9836\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.7879, Val F1 Weighted: 0.8745, Val Geometric Mean: 0.8390\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.7879, Test F1 Weighted: 0.8745\n",
      "##################################################\n",
      "New best score: 0.949\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.9333333333333333, 'f1_macro': np.float64(0.88), 'f1_weighted': np.float64(0.928)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.987 ± 0.027\n",
      "F1 Macro: 0.976 ± 0.048\n",
      "F1 Weighted: 0.986 ± 0.029\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "three_layers = True\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": three_layers,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT 3L\" if three_layers else \"BiRGAT 2L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mrna_mirna_te.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrna, mirna, circrna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, circrna, 2L no interactions\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.918 ± 0.113\n",
    "F1 Weighted: 0.953 ± 0.064\n",
    "---\n",
    "Accuracy: 0.946 ± 0.027\n",
    "F1 Macro: 0.904 ± 0.048\n",
    "F1 Weighted: 0.944 ± 0.028\n",
    "# mrna, mirna, circrna 3L, interactions, degree ~20 in diff exp graphs, larger degree shows degraded performance\n",
    "# making the avg degree to high shows large jumps on the validation set during training\n",
    "Accuracy: 0.945 ± 0.053\n",
    "F1 Macro: 0.910 ± 0.081\n",
    "F1 Weighted: 0.946 ± 0.048\n",
    "# mrna, mirna, circrna 2L, interactions, 64 cap\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053\n",
    "# mrna, mirna, circrna 3L, interactions, 64 cap\n",
    "Accuracy: 0.891 ± 0.054\n",
    "F1 Macro: 0.801 ± 0.088\n",
    "F1 Weighted: 0.888 ± 0.056\n",
    "# mrna, mirna, circrna 3L\n",
    "Accuracy: 0.920 ± 0.050\n",
    "F1 Macro: 0.829 ± 0.112\n",
    "F1 Weighted: 0.907 ± 0.062\n",
    "# mrna, mirna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, 3L\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.897 ± 0.089\n",
    "F1 Weighted: 0.944 ± 0.051"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
