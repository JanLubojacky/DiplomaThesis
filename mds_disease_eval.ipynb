{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_disease/mds_disease_mrna_mirna_circrna.csv'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_disease/mds_disease_{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:00,120] A new study created in memory with name: no-name-e7bff091-bee4-44c3-94ba-29bdbe47c411\n",
      "[I 2024-11-14 13:01:00,198] Trial 0 finished with value: 0.5085070524273005 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,264] Trial 1 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,329] Trial 2 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,395] Trial 3 finished with value: 0.4477242436013849 and parameters: {'n_neighbors': 11}. Best is trial 0 with value: 0.5085070524273005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.509\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.025\n",
      "F1 Macro: 0.682 ± 0.116\n",
      "F1 Weighted: 0.848 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:00,464] Trial 4 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,527] Trial 5 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,590] Trial 6 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,652] Trial 7 finished with value: 0.27794757275052406 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,714] Trial 8 finished with value: 0.32661842676062103 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.5085070524273005.\n",
      "[I 2024-11-14 13:01:00,776] Trial 9 finished with value: 0.6139268814359082 and parameters: {'n_neighbors': 3}. Best is trial 9 with value: 0.6139268814359082.\n",
      "[I 2024-11-14 13:01:00,875] Trial 10 finished with value: 0.4419268561846254 and parameters: {'n_neighbors': 1}. Best is trial 9 with value: 0.6139268814359082.\n",
      "[I 2024-11-14 13:01:00,942] Trial 11 finished with value: 0.5057181971981872 and parameters: {'n_neighbors': 4}. Best is trial 9 with value: 0.6139268814359082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.614\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.032\n",
      "F1 Macro: 0.782 ± 0.064\n",
      "F1 Weighted: 0.880 ± 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:01,007] Trial 12 finished with value: 0.46772551930590295 and parameters: {'n_neighbors': 7}. Best is trial 9 with value: 0.6139268814359082.\n",
      "[I 2024-11-14 13:01:01,071] Trial 13 finished with value: 0.5296409475158396 and parameters: {'n_neighbors': 8}. Best is trial 9 with value: 0.6139268814359082.\n",
      "[I 2024-11-14 13:01:01,135] Trial 14 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 6}. Best is trial 14 with value: 0.6528081564994511.\n",
      "[I 2024-11-14 13:01:01,199] Trial 15 finished with value: 0.6139268814359082 and parameters: {'n_neighbors': 3}. Best is trial 14 with value: 0.6528081564994511.\n",
      "[I 2024-11-14 13:01:01,264] Trial 16 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 5}. Best is trial 14 with value: 0.6528081564994511.\n",
      "[I 2024-11-14 13:01:01,334] Trial 17 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 6}. Best is trial 14 with value: 0.6528081564994511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.653\n",
      "Best model performance:\n",
      "Accuracy: 0.907 ± 0.053\n",
      "F1 Macro: 0.806 ± 0.106\n",
      "F1 Weighted: 0.893 ± 0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:01,403] Trial 18 finished with value: 0.5085070524273005 and parameters: {'n_neighbors': 10}. Best is trial 14 with value: 0.6528081564994511.\n",
      "[I 2024-11-14 13:01:01,469] Trial 19 finished with value: 0.6528081564994511 and parameters: {'n_neighbors': 5}. Best is trial 14 with value: 0.6528081564994511.\n",
      "[I 2024-11-14 13:01:01,470] A new study created in memory with name: no-name-39c8076c-fcec-4692-ab9d-dba5ce98c8d0\n",
      "[I 2024-11-14 13:01:01,541] Trial 0 finished with value: 0.5809969155423703 and parameters: {'C': 0.027203803863838624, 'class_weight': 'balanced', 'rfe_step': 0.16047342946164844, 'rfe_n_features': 181}. Best is trial 0 with value: 0.5809969155423703.\n",
      "[I 2024-11-14 13:01:01,615] Trial 1 finished with value: 0.5614209168802691 and parameters: {'C': 0.01808785237908672, 'class_weight': 'balanced', 'rfe_step': 0.10392392789249999, 'rfe_n_features': 176}. Best is trial 0 with value: 0.5809969155423703.\n",
      "[I 2024-11-14 13:01:01,675] Trial 2 finished with value: 0.32661842676062103 and parameters: {'C': 0.01686429575356097, 'class_weight': None, 'rfe_step': 0.1286505851255647, 'rfe_n_features': 178}. Best is trial 0 with value: 0.5809969155423703.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.581\n",
      "Best model performance:\n",
      "Accuracy: 0.867 ± 0.084\n",
      "F1 Macro: 0.775 ± 0.120\n",
      "F1 Weighted: 0.865 ± 0.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:01,794] Trial 3 finished with value: 0.7389770960193865 and parameters: {'C': 0.25817777503003897, 'class_weight': None, 'rfe_step': 0.13820158797311796, 'rfe_n_features': 119}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:01,952] Trial 4 finished with value: 0.7389770960193865 and parameters: {'C': 1.3686301478977219, 'class_weight': 'balanced', 'rfe_step': 0.116743621783358, 'rfe_n_features': 155}. Best is trial 3 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.739\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:02,145] Trial 5 finished with value: 0.7039337563024542 and parameters: {'C': 0.6319302254447319, 'class_weight': 'balanced', 'rfe_step': 0.11227973720404164, 'rfe_n_features': 126}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:02,274] Trial 6 finished with value: 0.7389770960193865 and parameters: {'C': 0.8660283351283377, 'class_weight': None, 'rfe_step': 0.16250197355361073, 'rfe_n_features': 164}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:02,497] Trial 7 finished with value: 0.669993140387569 and parameters: {'C': 0.19390436048362267, 'class_weight': 'balanced', 'rfe_step': 0.06296156664023142, 'rfe_n_features': 111}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:02,635] Trial 8 finished with value: 0.7389770960193865 and parameters: {'C': 1.17779955365954, 'class_weight': None, 'rfe_step': 0.1786027121695583, 'rfe_n_features': 172}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:02,738] Trial 9 finished with value: 0.6069596596049786 and parameters: {'C': 0.09752985730972155, 'class_weight': 'balanced', 'rfe_step': 0.17476216398675276, 'rfe_n_features': 152}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:03,039] Trial 10 finished with value: 0.7389770960193865 and parameters: {'C': 5.287135093635365, 'class_weight': None, 'rfe_step': 0.06885221164224825, 'rfe_n_features': 131}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:03,234] Trial 11 finished with value: 0.7389770960193865 and parameters: {'C': 3.7856391934229903, 'class_weight': None, 'rfe_step': 0.13627859577520177, 'rfe_n_features': 138}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:03,328] Trial 12 finished with value: 0.6069596596049786 and parameters: {'C': 0.0966390031036904, 'class_weight': 'balanced', 'rfe_step': 0.09121057780723911, 'rfe_n_features': 199}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:03,529] Trial 13 finished with value: 0.7389770960193865 and parameters: {'C': 2.2290553005190104, 'class_weight': None, 'rfe_step': 0.14221628214954404, 'rfe_n_features': 104}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:03,701] Trial 14 finished with value: 0.6682316917536709 and parameters: {'C': 0.3563837325460255, 'class_weight': 'balanced', 'rfe_step': 0.0863515654245694, 'rfe_n_features': 153}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:03,893] Trial 15 finished with value: 0.7389770960193865 and parameters: {'C': 9.582479628034005, 'class_weight': None, 'rfe_step': 0.14737426352716687, 'rfe_n_features': 118}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:04,037] Trial 16 finished with value: 0.6354207758053912 and parameters: {'C': 0.17571088152439385, 'class_weight': 'balanced', 'rfe_step': 0.11548427149054162, 'rfe_n_features': 143}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:04,140] Trial 17 finished with value: 0.553150014770979 and parameters: {'C': 0.05428869205817944, 'class_weight': None, 'rfe_step': 0.09546455365166193, 'rfe_n_features': 100}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:04,242] Trial 18 finished with value: 0.7389770960193865 and parameters: {'C': 0.4241404312795764, 'class_weight': None, 'rfe_step': 0.19992818033430756, 'rfe_n_features': 163}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:04,463] Trial 19 finished with value: 0.7389770960193865 and parameters: {'C': 1.7459740750222092, 'class_weight': 'balanced', 'rfe_step': 0.12218649388393371, 'rfe_n_features': 125}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:04,554] Trial 20 finished with value: 0.7389770960193865 and parameters: {'C': 0.19029383041555373, 'class_weight': None, 'rfe_step': 0.05154277925371331, 'rfe_n_features': 192}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:04,695] Trial 21 finished with value: 0.7389770960193865 and parameters: {'C': 0.8142151651665807, 'class_weight': None, 'rfe_step': 0.15793702930972045, 'rfe_n_features': 163}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:04,834] Trial 22 finished with value: 0.7389770960193865 and parameters: {'C': 0.8063200329207653, 'class_weight': None, 'rfe_step': 0.1620101112614664, 'rfe_n_features': 163}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:04,981] Trial 23 finished with value: 0.7389770960193865 and parameters: {'C': 1.5977915882677733, 'class_weight': None, 'rfe_step': 0.18953709887139236, 'rfe_n_features': 143}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:05,105] Trial 24 finished with value: 0.7389770960193865 and parameters: {'C': 0.4782902494800297, 'class_weight': None, 'rfe_step': 0.13186223876437822, 'rfe_n_features': 156}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:05,303] Trial 25 finished with value: 0.7389770960193865 and parameters: {'C': 2.2738502537727503, 'class_weight': None, 'rfe_step': 0.14919409249736135, 'rfe_n_features': 139}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:05,439] Trial 26 finished with value: 0.7389770960193865 and parameters: {'C': 4.186722390488998, 'class_weight': 'balanced', 'rfe_step': 0.1754020285801436, 'rfe_n_features': 168}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:05,569] Trial 27 finished with value: 0.7389770960193865 and parameters: {'C': 0.28312198595196, 'class_weight': None, 'rfe_step': 0.11797412590672007, 'rfe_n_features': 147}. Best is trial 3 with value: 0.7389770960193865.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-14 13:01:05,704] Trial 28 finished with value: 0.7389770960193865 and parameters: {'C': 1.2126199792285584, 'class_weight': 'balanced', 'rfe_step': 0.1513882402546037, 'rfe_n_features': 187}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:05,801] Trial 29 finished with value: 0.6069596596049786 and parameters: {'C': 0.037624530805039476, 'class_weight': 'balanced', 'rfe_step': 0.16199367038157744, 'rfe_n_features': 132}. Best is trial 3 with value: 0.7389770960193865.\n",
      "[I 2024-11-14 13:01:05,802] A new study created in memory with name: no-name-3a83e024-7aac-4a45-943b-47f26da19693\n",
      "[I 2024-11-14 13:01:05,913] Trial 0 finished with value: 0.7526861283892176 and parameters: {'lambda': 2.105127381564312e-06, 'alpha': 1.576981030318588e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,000] Trial 1 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.03142260650398809, 'alpha': 3.846693037563679e-05}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,084] Trial 2 finished with value: 0.7021274450808679 and parameters: {'lambda': 2.799996896858968e-06, 'alpha': 0.0003090763135059519}. Best is trial 0 with value: 0.7526861283892176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.753\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.870 ± 0.092\n",
      "F1 Weighted: 0.927 ± 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:06,171] Trial 3 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.01694761860106469, 'alpha': 1.4864468309677284e-05}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,259] Trial 4 finished with value: 0.553150014770979 and parameters: {'lambda': 0.030960126688156214, 'alpha': 0.0473458448805854}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,348] Trial 5 finished with value: 0.27794757275052406 and parameters: {'lambda': 2.6701102970387854e-07, 'alpha': 0.6694793346250602}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,454] Trial 6 finished with value: 0.7389770960193865 and parameters: {'lambda': 2.204321254027931e-06, 'alpha': 0.0264702001022464}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,553] Trial 7 finished with value: 0.553150014770979 and parameters: {'lambda': 0.04626876857643086, 'alpha': 0.042348611253685685}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,638] Trial 8 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.010212456745019166, 'alpha': 0.0014220010071685483}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,724] Trial 9 finished with value: 0.7021274450808679 and parameters: {'lambda': 7.577963897144814e-08, 'alpha': 0.012214119423808222}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,834] Trial 10 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00022104621025440611, 'alpha': 6.675170718440821e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:06,942] Trial 11 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.0001832158922390535, 'alpha': 1.3621246698963732e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,072] Trial 12 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00015287340933625905, 'alpha': 1.4874460505844247e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,173] Trial 13 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.0582529343186502e-05, 'alpha': 8.642830770279141e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,265] Trial 14 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.0008947648437635574, 'alpha': 5.158442645309809e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,368] Trial 15 finished with value: 0.7526861283892176 and parameters: {'lambda': 2.229165956346104e-05, 'alpha': 2.2449697985920328e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,494] Trial 16 finished with value: 0.5239116671839724 and parameters: {'lambda': 0.8138995145469369, 'alpha': 4.19807804741472e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,586] Trial 17 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.2026785672144414e-08, 'alpha': 1.1315256935781115e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,673] Trial 18 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.001261070314766432, 'alpha': 7.510003052137473e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,765] Trial 19 finished with value: 0.7526861283892176 and parameters: {'lambda': 4.508958653042668e-07, 'alpha': 3.2477694409907636e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,858] Trial 20 finished with value: 0.7526861283892176 and parameters: {'lambda': 5.439116898163323e-05, 'alpha': 3.81154182076013e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:07,948] Trial 21 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.0005198004714656804, 'alpha': 3.003949052392976e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,034] Trial 22 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00013088004475206747, 'alpha': 1.0038272965544625e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,141] Trial 23 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.0210372915538952e-05, 'alpha': 1.228898684142698e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,245] Trial 24 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.004303295378787423, 'alpha': 1.1392212490389404e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,347] Trial 25 finished with value: 0.7526861283892176 and parameters: {'lambda': 2.236879906991715e-06, 'alpha': 1.1187729236866153e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,450] Trial 26 finished with value: 0.7526861283892176 and parameters: {'lambda': 3.479020522537071e-05, 'alpha': 2.3459180088374775e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,547] Trial 27 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00026728234115784945, 'alpha': 5.613905831026411e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,641] Trial 28 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.001814224477345293, 'alpha': 0.00031012244064460785}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,737] Trial 29 finished with value: 0.7389770960193865 and parameters: {'lambda': 0.13878435850412707, 'alpha': 2.9889858159855807e-05}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,829] Trial 30 finished with value: 0.7526861283892176 and parameters: {'lambda': 5.756873167308573e-07, 'alpha': 4.9610099507575475e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:08,991] Trial 31 finished with value: 0.7526861283892176 and parameters: {'lambda': 5.537849211359574e-06, 'alpha': 1.0038790879787462e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,093] Trial 32 finished with value: 0.7172503758053912 and parameters: {'lambda': 1.4808457348758958e-05, 'alpha': 4.662905351461404e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,190] Trial 33 finished with value: 0.7526861283892176 and parameters: {'lambda': 8.722284810249691e-05, 'alpha': 2.42682334069644e-08}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,316] Trial 34 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.121082052702447e-06, 'alpha': 1.700399918620372e-07}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,414] Trial 35 finished with value: 0.7526861283892176 and parameters: {'lambda': 8.777981860023474e-08, 'alpha': 1.5472396189717367e-06}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,502] Trial 36 finished with value: 0.7526861283892176 and parameters: {'lambda': 6.719235498333443e-06, 'alpha': 0.00011579149333655385}. Best is trial 0 with value: 0.7526861283892176.\n",
      "[I 2024-11-14 13:01:09,605] Trial 37 finished with value: 0.7966829243918474 and parameters: {'lambda': 4.016917908953034e-05, 'alpha': 5.379168370005845e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:09,698] Trial 38 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.000507484322407358, 'alpha': 5.661339331890004e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:09,780] Trial 39 finished with value: 0.27794757275052406 and parameters: {'lambda': 0.004277855030735415, 'alpha': 0.5261043269157383}. Best is trial 37 with value: 0.7966829243918474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.797\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.894 ± 0.106\n",
      "F1 Weighted: 0.941 ± 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:09,873] Trial 40 finished with value: 0.7526861283892176 and parameters: {'lambda': 6.052428990708667e-05, 'alpha': 1.1933602260962324e-05}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:09,978] Trial 41 finished with value: 0.7526861283892176 and parameters: {'lambda': 4.3825441752949765e-06, 'alpha': 2.1328527830188963e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,074] Trial 42 finished with value: 0.7526861283892176 and parameters: {'lambda': 2.872659858507051e-05, 'alpha': 4.377963266123595e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,169] Trial 43 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00029039857179555636, 'alpha': 6.607391893569826e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,260] Trial 44 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.8588986711453626e-06, 'alpha': 2.15154443593222e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,388] Trial 45 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.62578792036912e-05, 'alpha': 2.3500675265352975e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,487] Trial 46 finished with value: 0.6725630157001974 and parameters: {'lambda': 0.00010570377365657571, 'alpha': 0.003171358909769967}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,575] Trial 47 finished with value: 0.7172503758053912 and parameters: {'lambda': 1.371348766855653e-07, 'alpha': 4.151537556873395e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,724] Trial 48 finished with value: 0.7526861283892176 and parameters: {'lambda': 3.846472171616564e-05, 'alpha': 9.579369214778451e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,819] Trial 49 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.339537266069786e-06, 'alpha': 1.05269231847638e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,908] Trial 50 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.0005419192982582396, 'alpha': 2.456223670846002e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:10,993] Trial 51 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.606629290147242e-05, 'alpha': 2.117529180350173e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,101] Trial 52 finished with value: 0.7526861283892176 and parameters: {'lambda': 0.00018419318901352025, 'alpha': 2.0156278238782085e-06}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,204] Trial 53 finished with value: 0.7526861283892176 and parameters: {'lambda': 3.82238812295094e-06, 'alpha': 7.20960236339654e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,304] Trial 54 finished with value: 0.7526861283892176 and parameters: {'lambda': 2.348656594367859e-05, 'alpha': 5.309754563996572e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,404] Trial 55 finished with value: 0.7526861283892176 and parameters: {'lambda': 9.5091013103301e-06, 'alpha': 1.0599091847230871e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,525] Trial 56 finished with value: 0.7526861283892176 and parameters: {'lambda': 6.703877505061961e-07, 'alpha': 2.7993705955272984e-07}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,616] Trial 57 finished with value: 0.7526861283892176 and parameters: {'lambda': 5.2449886930028436e-05, 'alpha': 3.728154702851285e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,703] Trial 58 finished with value: 0.7526861283892176 and parameters: {'lambda': 1.6901297574645e-08, 'alpha': 1.5913768066376888e-08}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,802] Trial 59 finished with value: 0.7021274450808679 and parameters: {'lambda': 0.0020575310456116725, 'alpha': 7.556449608452307e-06}. Best is trial 37 with value: 0.7966829243918474.\n",
      "[I 2024-11-14 13:01:11,803] A new study created in memory with name: no-name-b88f7387-a34e-4f52-a8b0-d6baaeadc2c2\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:13,149] Trial 0 finished with value: 0.7039337563024542 and parameters: {'lr': 0.005734429067071109, 'dropout': 0.10804118351912383}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:14,469] Trial 1 finished with value: 0.5814461133586074 and parameters: {'lr': 0.008498832441068865, 'dropout': 0.411026871622141}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:15,810] Trial 2 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00013105794687902047, 'dropout': 0.36049145819475836}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:17,165] Trial 3 finished with value: 0.6192439604955923 and parameters: {'lr': 0.002699174348026935, 'dropout': 0.4756475002891629}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:18,516] Trial 4 finished with value: 0.7039337563024542 and parameters: {'lr': 0.005055677514884795, 'dropout': 0.30732762686934223}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:19,844] Trial 5 finished with value: 0.669993140387569 and parameters: {'lr': 0.00783086699124396, 'dropout': 0.36092729227034936}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:21,226] Trial 6 finished with value: 0.6332788489801072 and parameters: {'lr': 0.0012111445066997938, 'dropout': 0.2398865265237151}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:22,618] Trial 7 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0007751430631246609, 'dropout': 0.3858584730065022}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:23,913] Trial 8 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00011236889138177313, 'dropout': 0.21099753947680308}. Best is trial 0 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-14 13:01:25,302] Trial 9 finished with value: 0.27794757275052406 and parameters: {'lr': 0.00017861208557496332, 'dropout': 0.14238788385434123}. Best is trial 0 with value: 0.7039337563024542.\n",
      "[I 2024-11-14 13:01:25,342] A new study created in memory with name: no-name-6aa54864-e984-445b-ae62-ca3289bb5bb3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_disease/mds_disease_te.csv\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.2859, Train Acc: 0.9322, Train F1 Macro: 0.8554, Train F1 Weighted: 0.9251\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.2600, Train Acc: 0.9322, Train F1 Macro: 0.8554, Train F1 Weighted: 0.9251\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.3707, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.3108, Train Acc: 0.8814, Train F1 Macro: 0.7327, Train F1 Weighted: 0.8577\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.3042, Train Acc: 0.8667, Train F1 Macro: 0.6765, Train F1 Weighted: 0.8336\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:32,206] Trial 0 finished with value: 0.6862235118921005 and parameters: {}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-14 13:01:32,246] A new study created in memory with name: no-name-dc528e16-61f4-4f53-b290-a7c4cf544e9d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.686\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.824 ± 0.109\n",
      "F1 Weighted: 0.906 ± 0.061\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4637, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4587, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4380, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.4034, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4435, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4479, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4495, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3001, Train Acc: 0.8983, Train F1 Macro: 0.7569, Train F1 Weighted: 0.8794\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4629, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4617, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4640, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.4292, Train Acc: 0.8475, Train F1 Macro: 0.5489, Train F1 Weighted: 0.7915\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5002, Train Acc: 0.8136, Train F1 Macro: 0.4486, Train F1 Weighted: 0.7299\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048, Val Geometric Mean: 0.6867\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4909, Train Acc: 0.8136, Train F1 Macro: 0.4486, Train F1 Weighted: 0.7299\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048, Val Geometric Mean: 0.6867\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4226, Train Acc: 0.8136, Train F1 Macro: 0.4486, Train F1 Weighted: 0.7299\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048, Val Geometric Mean: 0.6867\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3283, Train Acc: 0.8305, Train F1 Macro: 0.6722, Train F1 Weighted: 0.8151\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.6500, Val Geometric Mean: 0.5269\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.6500\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4641, Train Acc: 0.8167, Train F1 Macro: 0.4495, Train F1 Weighted: 0.7343\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.4812, Train Acc: 0.8167, Train F1 Macro: 0.4495, Train F1 Weighted: 0.7343\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4507, Train Acc: 0.8167, Train F1 Macro: 0.4495, Train F1 Weighted: 0.7343\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 13:01:48,214] Trial 0 finished with value: 0.3717283965781297 and parameters: {}. Best is trial 0 with value: 0.3717283965781297.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.5122, Train Acc: 0.8167, Train F1 Macro: 0.4495, Train F1 Weighted: 0.7343\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "New best score: 0.372\n",
      "Best model performance:\n",
      "Accuracy: 0.811 ± 0.077\n",
      "F1 Macro: 0.586 ± 0.116\n",
      "F1 Weighted: 0.781 ± 0.067\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:49:43,818] A new study created in memory with name: no-name-15fc25c5-235a-413c-9c09-c3b99f2534ff\n",
      "[I 2024-11-17 17:49:43,979] Trial 0 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0005522152199585571, 'alpha': 1.5631575756388323e-06}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:44,126] Trial 1 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0009441682055922977, 'alpha': 1.1298850310301638e-07}. Best is trial 0 with value: 0.806425114231087.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.806\n",
      "Best model performance:\n",
      "Accuracy: 0.946 ± 0.027\n",
      "F1 Macro: 0.904 ± 0.048\n",
      "F1 Weighted: 0.944 ± 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:49:44,285] Trial 2 finished with value: 0.43192671408544425 and parameters: {'lambda': 0.004917887497782284, 'alpha': 0.11819472221563164}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:44,500] Trial 3 finished with value: 0.6460639796087643 and parameters: {'lambda': 3.5013820406033616e-06, 'alpha': 0.018691467676913313}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:44,654] Trial 4 finished with value: 0.7948768203234714 and parameters: {'lambda': 0.008328351583202227, 'alpha': 2.072834250567776e-06}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:44,800] Trial 5 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.23693265707256447, 'alpha': 1.2992226851648535e-08}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:44,945] Trial 6 finished with value: 0.6969195446998454 and parameters: {'lambda': 0.0264117036771058, 'alpha': 0.012315385529456572}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,094] Trial 7 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0021196676278370363, 'alpha': 1.4289822725374239e-08}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,243] Trial 8 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0006422752007649365, 'alpha': 2.308548604487028e-07}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,399] Trial 9 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.09404517783132074, 'alpha': 1.3747738050425402e-05}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,585] Trial 10 finished with value: 0.6022120594382817 and parameters: {'lambda': 1.2489713640399204e-08, 'alpha': 0.0003260222058961784}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,750] Trial 11 finished with value: 0.6684310669837741 and parameters: {'lambda': 1.1161650038514754e-05, 'alpha': 7.017661139944055e-07}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:45,910] Trial 12 finished with value: 0.6022120594382817 and parameters: {'lambda': 3.7047518878122055e-05, 'alpha': 0.0001045620574545041}. Best is trial 0 with value: 0.806425114231087.\n",
      "[I 2024-11-17 17:49:46,069] Trial 13 finished with value: 0.8127007161803061 and parameters: {'lambda': 0.00017951445598540832, 'alpha': 2.2940842423737112e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:46,228] Trial 14 finished with value: 0.6969916972772877 and parameters: {'lambda': 6.621821411319224e-07, 'alpha': 1.0904296584799354e-05}. Best is trial 13 with value: 0.8127007161803061.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.813\n",
      "Best model performance:\n",
      "Accuracy: 0.946 ± 0.027\n",
      "F1 Macro: 0.909 ± 0.047\n",
      "F1 Weighted: 0.945 ± 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:49:46,389] Trial 15 finished with value: 0.7693788056664252 and parameters: {'lambda': 0.00016310395977176353, 'alpha': 6.635973437137921e-06}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:46,586] Trial 16 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.9772311749606422, 'alpha': 0.00036215662812966583}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:46,748] Trial 17 finished with value: 0.6684310669837741 and parameters: {'lambda': 4.133461888317546e-07, 'alpha': 6.681742465534682e-08}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:46,906] Trial 18 finished with value: 0.7699660701774514 and parameters: {'lambda': 0.00011760138805022151, 'alpha': 2.440916720271865e-06}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,066] Trial 19 finished with value: 0.6969195446998454 and parameters: {'lambda': 0.0003333712475177214, 'alpha': 4.2055012276993824e-05}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,231] Trial 20 finished with value: 0.7362098798554709 and parameters: {'lambda': 3.177026201532607e-05, 'alpha': 4.466364143248948e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,391] Trial 21 finished with value: 0.806425114231087 and parameters: {'lambda': 0.001325490694273811, 'alpha': 1.47630374323505e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,569] Trial 22 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0170513006758109, 'alpha': 4.009381715331721e-08}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,723] Trial 23 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0006368990815510351, 'alpha': 9.831328455515832e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:47,882] Trial 24 finished with value: 0.6684310669837741 and parameters: {'lambda': 7.181741731078519e-06, 'alpha': 1.0091234554315909e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,040] Trial 25 finished with value: 0.7362098798554709 and parameters: {'lambda': 4.3385004087183587e-05, 'alpha': 2.1872069633486417e-06}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,194] Trial 26 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0020561233355916265, 'alpha': 2.666943097081905e-08}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,358] Trial 27 finished with value: 0.7013240844574544 and parameters: {'lambda': 1.0943800980510772e-06, 'alpha': 2.6603756517544024e-07}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,548] Trial 28 finished with value: 0.6969195446998454 and parameters: {'lambda': 0.00018924085558448447, 'alpha': 3.9995013094820895e-05}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,722] Trial 29 finished with value: 0.6631836987350632 and parameters: {'lambda': 0.0057790470660645105, 'alpha': 0.0018848024090487403}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:48,883] Trial 30 finished with value: 0.6684310669837741 and parameters: {'lambda': 1.1343989046465644e-07, 'alpha': 6.776189395398926e-08}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:49,050] Trial 31 finished with value: 0.806425114231087 and parameters: {'lambda': 0.001295045562020868, 'alpha': 1.0764471958017134e-08}. Best is trial 13 with value: 0.8127007161803061.\n",
      "[I 2024-11-17 17:49:49,216] Trial 32 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.0030407459959847375, 'alpha': 2.7037726637421657e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:49,369] Trial 33 finished with value: 0.27794757275052406 and parameters: {'lambda': 0.02773937381780222, 'alpha': 0.41956749026453727}. Best is trial 32 with value: 0.8523939840000002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.852\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.033\n",
      "F1 Macro: 0.928 ± 0.059\n",
      "F1 Weighted: 0.957 ± 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:49:49,534] Trial 34 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0041929710964316805, 'alpha': 8.728473616443046e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:49,717] Trial 35 finished with value: 0.8523255135749392 and parameters: {'lambda': 0.0003730328218059249, 'alpha': 3.4751259612204713e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:49,877] Trial 36 finished with value: 0.806425114231087 and parameters: {'lambda': 0.00033894234388439265, 'alpha': 4.946794044412732e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,040] Trial 37 finished with value: 0.7013240844574544 and parameters: {'lambda': 1.7274186184011694e-05, 'alpha': 3.9886668717947476e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,199] Trial 38 finished with value: 0.7699660701774514 and parameters: {'lambda': 9.467265638899228e-05, 'alpha': 2.0189527871812583e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,360] Trial 39 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.06925911843282649, 'alpha': 2.5298671229667196e-05}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,525] Trial 40 finished with value: 0.7948768203234714 and parameters: {'lambda': 0.012341076430119377, 'alpha': 3.3523013009394094e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,750] Trial 41 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0006033812621393511, 'alpha': 1.2199892384820587e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:50,905] Trial 42 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.0030249652214339637, 'alpha': 2.1760116199454698e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,060] Trial 43 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.00420690536723231, 'alpha': 2.066538124410368e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,212] Trial 44 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.003738344108264832, 'alpha': 2.4570215816733663e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,366] Trial 45 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.003534581656255478, 'alpha': 5.223821207177752e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,520] Trial 46 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.06254252238155938, 'alpha': 2.246878613606786e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,694] Trial 47 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.004193463226712024, 'alpha': 6.334478480181987e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:51,854] Trial 48 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.3224864021302676, 'alpha': 4.765409519046762e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,007] Trial 49 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.026135190518898566, 'alpha': 1.6223401731136175e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,159] Trial 50 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.0026196208512805894, 'alpha': 1.1847533939789862e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,312] Trial 51 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.007638421286748926, 'alpha': 5.396316029606719e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,469] Trial 52 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.003757037795606135, 'alpha': 3.54925169494436e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,640] Trial 53 finished with value: 0.7948768203234714 and parameters: {'lambda': 0.01034327333252594, 'alpha': 1.95288800776646e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,791] Trial 54 finished with value: 0.806425114231087 and parameters: {'lambda': 0.0010409291211408947, 'alpha': 9.335308711186587e-08}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:52,942] Trial 55 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.1703039433646711, 'alpha': 8.529693951367404e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:53,091] Trial 56 finished with value: 0.6460639796087643 and parameters: {'lambda': 0.03093464769095658, 'alpha': 0.029797899806912866}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:53,243] Trial 57 finished with value: 0.806425114231087 and parameters: {'lambda': 0.003221700016962337, 'alpha': 2.035264565348717e-07}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:53,394] Trial 58 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.0016233209495874134, 'alpha': 1.2617482536767587e-06}. Best is trial 32 with value: 0.8523939840000002.\n",
      "[I 2024-11-17 17:49:53,568] Trial 59 finished with value: 0.8523939840000002 and parameters: {'lambda': 0.00633142165657283, 'alpha': 6.417681117894139e-07}. Best is trial 32 with value: 0.8523939840000002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.033\n",
      "F1 Macro: 0.928 ± 0.059\n",
      "F1 Weighted: 0.957 ± 0.035\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "# xgb_eval.save_results(results_file=save_folder, row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 12:46:25,884] A new study created in memory with name: no-name-a09d1753-b355-4d8b-9dab-5aa06e28e0d1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:28,495] Trial 0 finished with value: 0.6631836987350632 and parameters: {'lr': 0.004521333766373588, 'dropout': 0.5753805682085522}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:31,134] Trial 1 finished with value: 0.6192439604955923 and parameters: {'lr': 0.00017411872686215547, 'dropout': 0.12439247736015455}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:33,840] Trial 2 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0006480294749058075, 'dropout': 0.5298168732263804}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:36,493] Trial 3 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0008015486500913524, 'dropout': 0.5375695215584003}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:39,114] Trial 4 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0010227374444337156, 'dropout': 0.4508362955596811}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:41,841] Trial 5 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0002924128430761723, 'dropout': 0.19158635397257598}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:44,605] Trial 6 finished with value: 0.6631836987350632 and parameters: {'lr': 0.002001903765664147, 'dropout': 0.18475528737616684}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:47,261] Trial 7 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0010523154176540995, 'dropout': 0.24306862229806356}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:49,936] Trial 8 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0009862318980649978, 'dropout': 0.14681645176731092}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-17 12:46:52,620] Trial 9 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0017499051828280452, 'dropout': 0.3324187175255665}. Best is trial 0 with value: 0.6631836987350632.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "# mlp_eval.save_results(results_file=save_folder, row_name=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n",
      "Best hyperparameters:\n",
      "{'lr': 0.004521333766373588, 'dropout': 0.5753805682085522}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:59:30,901] A new study created in memory with name: no-name-06dadf4f-1116-41e1-b3a1-f279da0538d6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: attention integrator\n",
      "Using: attention integrator\n",
      "Using: attention integrator\n",
      "Using: attention integrator\n",
      "Using: attention integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:59:44,598] Trial 0 finished with value: 0.8999268693333332 and parameters: {}. Best is trial 0 with value: 0.8999268693333332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.900\n",
      "Best model performance:\n",
      "Accuracy: 0.973 ± 0.033\n",
      "F1 Macro: 0.952 ± 0.059\n",
      "F1 Weighted: 0.971 ± 0.035\n",
      "Best model performance:\n",
      "Accuracy: 0.973 ± 0.033\n",
      "F1 Macro: 0.952 ± 0.059\n",
      "F1 Weighted: 0.971 ± 0.035\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            # \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"attention\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 8,\n",
    "        \"vcdn_hidden_channels\": 8,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"mogonet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:47:24,249] A new study created in memory with name: no-name-8982798b-dd55-4149-a42e-5888ce9dfb5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.6892)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.1351)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2900, Train Acc: 0.8136, Train F1 Macro: 0.4486, Train F1 Weighted: 0.7451\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1890, Train Acc: 0.9322, Train F1 Macro: 0.8883, Train F1 Weighted: 0.9346\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.1008, Train Acc: 0.9322, Train F1 Macro: 0.8554, Train F1 Weighted: 0.9251\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0409, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.9068, Val F1 Weighted: 0.9366, Val Geometric Mean: 0.9255\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.9068, Test F1 Weighted: 0.9366\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0650, Train Acc: 0.9661, Train F1 Macro: 0.9398, Train F1 Weighted: 0.9661\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.6486)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(8) tensor(15.4730)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.1453, Train Acc: 0.9492, Train F1 Macro: 0.8969, Train F1 Weighted: 0.9454\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5130\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0565, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.6667, Val F1 Macro: 0.4000, Val F1 Weighted: 0.6400, Val Geometric Mean: 0.5547\n",
      "Test Acc: 0.6667, Test F1 Macro: 0.4000, Test F1 Weighted: 0.6400\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0883, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.6000, Val Geometric Mean: 0.5130\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.6000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1762, Train Acc: 0.8983, Train F1 Macro: 0.7569, Train F1 Weighted: 0.8794\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0727, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.4231, Val F1 Weighted: 0.6769, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.4231, Test F1 Weighted: 0.6769\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.4054)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(9) tensor(16.4189)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2685, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0431, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0132, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.4231, Val F1 Weighted: 0.6769, Val Geometric Mean: 0.5944\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.4231, Test F1 Weighted: 0.6769\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0050, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8409\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0241, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(25.7568)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.8108)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(9) tensor(16.3378)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2478, Train Acc: 0.8475, Train F1 Macro: 0.7393, Train F1 Weighted: 0.8446\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048, Val Geometric Mean: 0.6867\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1701, Train Acc: 0.8814, Train F1 Macro: 0.7973, Train F1 Weighted: 0.8791\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8115\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0439, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9387, Val Geometric Mean: 0.9169\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9387\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2281, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0007, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235, Val Geometric Mean: 0.8889\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.5676)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.7432)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(9) tensor(16.7297)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2303, Train Acc: 0.8167, Train F1 Macro: 0.4495, Train F1 Weighted: 0.7343\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912, Val Geometric Mean: 0.6790\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0276, Train Acc: 0.9833, Train F1 Macro: 0.9711, Train F1 Weighted: 0.9830\n",
      "Val Acc: 0.9286, Val F1 Macro: 0.8783, Val F1 Weighted: 0.9342, Val Geometric Mean: 0.9133\n",
      "Test Acc: 0.9286, Test F1 Macro: 0.8783, Test F1 Weighted: 0.9342\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0616, Train Acc: 0.9500, Train F1 Macro: 0.9062, Train F1 Weighted: 0.9468\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0005, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-17 17:48:28,504] Trial 0 finished with value: 0.8666250016074483 and parameters: {}. Best is trial 0 with value: 0.8666250016074483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n",
      "New best score: 0.867\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.940 ± 0.082\n",
      "F1 Weighted: 0.961 ± 0.053\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.940 ± 0.082\n",
      "F1 Weighted: 0.961 ± 0.053\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "# birgat_eval.save_results(results_file=save_folder, row_name=\"birgat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                # \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mrna, mirna, circrna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, circrna, 2L no interactions\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.918 ± 0.113\n",
    "F1 Weighted: 0.953 ± 0.064\n",
    "---\n",
    "Accuracy: 0.946 ± 0.027\n",
    "F1 Macro: 0.904 ± 0.048\n",
    "F1 Weighted: 0.944 ± 0.028\n",
    "# mrna, mirna, circrna 3L, interactions, degree ~20 in diff exp graphs, larger degree shows degraded performance\n",
    "# making the avg degree to high shows large jumps on the validation set during training\n",
    "Accuracy: 0.945 ± 0.053\n",
    "F1 Macro: 0.910 ± 0.081\n",
    "F1 Weighted: 0.946 ± 0.048\n",
    "# mrna, mirna, circrna 2L, interactions, 64 cap\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.940 ± 0.082\n",
    "F1 Weighted: 0.961 ± 0.053\n",
    "# mrna, mirna, circrna 3L, interactions, 64 cap\n",
    "Accuracy: 0.891 ± 0.054\n",
    "F1 Macro: 0.801 ± 0.088\n",
    "F1 Weighted: 0.888 ± 0.056\n",
    "# mrna, mirna, circrna 3L\n",
    "Accuracy: 0.920 ± 0.050\n",
    "F1 Macro: 0.829 ± 0.112\n",
    "F1 Weighted: 0.907 ± 0.062\n",
    "# mrna, mirna, 2L\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.915 ± 0.073\n",
    "F1 Weighted: 0.956 ± 0.036\n",
    "# mrna, mirna, 3L\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.897 ± 0.089\n",
    "F1 Weighted: 0.944 ± 0.051"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
