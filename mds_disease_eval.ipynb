{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "  \n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  1\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  2\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  3\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 48    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 13    │\n",
      "└───────┴───────┘\n",
      "fold:  4\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "for fold_idx in range(5):\n",
    "    train_df, test_df = mrna_loader.get_fold(fold_idx)\n",
    "\n",
    "    print(\"fold: \", fold_idx)\n",
    "    print(train_df[\"class\"].value_counts(), test_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.feature_dim, odm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:48,070] A new study created in memory with name: no-name-98f39504-4a21-41b5-a710-2c0993ddd36a\n",
      "[I 2024-11-13 21:53:48,292] Trial 0 finished with value: 0.6862235118921005 and parameters: {'n_neighbors': 18}. Best is trial 0 with value: 0.6862235118921005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.686\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.824 ± 0.109\n",
      "F1 Weighted: 0.906 ± 0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:48,500] Trial 1 finished with value: 0.5472774151647614 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-13 21:53:48,700] Trial 2 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-13 21:53:48,898] Trial 3 finished with value: 0.40046307690049987 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-13 21:53:49,104] Trial 4 finished with value: 0.5171422862851435 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-13 21:53:49,303] Trial 5 finished with value: 0.5472774151647614 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.6862235118921005.\n",
      "[I 2024-11-13 21:53:49,514] Trial 6 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 6 with value: 0.7039337563024542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:49,720] Trial 7 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 13}. Best is trial 6 with value: 0.7039337563024542.\n",
      "[I 2024-11-13 21:53:49,936] Trial 8 finished with value: 0.7039337563024542 and parameters: {'n_neighbors': 11}. Best is trial 6 with value: 0.7039337563024542.\n",
      "[I 2024-11-13 21:53:50,134] Trial 9 finished with value: 0.5171422862851435 and parameters: {'n_neighbors': 1}. Best is trial 6 with value: 0.7039337563024542.\n",
      "[I 2024-11-13 21:53:50,334] Trial 10 finished with value: 0.612600042864821 and parameters: {'n_neighbors': 7}. Best is trial 6 with value: 0.7039337563024542.\n",
      "[I 2024-11-13 21:53:50,536] Trial 11 finished with value: 0.7389770960193865 and parameters: {'n_neighbors': 9}. Best is trial 11 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.739\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:50,738] Trial 12 finished with value: 0.6472821557663117 and parameters: {'n_neighbors': 6}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:50,953] Trial 13 finished with value: 0.6014691748907844 and parameters: {'n_neighbors': 8}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:51,154] Trial 14 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 10}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:51,354] Trial 15 finished with value: 0.6623938241144065 and parameters: {'n_neighbors': 5}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:51,557] Trial 16 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 15}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:51,759] Trial 17 finished with value: 0.6631836987350632 and parameters: {'n_neighbors': 10}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:51,965] Trial 18 finished with value: 0.4600966425861633 and parameters: {'n_neighbors': 4}. Best is trial 11 with value: 0.7389770960193865.\n",
      "[I 2024-11-13 21:53:52,168] Trial 19 finished with value: 0.6192439604955923 and parameters: {'n_neighbors': 13}. Best is trial 11 with value: 0.7389770960193865.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:55,110] A new study created in memory with name: no-name-6ac59e52-4f6a-4e4a-9e43-0d6330f068ef\n",
      "[I 2024-11-13 21:53:55,448] Trial 0 finished with value: 0.7155098095140549 and parameters: {'C': 0.01772957818805434, 'class_weight': 'balanced', 'rfe_step': 0.09919097900989131, 'rfe_n_features': 185}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:55,830] Trial 1 finished with value: 0.7039337563024542 and parameters: {'C': 0.03134410486467334, 'class_weight': None, 'rfe_step': 0.05380628664121365, 'rfe_n_features': 186}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:56,153] Trial 2 finished with value: 0.6631836987350632 and parameters: {'C': 0.09836892513300621, 'class_weight': 'balanced', 'rfe_step': 0.16061841838350865, 'rfe_n_features': 186}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:56,566] Trial 3 finished with value: 0.7039337563024542 and parameters: {'C': 0.06340808334748284, 'class_weight': None, 'rfe_step': 0.0658841898678164, 'rfe_n_features': 152}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:56,976] Trial 4 finished with value: 0.6631836987350632 and parameters: {'C': 0.14411431771340535, 'class_weight': None, 'rfe_step': 0.07037657350406427, 'rfe_n_features': 186}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:57,289] Trial 5 finished with value: 0.7039337563024542 and parameters: {'C': 0.019501439143872357, 'class_weight': None, 'rfe_step': 0.07161443840587002, 'rfe_n_features': 171}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:57,587] Trial 6 finished with value: 0.7039337563024542 and parameters: {'C': 0.11677663781158458, 'class_weight': None, 'rfe_step': 0.14054068293791963, 'rfe_n_features': 197}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:57,993] Trial 7 finished with value: 0.7147297129153442 and parameters: {'C': 2.5412274050245283, 'class_weight': 'balanced', 'rfe_step': 0.11468105193922237, 'rfe_n_features': 118}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:58,301] Trial 8 finished with value: 0.6631836987350632 and parameters: {'C': 0.08954760429316694, 'class_weight': None, 'rfe_step': 0.12320956139611365, 'rfe_n_features': 193}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:58,623] Trial 9 finished with value: 0.7155098095140549 and parameters: {'C': 0.03724880380160716, 'class_weight': 'balanced', 'rfe_step': 0.14090650993475806, 'rfe_n_features': 104}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:58,938] Trial 10 finished with value: 0.7147297129153442 and parameters: {'C': 1.0848087584138868, 'class_weight': 'balanced', 'rfe_step': 0.1984305104040972, 'rfe_n_features': 155}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-13 21:53:59,273] Trial 11 finished with value: 0.720542876247867 and parameters: {'C': 0.0119354702489785, 'class_weight': 'balanced', 'rfe_step': 0.09905506575703264, 'rfe_n_features': 108}. Best is trial 11 with value: 0.720542876247867.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.721\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.052\n",
      "F1 Macro: 0.857 ± 0.099\n",
      "F1 Weighted: 0.916 ± 0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:53:59,613] Trial 12 finished with value: 0.6804361055090542 and parameters: {'C': 0.010299627290047192, 'class_weight': 'balanced', 'rfe_step': 0.09720676384643837, 'rfe_n_features': 131}. Best is trial 11 with value: 0.720542876247867.\n",
      "[I 2024-11-13 21:54:00,191] Trial 13 finished with value: 0.7500998536858599 and parameters: {'C': 0.4624453241199983, 'class_weight': 'balanced', 'rfe_step': 0.09488325164479136, 'rfe_n_features': 134}. Best is trial 13 with value: 0.7500998536858599.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.750\n",
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.002\n",
      "F1 Macro: 0.867 ± 0.026\n",
      "F1 Weighted: 0.928 ± 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:54:00,628] Trial 14 finished with value: 0.7500998536858599 and parameters: {'C': 0.5354323664708412, 'class_weight': 'balanced', 'rfe_step': 0.093207078605448, 'rfe_n_features': 133}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:01,080] Trial 15 finished with value: 0.7039337563024542 and parameters: {'C': 0.5096211154336758, 'class_weight': 'balanced', 'rfe_step': 0.08843596965022621, 'rfe_n_features': 135}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:01,568] Trial 16 finished with value: 0.7039337563024542 and parameters: {'C': 9.025990574812704, 'class_weight': 'balanced', 'rfe_step': 0.08522161323802442, 'rfe_n_features': 137}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:01,935] Trial 17 finished with value: 0.6631836987350632 and parameters: {'C': 0.30804291221960983, 'class_weight': 'balanced', 'rfe_step': 0.1389023581081106, 'rfe_n_features': 121}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:02,334] Trial 18 finished with value: 0.7039337563024542 and parameters: {'C': 1.0442712548807571, 'class_weight': 'balanced', 'rfe_step': 0.11524699572753722, 'rfe_n_features': 163}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:02,653] Trial 19 finished with value: 0.6631836987350632 and parameters: {'C': 0.30228287478622456, 'class_weight': 'balanced', 'rfe_step': 0.19490951714911375, 'rfe_n_features': 143}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:02,991] Trial 20 finished with value: 0.6631836987350632 and parameters: {'C': 3.373154256585947, 'class_weight': 'balanced', 'rfe_step': 0.1752296707328394, 'rfe_n_features': 125}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:03,415] Trial 21 finished with value: 0.7147297129153442 and parameters: {'C': 0.5766848902279452, 'class_weight': 'balanced', 'rfe_step': 0.10410586536760959, 'rfe_n_features': 110}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:03,900] Trial 22 finished with value: 0.7147297129153442 and parameters: {'C': 1.4828338477015062, 'class_weight': 'balanced', 'rfe_step': 0.08365940016644911, 'rfe_n_features': 111}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:04,520] Trial 23 finished with value: 0.7147297129153442 and parameters: {'C': 0.4283258516635924, 'class_weight': 'balanced', 'rfe_step': 0.050343361221892244, 'rfe_n_features': 143}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:04,926] Trial 24 finished with value: 0.6631836987350632 and parameters: {'C': 0.18551542736931442, 'class_weight': 'balanced', 'rfe_step': 0.10834976804815993, 'rfe_n_features': 100}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:05,294] Trial 25 finished with value: 0.7039337563024542 and parameters: {'C': 0.20337122475429642, 'class_weight': 'balanced', 'rfe_step': 0.12896713590589626, 'rfe_n_features': 127}. Best is trial 13 with value: 0.7500998536858599.\n",
      "[I 2024-11-13 21:54:05,756] Trial 26 finished with value: 0.7940961203523854 and parameters: {'C': 2.455961473234814, 'class_weight': 'balanced', 'rfe_step': 0.08904838929523133, 'rfe_n_features': 114}. Best is trial 26 with value: 0.7940961203523854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.794\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.027\n",
      "F1 Macro: 0.891 ± 0.060\n",
      "F1 Weighted: 0.941 ± 0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:54:06,264] Trial 27 finished with value: 0.7039337563024542 and parameters: {'C': 8.001215382243764, 'class_weight': 'balanced', 'rfe_step': 0.07982999421017259, 'rfe_n_features': 119}. Best is trial 26 with value: 0.7940961203523854.\n",
      "[I 2024-11-13 21:54:06,837] Trial 28 finished with value: 0.7573892492973633 and parameters: {'C': 4.28120279756211, 'class_weight': 'balanced', 'rfe_step': 0.06281939731108788, 'rfe_n_features': 143}. Best is trial 26 with value: 0.7940961203523854.\n",
      "[I 2024-11-13 21:54:07,346] Trial 29 finished with value: 0.7147297129153442 and parameters: {'C': 4.730736025598158, 'class_weight': 'balanced', 'rfe_step': 0.07611507962460702, 'rfe_n_features': 143}. Best is trial 26 with value: 0.7940961203523854.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.027\n",
      "F1 Macro: 0.891 ± 0.060\n",
      "F1 Weighted: 0.941 ± 0.029\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.938 ± 0.058\n",
    "F1 Macro: 0.684 ± 0.258\n",
    "F1 Weighted: 0.924 ± 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:51:22,141] A new study created in memory with name: no-name-a6c84dbb-96fb-4e06-89e3-5cf9800d3432\n",
      "[I 2024-11-13 21:51:22,362] Trial 0 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.3584886463563621, 'alpha': 2.1203835053596267e-08}. Best is trial 0 with value: 0.7039337563024542.\n",
      "[I 2024-11-13 21:51:22,553] Trial 1 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.004089650990495877, 'alpha': 1.7858414787418788e-08}. Best is trial 1 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:51:22,749] Trial 2 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.017511112403939873, 'alpha': 1.028261317174957e-06}. Best is trial 1 with value: 0.7581635155117249.\n",
      "[I 2024-11-13 21:51:22,942] Trial 3 finished with value: 0.6022120594382817 and parameters: {'lambda': 2.486683965835714e-06, 'alpha': 2.540926338636746e-05}. Best is trial 1 with value: 0.7581635155117249.\n",
      "[I 2024-11-13 21:51:23,267] Trial 4 finished with value: 0.27794757275052406 and parameters: {'lambda': 0.010966976886641595, 'alpha': 0.9470306339118821}. Best is trial 1 with value: 0.7581635155117249.\n",
      "[I 2024-11-13 21:51:23,467] Trial 5 finished with value: 0.775476069168414 and parameters: {'lambda': 4.0819797745300316e-05, 'alpha': 5.528383457492382e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:23,662] Trial 6 finished with value: 0.6305302169311957 and parameters: {'lambda': 7.852368576823749e-08, 'alpha': 0.00010179406181330772}. Best is trial 5 with value: 0.775476069168414.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.775\n",
      "Best model performance:\n",
      "Accuracy: 0.932 ± 0.042\n",
      "F1 Macro: 0.891 ± 0.067\n",
      "F1 Weighted: 0.933 ± 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:51:23,881] Trial 7 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.03037996203506876, 'alpha': 7.35072665195095e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:24,075] Trial 8 finished with value: 0.7415719661868713 and parameters: {'lambda': 6.651696267369343e-06, 'alpha': 6.651813445797384e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:24,268] Trial 9 finished with value: 0.7039337563024542 and parameters: {'lambda': 3.601323635499819e-07, 'alpha': 0.00040128100391318225}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:24,462] Trial 10 finished with value: 0.6969195446998454 and parameters: {'lambda': 0.00014946013119694412, 'alpha': 0.013581880025666163}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:24,662] Trial 11 finished with value: 0.7334652041272984 and parameters: {'lambda': 0.00040345155052458505, 'alpha': 1.0856781764411731e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:24,865] Trial 12 finished with value: 0.7693788056664252 and parameters: {'lambda': 0.0007006961271483547, 'alpha': 1.3441066266349815e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:25,075] Trial 13 finished with value: 0.7415719661868713 and parameters: {'lambda': 1.2689520948451923e-05, 'alpha': 1.9475571470957057e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:25,273] Trial 14 finished with value: 0.7155098095140549 and parameters: {'lambda': 0.0009430809362177085, 'alpha': 8.894516205789824e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:25,477] Trial 15 finished with value: 0.6631836987350632 and parameters: {'lambda': 4.301473358668053e-05, 'alpha': 0.0011174829593144477}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:25,677] Trial 16 finished with value: 0.7415719661868713 and parameters: {'lambda': 1.283434866123417e-08, 'alpha': 2.5536410507908137e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:25,874] Trial 17 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.36446240084343456, 'alpha': 6.19998662409024e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:26,091] Trial 18 finished with value: 0.7415719661868713 and parameters: {'lambda': 1.0461317303075942e-06, 'alpha': 1.0316587885540192e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:26,287] Trial 19 finished with value: 0.6631836987350632 and parameters: {'lambda': 4.809746314752152e-05, 'alpha': 0.00505732959706068}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:26,486] Trial 20 finished with value: 0.7155098095140549 and parameters: {'lambda': 0.0014650761904101345, 'alpha': 2.1277566572224358e-05}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:26,683] Trial 21 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0052990161016040396, 'alpha': 1.8851818983514642e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:26,889] Trial 22 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0786379870096572, 'alpha': 1.0721434390429664e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:27,090] Trial 23 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0031121203382993602, 'alpha': 2.0486460470881156e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:27,290] Trial 24 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00016539596134867554, 'alpha': 4.0969544534774194e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:27,488] Trial 25 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00019576770488706556, 'alpha': 3.124930086902745e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:27,683] Trial 26 finished with value: 0.7334652041272984 and parameters: {'lambda': 0.000181715931450631, 'alpha': 8.312016610116337e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:27,878] Trial 27 finished with value: 0.7415719661868713 and parameters: {'lambda': 1.2286452767881466e-05, 'alpha': 3.3895798957742154e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:28,081] Trial 28 finished with value: 0.7415719661868713 and parameters: {'lambda': 3.467274459761183e-05, 'alpha': 5.2697294432370866e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:28,294] Trial 29 finished with value: 0.7415719661868713 and parameters: {'lambda': 4.235022552244841e-06, 'alpha': 3.874408705148281e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:28,490] Trial 30 finished with value: 0.7334652041272984 and parameters: {'lambda': 0.00025602836194868077, 'alpha': 5.159278985379484e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:28,691] Trial 31 finished with value: 0.7693788056664252 and parameters: {'lambda': 0.0009078350103987309, 'alpha': 4.945381493172544e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:28,889] Trial 32 finished with value: 0.775476069168414 and parameters: {'lambda': 7.12101010551161e-05, 'alpha': 1.963350224246206e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:29,085] Trial 33 finished with value: 0.775476069168414 and parameters: {'lambda': 6.566875439991395e-05, 'alpha': 1.9350228231334634e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:29,281] Trial 34 finished with value: 0.6022120594382817 and parameters: {'lambda': 1.744027575142221e-05, 'alpha': 1.8627132667811937e-05}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:29,477] Trial 35 finished with value: 0.669993140387569 and parameters: {'lambda': 1.402440724205125e-06, 'alpha': 0.00010423436995470274}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:29,674] Trial 36 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00010820338817480668, 'alpha': 2.4513054605090843e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:29,869] Trial 37 finished with value: 0.7415719661868713 and parameters: {'lambda': 2.201216625151248e-05, 'alpha': 8.203586041045254e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:30,070] Trial 38 finished with value: 0.6890419268708564 and parameters: {'lambda': 3.4612110908722817e-07, 'alpha': 3.4177398457629403e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:30,268] Trial 39 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.0003530324092843966, 'alpha': 4.935765196386763e-05}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:30,466] Trial 40 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.052440402097596886, 'alpha': 5.778033970614947e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:30,677] Trial 41 finished with value: 0.775476069168414 and parameters: {'lambda': 8.98518847257612e-05, 'alpha': 1.6538397064470647e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:30,869] Trial 42 finished with value: 0.27794757275052406 and parameters: {'lambda': 6.999235900666252e-05, 'alpha': 0.1959541195449841}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:31,073] Trial 43 finished with value: 0.7415719661868713 and parameters: {'lambda': 5.888254480230024e-06, 'alpha': 3.2311494815709824e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:31,271] Trial 44 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0038501447005081877, 'alpha': 1.1429143106876639e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:31,471] Trial 45 finished with value: 0.7693788056664252 and parameters: {'lambda': 0.0005244163635639737, 'alpha': 1.100301474844686e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:31,669] Trial 46 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00011391509360537495, 'alpha': 2.6538659999101735e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:31,870] Trial 47 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.0023136230663607445, 'alpha': 2.362942376037217e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:32,088] Trial 48 finished with value: 0.7581635155117249 and parameters: {'lambda': 0.011981013076831203, 'alpha': 2.2960684475708116e-06}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:32,287] Trial 49 finished with value: 0.6514583147512699 and parameters: {'lambda': 2.6924606742719697e-05, 'alpha': 1.0222528372266033e-05}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:32,486] Trial 50 finished with value: 0.669993140387569 and parameters: {'lambda': 8.834859449878125e-06, 'alpha': 0.00021644920882103383}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:32,687] Trial 51 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00018620382964153285, 'alpha': 2.102451512157162e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:32,889] Trial 52 finished with value: 0.775476069168414 and parameters: {'lambda': 7.703704418087238e-05, 'alpha': 5.46854816519708e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:33,107] Trial 53 finished with value: 0.7334652041272984 and parameters: {'lambda': 0.00044412083550013294, 'alpha': 1.1018287853187185e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:33,307] Trial 54 finished with value: 0.7415719661868713 and parameters: {'lambda': 2.556549898574722e-06, 'alpha': 4.156283100998844e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:33,507] Trial 55 finished with value: 0.775476069168414 and parameters: {'lambda': 4.463239077864399e-05, 'alpha': 9.381793467792716e-08}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:33,705] Trial 56 finished with value: 0.7039337563024542 and parameters: {'lambda': 0.8496644835772814, 'alpha': 1.5226539613783855e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:33,907] Trial 57 finished with value: 0.775476069168414 and parameters: {'lambda': 0.00022518037511355812, 'alpha': 9.025378078640485e-07}. Best is trial 5 with value: 0.775476069168414.\n",
      "[I 2024-11-13 21:51:34,108] Trial 58 finished with value: 0.8139820349629631 and parameters: {'lambda': 0.0015811565851786395, 'alpha': 3.865368602997559e-08}. Best is trial 58 with value: 0.8139820349629631.\n",
      "[I 2024-11-13 21:51:34,306] Trial 59 finished with value: 0.7693788056664252 and parameters: {'lambda': 0.0011142935020468775, 'alpha': 5.4823772369451865e-08}. Best is trial 58 with value: 0.8139820349629631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.814\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.945 ± 0.051\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.945 ± 0.051\n"
     ]
    }
   ],
   "source": [
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:52:55,151] A new study created in memory with name: no-name-6b2c9513-2f1e-4374-912e-c9eb65b9fb60\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:52:58,193] Trial 0 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0004712240638346166, 'dropout': 0.10870117291852335}. Best is trial 0 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:01,215] Trial 1 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0002717018667066596, 'dropout': 0.26157280166479474}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.704\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:04,235] Trial 2 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0037378291126883238, 'dropout': 0.42361815559250626}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:07,350] Trial 3 finished with value: 0.7039337563024542 and parameters: {'lr': 0.00040171815523197573, 'dropout': 0.3590144296875618}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:10,390] Trial 4 finished with value: 0.6631836987350632 and parameters: {'lr': 0.00038322004920353234, 'dropout': 0.4082234313372539}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:13,421] Trial 5 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0006106944927353708, 'dropout': 0.3025481509048129}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:16,438] Trial 6 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0003195269723399977, 'dropout': 0.19724537194600558}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:19,462] Trial 7 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0008639824209714169, 'dropout': 0.4929466678714677}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:22,466] Trial 8 finished with value: 0.7039337563024542 and parameters: {'lr': 0.0006207396370620406, 'dropout': 0.34745174610488094}. Best is trial 1 with value: 0.7039337563024542.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "[I 2024-11-13 21:53:25,507] Trial 9 finished with value: 0.6192439604955923 and parameters: {'lr': 0.0001478994271602511, 'dropout': 0.4793597779330615}. Best is trial 1 with value: 0.7039337563024542.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.5],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n",
      "Best hyperparameters:\n",
      "{'lr': 0.0002717018667066596, 'dropout': 0.26157280166479474}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"knn\")\n",
    "svm_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"svm\")\n",
    "xgb_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.042\n",
      "F1 Macro: 0.857 ± 0.094\n",
      "F1 Weighted: 0.924 ± 0.051\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.027\n",
      "F1 Macro: 0.891 ± 0.060\n",
      "F1 Weighted: 0.941 ± 0.029\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.910 ± 0.080\n",
      "F1 Weighted: 0.945 ± 0.051\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.840 ± 0.097\n",
      "F1 Weighted: 0.911 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "knn_eval.print_best_results()\n",
    "svm_eval.print_best_results()\n",
    "xgb_eval.print_best_results()\n",
    "mlp_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:56:42,386] A new study created in memory with name: no-name-7f3917be-c09c-4430-85de-1c7cc13d127f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.0732, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9481\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.0590, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280, Val Geometric Mean: 0.9135\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.0264, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8667, Val Geometric Mean: 0.8409\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.0644, Train Acc: 0.9831, Train F1 Macro: 0.9730, Train F1 Weighted: 0.9833\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 101:\n",
      "Train Loss: 0.0127, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.9286, Val F1 Macro: 0.8783, Val F1 Weighted: 0.9342, Val Geometric Mean: 0.9133\n",
      "Test Acc: 0.9286, Test F1 Macro: 0.8783, Test F1 Weighted: 0.9342\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:56:57,219] Trial 0 finished with value: 0.8601248426666666 and parameters: {}. Best is trial 0 with value: 0.8601248426666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.860\n",
      "Best model performance:\n",
      "Accuracy: 0.960 ± 0.053\n",
      "F1 Macro: 0.934 ± 0.085\n",
      "F1 Weighted: 0.959 ± 0.054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': np.float64(0.96),\n",
       " 'f1_macro': np.float64(0.9343333333333333),\n",
       " 'f1_weighted': np.float64(0.9589333333333332),\n",
       " 'acc_std': np.float64(0.053333333333333316),\n",
       " 'f1_macro_std': np.float64(0.08513779680285628),\n",
       " 'f1_weighted_std': np.float64(0.05390625607890463)}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders={\n",
    "            \"mrna\": mrna_loader,\n",
    "            \"mirna\": mirna_loader,\n",
    "            \"circrna\": circrna_loader,\n",
    "            \"te\": te_loader,\n",
    "        },\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"attention\",\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 200,\n",
    "        \"log_interval\": 101,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.918 ± 0.070\n",
    "F1 Weighted: 0.947 ± 0.047\n",
    "\n",
    "Best model performance:\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.947 ± 0.069\n",
    "F1 Weighted: 0.963 ± 0.049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 21:59:12,056] A new study created in memory with name: no-name-ce572e65-001d-4a8f-beff-d92b22013261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4343, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1226, Train Acc: 0.9322, Train F1 Macro: 0.8689, Train F1 Weighted: 0.9291\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0367, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0429, Train Acc: 0.9661, Train F1 Macro: 0.9344, Train F1 Weighted: 0.9646\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4053, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0729, Train Acc: 0.9661, Train F1 Macro: 0.9441, Train F1 Weighted: 0.9673\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.6400, Val F1 Weighted: 0.7840, Val Geometric Mean: 0.7377\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.6400, Test F1 Weighted: 0.7840\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0117, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4345, Train Acc: 0.8305, Train F1 Macro: 0.4537, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111, Val Geometric Mean: 0.6323\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0642, Train Acc: 0.9831, Train F1 Macro: 0.9686, Train F1 Weighted: 0.9827\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0436, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9834\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0191, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385, Val Geometric Mean: 0.8026\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3196, Train Acc: 0.8475, Train F1 Macro: 0.6907, Train F1 Weighted: 0.8288\n",
      "Val Acc: 0.7333, Val F1 Macro: 0.5833, Val F1 Weighted: 0.7667, Val Geometric Mean: 0.6896\n",
      "Test Acc: 0.7333, Test F1 Macro: 0.5833, Test F1 Weighted: 0.7667\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1509, Train Acc: 0.9492, Train F1 Macro: 0.9059, Train F1 Weighted: 0.9459\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0854, Train Acc: 0.9831, Train F1 Macro: 0.9710, Train F1 Weighted: 0.9827\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1564, Train Acc: 0.9492, Train F1 Macro: 0.9131, Train F1 Weighted: 0.9482\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000, Val Geometric Mean: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.2600, Train Acc: 0.8833, Train F1 Macro: 0.7980, Train F1 Weighted: 0.8811\n",
      "Val Acc: 0.9286, Val F1 Macro: 0.8783, Val F1 Weighted: 0.9342, Val Geometric Mean: 0.9133\n",
      "Test Acc: 0.9286, Test F1 Macro: 0.8783, Test F1 Weighted: 0.9342\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.0801, Train Acc: 0.9667, Train F1 Macro: 0.9479, Train F1 Weighted: 0.9677\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0346, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 22:00:06,125] Trial 0 finished with value: 0.7948768203234714 and parameters: {}. Best is trial 0 with value: 0.7948768203234714.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0096, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.7143, Val F1 Macro: 0.4167, Val F1 Weighted: 0.7143, Val Geometric Mean: 0.5968\n",
      "Test Acc: 0.7143, Test F1 Macro: 0.4167, Test F1 Weighted: 0.7143\n",
      "##################################################\n",
      "New best score: 0.795\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.894 ± 0.106\n",
      "F1 Weighted: 0.939 ± 0.060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': np.float64(0.9466666666666667),\n",
       " 'f1_macro': np.float64(0.8943076923076922),\n",
       " 'f1_weighted': np.float64(0.9388923076923078),\n",
       " 'acc_std': np.float64(0.04988876515698587),\n",
       " 'f1_macro_std': np.float64(0.10597711040122806),\n",
       " 'f1_weighted_std': np.float64(0.05965219905477923)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders={\n",
    "            \"mrna\": mrna_loader,\n",
    "            \"mirna\": mirna_loader,\n",
    "            \"circrna\": circrna_loader,\n",
    "            \"te\": te_loader,\n",
    "        },\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.8,\n",
    "                \"mirna\": 1.8,\n",
    "                \"circrna\": 1.8,\n",
    "                \"te\": 1.8,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 200,\n",
    "        \"log_interval\": 50,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est model performance:\n",
    "Accuracy: 0.960 ± 0.033\n",
    "F1 Macro: 0.920 ± 0.072\n",
    "F1 Weighted: 0.958 ± 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200]) torch.Size([200])\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(5) tensor(1) tensor(12.9189)\n",
      "torch.Size([200]) torch.Size([200])\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(12.7027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  feature_names=[2],\n",
       "  omics=[2],\n",
       "  num_relations=6,\n",
       "  y=[74],\n",
       "  train_mask=[74],\n",
       "  test_mask=[74],\n",
       "  val_mask=[74],\n",
       "  mrna={ x=[74, 200] },\n",
       "  mrna_feature={ x=[200, 200] },\n",
       "  mirna={ x=[74, 200] },\n",
       "  mirna_feature={ x=[200, 200] },\n",
       "  (mrna, diff_exp, mrna_feature)={ edge_index=[2, 956] },\n",
       "  (mirna, diff_exp, mirna_feature)={ edge_index=[2, 940] },\n",
       "  (mrna_feature, rev_diff_exp, mrna)={ edge_index=[2, 956] },\n",
       "  (mirna_feature, rev_diff_exp, mirna)={ edge_index=[2, 940] },\n",
       "  (mrna_feature, interacts, mrna_feature)={ edge_index=[2, 293] },\n",
       "  (mirna_feature, regulates, mrna_feature)={ edge_index=[2, 462] }\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "\n",
    "bpdm = BipartiteGraphDataManager(\n",
    "    omic_data_loaders={\n",
    "        \"mrna\": mrna_loader,\n",
    "        \"mirna\": mirna_loader,\n",
    "    },\n",
    "    n_splits=5,\n",
    "    params={\n",
    "        \"diff_exp_thresholds\" : {\n",
    "            \"mrna\": 1.8,\n",
    "            \"mirna\": 1.8,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "data, _, _, _ = bpdm.get_split(0)\n",
    "# params={\n",
    "#     \"graph_style\": \"threshold\",\n",
    "#     \"self_connections\": True,\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1996,  0.0200],\n",
       "        [ 0.1476, -0.0125],\n",
       "        [-0.0169, -0.3833],\n",
       "        [-0.2606, -0.1123],\n",
       "        [ 0.0988,  0.1072],\n",
       "        [ 0.2152,  0.1267],\n",
       "        [ 0.2539,  0.2399],\n",
       "        [-0.1327,  0.1547],\n",
       "        [ 0.1204,  0.0057],\n",
       "        [ 0.0787,  0.0047],\n",
       "        [-0.1699, -0.0654],\n",
       "        [-0.1015, -0.0169],\n",
       "        [-0.1148, -0.1564],\n",
       "        [-0.2891, -0.1228],\n",
       "        [-0.1221, -0.1194],\n",
       "        [-0.2548, -0.1509],\n",
       "        [-0.1392, -0.1450],\n",
       "        [-0.0354, -0.2231],\n",
       "        [-0.2383,  0.0189],\n",
       "        [-0.0243, -0.2815],\n",
       "        [-0.1663,  0.0082],\n",
       "        [ 0.0280, -0.0584],\n",
       "        [ 0.1171,  0.0122],\n",
       "        [ 0.0266, -0.0320],\n",
       "        [-0.1866,  0.0935],\n",
       "        [-0.1981, -0.1071],\n",
       "        [-0.1796, -0.3447],\n",
       "        [-0.0147, -0.2409],\n",
       "        [ 0.1196, -0.2709],\n",
       "        [ 0.0811, -0.2482],\n",
       "        [ 0.1268, -0.0737],\n",
       "        [-0.0107, -0.0359],\n",
       "        [-0.1744, -0.2606],\n",
       "        [-0.2738, -0.0500],\n",
       "        [-0.0365, -0.1182],\n",
       "        [ 0.0636, -0.2654],\n",
       "        [-0.0823, -0.1176],\n",
       "        [-0.1565, -0.2752],\n",
       "        [ 0.0837,  0.2634],\n",
       "        [-0.1940, -0.2055],\n",
       "        [-0.0094, -0.1054],\n",
       "        [-0.0698, -0.2017],\n",
       "        [-0.3152,  0.0972],\n",
       "        [-0.1653,  0.0656],\n",
       "        [ 0.1305,  0.0121],\n",
       "        [-0.0037, -0.4056],\n",
       "        [-0.2522,  0.1519],\n",
       "        [-0.0549, -0.1125],\n",
       "        [ 0.2435, -0.0827],\n",
       "        [-0.1944, -0.0879],\n",
       "        [ 0.0065,  0.0019],\n",
       "        [-0.0183, -0.3525],\n",
       "        [ 0.1464, -0.0199],\n",
       "        [-0.0770, -0.0364],\n",
       "        [-0.0929, -0.2363],\n",
       "        [-0.0214,  0.0484],\n",
       "        [-0.0698, -0.2347],\n",
       "        [ 0.0091, -0.1021],\n",
       "        [ 0.0795,  0.0861],\n",
       "        [-0.2792, -0.2338],\n",
       "        [-0.0985, -0.2449],\n",
       "        [-0.1814, -0.2986],\n",
       "        [ 0.0176,  0.1467],\n",
       "        [-0.1159, -0.0345],\n",
       "        [ 0.1978, -0.0071],\n",
       "        [ 0.2575,  0.2147],\n",
       "        [ 0.0825,  0.0425],\n",
       "        [-0.1977, -0.3105],\n",
       "        [-0.1683, -0.2478],\n",
       "        [-0.0759,  0.0594],\n",
       "        [-0.1281, -0.2853],\n",
       "        [-0.2448, -0.0294],\n",
       "        [-0.0968,  0.0006],\n",
       "        [ 0.0042, -0.1574]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from src.models.birgat import BiRGAT\n",
    "\n",
    "params = {\n",
    "    \"hidden_channels\": [200, 32, 32, 32, 32],\n",
    "    \"heads\": 2,\n",
    "    \"dropout\": 0.2,\n",
    "    \"attention_dropout\": 0.2,\n",
    "    \"use_proj_module\": False,\n",
    "    \"integrator_type\": \"attention\",\n",
    "    \"proj_dim\" : 64,\n",
    "    \"three_layers\": False\n",
    "}\n",
    "\n",
    "model = BiRGAT(\n",
    "    omic_channels=data.omics,\n",
    "    feature_names=data.feature_names,\n",
    "    relations=list(data.edge_index_dict.keys()),\n",
    "    input_dims={\n",
    "        omic: data.x_dict[omic].shape[1] for omic in data.x_dict.keys()\n",
    "    },\n",
    "    proj_dim=params[\"proj_dim\"],\n",
    "    hidden_channels=params[\"hidden_channels\"],\n",
    "    num_classes=len(torch.unique(data.y)),\n",
    "    heads=params[\"heads\"],\n",
    "    dropout=params[\"dropout\"],\n",
    "    attention_dropout=params[\"attention_dropout\"],\n",
    "    use_proj_module=params[\"use_proj_module\"],\n",
    "    integrator_type=params[\"integrator_type\"],\n",
    "    three_layers=params[\"three_layers\"],\n",
    ")\n",
    "model.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('mrna_feature',\n",
       "  'interacts',\n",
       "  'mrna_feature'): tensor([[  0,   1,   2,   2,   3,   4,   4,   4,   5,   6,   7,   8,   9,  10,\n",
       "           10,  10,  10,  10,  10,  11,  12,  12,  12,  12,  12,  13,  13,  14,\n",
       "           15,  15,  16,  17,  17,  17,  17,  18,  19,  19,  19,  19,  20,  20,\n",
       "           21,  21,  21,  21,  21,  21,  21,  22,  23,  23,  23,  24,  25,  26,\n",
       "           27,  27,  28,  29,  30,  30,  31,  31,  32,  33,  33,  34,  35,  36,\n",
       "           36,  36,  37,  38,  38,  38,  38,  38,  38,  38,  38,  38,  39,  40,\n",
       "           40,  40,  40,  40,  40,  40,  40,  41,  41,  42,  43,  43,  43,  43,\n",
       "           43,  44,  44,  45,  45,  46,  47,  48,  48,  48,  48,  49,  49,  49,\n",
       "           49,  49,  50,  50,  50,  51,  51,  51,  52,  52,  52,  52,  52,  52,\n",
       "           53,  53,  53,  53,  53,  53,  53,  53,  54,  55,  56,  57,  58,  59,\n",
       "           59,  60,  61,  62,  62,  62,  62,  62,  63,  64,  65,  65,  65,  65,\n",
       "           66,  67,  67,  67,  68,  69,  70,  71,  72,  73,  74,  74,  75,  75,\n",
       "           75,  75,  75,  76,  76,  76,  76,  77,  77,  77,  77,  77,  77,  77,\n",
       "           78,  79,  80,  81,  82,  83,  83,  83,  83,  83,  83,  83,  83,  83,\n",
       "           83,  83,  83,  83,  83,  84,  85,  86,  86,  86,  87,  87,  88,  88,\n",
       "           89,  89,  90,  90,  90,  90,  91,  91,  91,  91,  91,  91,  92,  93,\n",
       "           93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
       "           95,  96,  96,  97,  98,  99, 100, 101, 101, 101, 101, 101, 101, 102,\n",
       "          102, 103, 103, 103, 103, 103, 104, 104, 105, 106, 106, 107, 108, 108,\n",
       "          109, 110, 110, 111, 112, 112, 112, 113, 114, 115, 116, 117, 117, 117,\n",
       "          118, 119, 120, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124,\n",
       "          125, 126, 126, 127, 127, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "          128, 128, 128, 129, 130, 131, 132, 133, 133, 134, 135, 135, 136, 137,\n",
       "          138, 138, 138, 138, 138, 138, 138, 138, 138, 139, 139, 139, 139, 140,\n",
       "          140, 140, 141, 141, 141, 142, 142, 143, 143, 144, 144, 144, 144, 144,\n",
       "          145, 146, 147, 147, 147, 147, 148, 148, 149, 150, 150, 151, 152, 152,\n",
       "          153, 153, 153, 153, 153, 153, 154, 155, 156, 157, 157, 157, 157, 157,\n",
       "          158, 159, 159, 159, 159, 159, 159, 159, 160, 160, 160, 161, 162, 163,\n",
       "          164, 165, 165, 166, 166, 167, 168, 168, 168, 169, 169, 170, 170, 171,\n",
       "          171, 172, 173, 173, 173, 174, 174, 174, 174, 174, 175, 176, 177, 177,\n",
       "          177, 177, 178, 178, 178, 178, 178, 179, 180, 180, 180, 180, 181, 181,\n",
       "          181, 181, 181, 182, 183, 184, 185, 186, 187, 188, 188, 188, 189, 190,\n",
       "          191, 192, 192, 192, 193, 194, 195, 196, 197, 197, 198, 199, 199],\n",
       "         [  0,   1,   2, 167,   3,   4,  65,  67,   5,   6,   7,   8,   9,  10,\n",
       "           21,  93, 144, 153, 159,  11,  12,  50,  75, 169, 177,  13,  52,  14,\n",
       "           15, 165,  16,  17,  19,  48, 139,  18,  17,  19,  48, 139,  20,  62,\n",
       "           10,  21,  53,  93, 144, 153, 159,  22,  23, 163, 198,  24,  25,  26,\n",
       "           27, 168,  28,  29,  30, 126,  31, 147,  32,  33, 110,  34,  35,  36,\n",
       "          136, 194,  37,   0,  33,  35,  38, 147, 169, 179, 187, 198,  39,  40,\n",
       "           43,  44,  50,  52,  70,  83, 110,  41, 140,  42,  40,  43,  83, 136,\n",
       "          169,  40,  44,  45, 173,  46,  47,  17,  19,  48, 139,  49,  83,  94,\n",
       "          128, 167,  50,  52, 146,  51,  76,  93,  40,  50,  52,  83, 117, 147,\n",
       "           21,  53,  93, 123, 153, 159, 181, 192,  54,  55,  56,  57,  58,  59,\n",
       "           83,  60,  61,  18,  20,  62, 134, 147,  63,  64,   7,  22,  65, 192,\n",
       "           66,   4,  67, 180,  68,  69,  70,  71,  72,  73,  74, 171,  14,  50,\n",
       "           75, 140, 199,  65,  76,  86, 102,  77,  83,  91,  94, 103, 128, 174,\n",
       "           78,  79,  80,  81,  82,  40,  43,  49,  52,  59,  77,  83,  91,  94,\n",
       "          101, 112, 128, 138, 181,  84,  85,   7,  65,  86,  87, 148,  88, 103,\n",
       "           89, 152,  18,  90, 147, 170,  77,  83,  91,  94, 128, 138,  92,  10,\n",
       "           21,  53,  93, 144, 159,  49,  77,  83,  91,  94, 128, 138, 174, 180,\n",
       "           95,  96, 102,  97,  98,  99, 100,  83, 101, 103, 128, 138, 157,  96,\n",
       "          102,  77,  88, 101, 103, 157, 104, 137, 105,   2, 106, 107, 108, 140,\n",
       "          109,  33, 110, 111,  83, 112, 138, 113, 114, 115, 116,  52, 117, 147,\n",
       "          118, 119, 120, 121,  19,  50, 110, 122,  21,  53, 123, 181, 124, 142,\n",
       "          125,  30, 126, 127, 142,  18,  49,  77,  83,  91,  94, 101, 122, 128,\n",
       "          138, 149, 157, 129, 130, 131, 132,  19, 133, 134, 135, 169, 136, 137,\n",
       "           43,  83,  91,  94, 101, 112, 128, 138, 157,  17,  19,  48, 139,  75,\n",
       "          108, 140, 141, 174, 192, 127, 142, 140, 143,  10,  21,  93, 144, 159,\n",
       "          145, 146,  43,  52, 117, 147,  87, 148, 149,   9, 150, 151,  89, 152,\n",
       "           10,  21,  53, 153, 159, 192, 154, 155, 156, 101, 103, 128, 138, 157,\n",
       "          158,  10,  21,  53,  93, 144, 153, 159,  50, 136, 160, 161, 162, 163,\n",
       "          164,  15, 165,  86, 166, 167,  27, 166, 168,  12, 169,  90, 170,  74,\n",
       "          171, 172,  45, 173, 179,  74,  77,  94, 141, 174, 175, 176,  50,  75,\n",
       "          140, 177,  64, 104, 172, 178, 198, 179,  67,  94, 104, 180,  53,  83,\n",
       "          110, 123, 181, 182, 183, 184, 185, 186, 187, 116, 133, 188, 189, 190,\n",
       "          191,  53, 153, 192, 193, 194, 195, 196, 120, 197, 198,  75, 199]]),\n",
       " ('mirna_feature',\n",
       "  'regulates',\n",
       "  'mrna_feature'): tensor([[  1,   5,   5,   5,   5,   5,   5,   5,   5,   6,   6,   6,   6,   6,\n",
       "            7,   7,   7,   7,   7,   7,   7,   7,   7,   8,   8,  10,  10,  10,\n",
       "           10,  10,  10,  12,  12,  12,  13,  13,  16,  16,  17,  17,  17,  17,\n",
       "           17,  17,  17,  17,  17,  17,  17,  19,  19,  19,  19,  19,  19,  22,\n",
       "           22,  22,  26,  26,  26,  27,  27,  27,  28,  28,  29,  29,  29,  29,\n",
       "           29,  29,  29,  30,  30,  30,  31,  31,  32,  32,  34,  34,  34,  34,\n",
       "           34,  34,  35,  35,  35,  35,  35,  40,  41,  42,  42,  42,  42,  42,\n",
       "           43,  43,  45,  46,  46,  47,  47,  52,  52,  52,  52,  52,  52,  54,\n",
       "           54,  54,  54,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,  55,\n",
       "           55,  56,  56,  56,  56,  56,  56,  57,  57,  59,  59,  59,  61,  61,\n",
       "           61,  63,  64,  64,  64,  64,  64,  64,  64,  64,  65,  66,  66,  67,\n",
       "           68,  69,  69,  69,  70,  70,  71,  71,  71,  71,  71,  71,  71,  74,\n",
       "           74,  74,  74,  74,  75,  75,  75,  76,  76,  76,  77,  78,  79,  79,\n",
       "           79,  79,  79,  79,  79,  79,  79,  79,  79,  79,  79,  81,  81,  81,\n",
       "           81,  81,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,  82,\n",
       "           82,  83,  85,  87,  87,  87,  89,  89,  89,  90,  93,  93,  93,  93,\n",
       "           93,  93,  94,  94,  95,  95,  96,  98, 101, 101, 101, 101, 101, 101,\n",
       "          101, 103, 104, 105, 105, 105, 105, 105, 105, 106, 108, 108, 108, 108,\n",
       "          108, 108, 108, 108, 108, 111, 111, 111, 112, 113, 116, 116, 117, 117,\n",
       "          117, 118, 118, 119, 120, 121, 121, 121, 121, 122, 124, 124, 124, 124,\n",
       "          124, 126, 126, 127, 127, 127, 127, 127, 127, 128, 128, 128, 128, 128,\n",
       "          128, 128, 128, 129, 131, 131, 131, 131, 131, 131, 131, 131, 131, 131,\n",
       "          131, 131, 132, 132, 133, 133, 135, 135, 135, 137, 138, 138, 141, 141,\n",
       "          141, 141, 141, 141, 142, 143, 143, 143, 146, 147, 147, 147, 147, 147,\n",
       "          147, 150, 151, 151, 151, 151, 152, 152, 153, 153, 153, 153, 155, 155,\n",
       "          155, 155, 155, 155, 156, 160, 160, 162, 163, 163, 163, 164, 165, 167,\n",
       "          167, 167, 167, 167, 167, 167, 168, 168, 168, 168, 168, 168, 168, 168,\n",
       "          168, 169, 169, 169, 169, 169, 169, 170, 171, 172, 172, 172, 172, 172,\n",
       "          173, 173, 173, 173, 173, 173, 176, 177, 177, 177, 177, 177, 178, 180,\n",
       "          180, 180, 180, 180, 180, 180, 180, 180, 180, 181, 183, 183, 183, 185,\n",
       "          185, 187, 187, 188, 190, 191, 193, 193, 193, 193, 193, 193, 193, 193,\n",
       "          193, 193, 193, 193, 193, 193, 194, 194, 194, 194, 195, 195, 195, 195,\n",
       "          196, 196, 197, 197, 197, 197, 197, 197, 198, 198, 198, 198, 198, 198],\n",
       "         [154,   8,  67, 104, 110, 142, 169, 175, 190,  52,  84, 133, 154, 182,\n",
       "            0,  40, 104, 118, 127, 143, 156, 167, 197,  29, 192,  20,  41,  71,\n",
       "          130, 140, 142,   0,  24, 104,  81, 127,   2,  31,  20,  35,  48,  51,\n",
       "           52,  82, 119, 131, 149, 175, 180,   0,  30,  45,  64, 141, 180,  12,\n",
       "           84, 130,  41,  64, 169,   2,  39, 127, 170, 182,  81,  94, 135, 136,\n",
       "          150, 154, 198,   4, 119, 189,  83, 141,  18,  91,  27,  51,  64, 131,\n",
       "          139, 156,   8,  47, 106, 132, 151,  27,  39,  41, 104, 108, 180, 187,\n",
       "           94, 174,  80,  31,  57,  35, 189,   8,  18,  27, 134, 143, 190,   9,\n",
       "          113, 175, 190,   9,  12,  24,  47,  55,  75,  78, 113, 134, 142, 156,\n",
       "          162,  30,  35,  64, 145, 169, 190,  45, 197,  28,  94, 136,  14, 113,\n",
       "          147, 117,  18,  24,  79, 103, 114, 145, 180, 190, 101, 123, 153, 119,\n",
       "           93,  58,  71, 117,  67, 134,  58,  98, 104, 107, 135, 145, 186,  15,\n",
       "          104, 106, 168, 189,  77, 154, 156,  11,  39,  91,  45,  59,   0,  24,\n",
       "           30,  41,  51,  64,  77, 103, 131, 143, 147, 148, 190,  36,  91, 104,\n",
       "          180, 197,  24,  30,  41,  43,  49,  75,  94, 104, 108, 131, 169, 174,\n",
       "          180, 145,  24,  14,  94, 180,  77, 133, 148,  30,  30,  32,  36,  41,\n",
       "           64, 113, 117, 154,  30, 113, 182,  49,  41,  46,  67,  77,  94, 104,\n",
       "          176,  59,  35,  79, 117, 143, 167, 170, 181, 135,   2,  24,  30,  35,\n",
       "           41,  74, 131, 182, 190,   2,  50, 104, 180, 180,  33,  39,  44, 140,\n",
       "          180,  59, 141, 135, 141,  33,  36,  85, 142,  33,  41,  46,  67,  77,\n",
       "          104, 128, 165,  47,  64,  94, 103, 104, 156,  14,  81, 124, 163, 169,\n",
       "          174, 179, 197,  33,  24,  30,  43,  49,  67,  94, 113, 122, 131, 148,\n",
       "          162, 184,  39,  50,   2, 148,  81, 136, 154, 189,  64, 190,  33, 127,\n",
       "          145, 162, 180, 190, 154, 148, 149, 198,   2,  55,  67,  94, 114, 145,\n",
       "          163,  64,  20,  42,  52, 156,  18,  39,  41,  78, 104, 149,  20,  41,\n",
       "           71, 130, 140, 142, 173,  77, 131, 180,  50,  87, 128,  31, 134,   2,\n",
       "           41, 108, 119, 153, 156, 162,   2,  51,  77, 104, 107, 126, 131, 156,\n",
       "          174,  13,  34,  85, 117, 153, 180,   0, 142,  35,  51,  64,  94, 127,\n",
       "            0,   2,  57,  78,  92, 142, 127,  77, 145, 156, 169, 172, 190,   2,\n",
       "           30,  51,  64,  74,  78, 117, 127, 145, 181, 197,  39,  67, 141,  24,\n",
       "          175,  64, 180, 197,  78,  31,   2,   9,  30,  55,  67,  77,  78,  88,\n",
       "          117, 131, 135, 156, 180, 199,  29,  42,  95, 165,  90, 128, 132, 154,\n",
       "           64, 104,  31,  58,  67, 108, 119, 127,  37,  39,  64,  84, 118, 180]])}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,   2,   3,   4,   4,   4,   5,   6,   7,   8,   9,  10,\n",
       "          10,  10,  10,  10,  10,  11,  12,  12,  12,  12,  12,  13,  13,  14,\n",
       "          15,  15,  16,  17,  17,  17,  17,  18,  19,  19,  19,  19,  20,  20,\n",
       "          21,  21,  21,  21,  21,  21,  21,  22,  23,  23,  23,  24,  25,  26,\n",
       "          27,  27,  28,  29,  30,  30,  31,  31,  32,  33,  33,  34,  35,  36,\n",
       "          36,  36,  37,  38,  38,  38,  38,  38,  38,  38,  38,  38,  39,  40,\n",
       "          40,  40,  40,  40,  40,  40,  40,  41,  41,  42,  43,  43,  43,  43,\n",
       "          43,  44,  44,  45,  45,  46,  47,  48,  48,  48,  48,  49,  49,  49,\n",
       "          49,  49,  50,  50,  50,  51,  51,  51,  52,  52,  52,  52,  52,  52,\n",
       "          53,  53,  53,  53,  53,  53,  53,  53,  54,  55,  56,  57,  58,  59,\n",
       "          59,  60,  61,  62,  62,  62,  62,  62,  63,  64,  65,  65,  65,  65,\n",
       "          66,  67,  67,  67,  68,  69,  70,  71,  72,  73,  74,  74,  75,  75,\n",
       "          75,  75,  75,  76,  76,  76,  76,  77,  77,  77,  77,  77,  77,  77,\n",
       "          78,  79,  80,  81,  82,  83,  83,  83,  83,  83,  83,  83,  83,  83,\n",
       "          83,  83,  83,  83,  83,  84,  85,  86,  86,  86,  87,  87,  88,  88,\n",
       "          89,  89,  90,  90,  90,  90,  91,  91,  91,  91,  91,  91,  92,  93,\n",
       "          93,  93,  93,  93,  93,  94,  94,  94,  94,  94,  94,  94,  94,  94,\n",
       "          95,  96,  96,  97,  98,  99, 100, 101, 101, 101, 101, 101, 101, 102,\n",
       "         102, 103, 103, 103, 103, 103, 104, 104, 105, 106, 106, 107, 108, 108,\n",
       "         109, 110, 110, 111, 112, 112, 112, 113, 114, 115, 116, 117, 117, 117,\n",
       "         118, 119, 120, 121, 122, 122, 122, 122, 123, 123, 123, 123, 124, 124,\n",
       "         125, 126, 126, 127, 127, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
       "         128, 128, 128, 129, 130, 131, 132, 133, 133, 134, 135, 135, 136, 137,\n",
       "         138, 138, 138, 138, 138, 138, 138, 138, 138, 139, 139, 139, 139, 140,\n",
       "         140, 140, 141, 141, 141, 142, 142, 143, 143, 144, 144, 144, 144, 144,\n",
       "         145, 146, 147, 147, 147, 147, 148, 148, 149, 150, 150, 151, 152, 152,\n",
       "         153, 153, 153, 153, 153, 153, 154, 155, 156, 157, 157, 157, 157, 157,\n",
       "         158, 159, 159, 159, 159, 159, 159, 159, 160, 160, 160, 161, 162, 163,\n",
       "         164, 165, 165, 166, 166, 167, 168, 168, 168, 169, 169, 170, 170, 171,\n",
       "         171, 172, 173, 173, 173, 174, 174, 174, 174, 174, 175, 176, 177, 177,\n",
       "         177, 177, 178, 178, 178, 178, 178, 179, 180, 180, 180, 180, 181, 181,\n",
       "         181, 181, 181, 182, 183, 184, 185, 186, 187, 188, 188, 188, 189, 190,\n",
       "         191, 192, 192, 192, 193, 194, 195, 196, 197, 197, 198, 199, 199],\n",
       "        [  0,   1,   2, 167,   3,   4,  65,  67,   5,   6,   7,   8,   9,  10,\n",
       "          21,  93, 144, 153, 159,  11,  12,  50,  75, 169, 177,  13,  52,  14,\n",
       "          15, 165,  16,  17,  19,  48, 139,  18,  17,  19,  48, 139,  20,  62,\n",
       "          10,  21,  53,  93, 144, 153, 159,  22,  23, 163, 198,  24,  25,  26,\n",
       "          27, 168,  28,  29,  30, 126,  31, 147,  32,  33, 110,  34,  35,  36,\n",
       "         136, 194,  37,   0,  33,  35,  38, 147, 169, 179, 187, 198,  39,  40,\n",
       "          43,  44,  50,  52,  70,  83, 110,  41, 140,  42,  40,  43,  83, 136,\n",
       "         169,  40,  44,  45, 173,  46,  47,  17,  19,  48, 139,  49,  83,  94,\n",
       "         128, 167,  50,  52, 146,  51,  76,  93,  40,  50,  52,  83, 117, 147,\n",
       "          21,  53,  93, 123, 153, 159, 181, 192,  54,  55,  56,  57,  58,  59,\n",
       "          83,  60,  61,  18,  20,  62, 134, 147,  63,  64,   7,  22,  65, 192,\n",
       "          66,   4,  67, 180,  68,  69,  70,  71,  72,  73,  74, 171,  14,  50,\n",
       "          75, 140, 199,  65,  76,  86, 102,  77,  83,  91,  94, 103, 128, 174,\n",
       "          78,  79,  80,  81,  82,  40,  43,  49,  52,  59,  77,  83,  91,  94,\n",
       "         101, 112, 128, 138, 181,  84,  85,   7,  65,  86,  87, 148,  88, 103,\n",
       "          89, 152,  18,  90, 147, 170,  77,  83,  91,  94, 128, 138,  92,  10,\n",
       "          21,  53,  93, 144, 159,  49,  77,  83,  91,  94, 128, 138, 174, 180,\n",
       "          95,  96, 102,  97,  98,  99, 100,  83, 101, 103, 128, 138, 157,  96,\n",
       "         102,  77,  88, 101, 103, 157, 104, 137, 105,   2, 106, 107, 108, 140,\n",
       "         109,  33, 110, 111,  83, 112, 138, 113, 114, 115, 116,  52, 117, 147,\n",
       "         118, 119, 120, 121,  19,  50, 110, 122,  21,  53, 123, 181, 124, 142,\n",
       "         125,  30, 126, 127, 142,  18,  49,  77,  83,  91,  94, 101, 122, 128,\n",
       "         138, 149, 157, 129, 130, 131, 132,  19, 133, 134, 135, 169, 136, 137,\n",
       "          43,  83,  91,  94, 101, 112, 128, 138, 157,  17,  19,  48, 139,  75,\n",
       "         108, 140, 141, 174, 192, 127, 142, 140, 143,  10,  21,  93, 144, 159,\n",
       "         145, 146,  43,  52, 117, 147,  87, 148, 149,   9, 150, 151,  89, 152,\n",
       "          10,  21,  53, 153, 159, 192, 154, 155, 156, 101, 103, 128, 138, 157,\n",
       "         158,  10,  21,  53,  93, 144, 153, 159,  50, 136, 160, 161, 162, 163,\n",
       "         164,  15, 165,  86, 166, 167,  27, 166, 168,  12, 169,  90, 170,  74,\n",
       "         171, 172,  45, 173, 179,  74,  77,  94, 141, 174, 175, 176,  50,  75,\n",
       "         140, 177,  64, 104, 172, 178, 198, 179,  67,  94, 104, 180,  53,  83,\n",
       "         110, 123, 181, 182, 183, 184, 185, 186, 187, 116, 133, 188, 189, 190,\n",
       "         191,  53, 153, 192, 193, 194, 195, 196, 120, 197, 198,  75, 199]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"mrna_feature\", \"interacts\", \"mrna_feature\"].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrna': tensor([[ 0.1901,  0.8589,  0.7247,  ...,  0.4696,  0.2355,  0.2009],\n",
       "         [ 0.1317,  0.7882,  0.7385,  ...,  0.8404,  0.4612,  0.0611],\n",
       "         [ 0.9406,  0.6232,  0.7357,  ...,  0.8423,  0.1146,  0.2261],\n",
       "         ...,\n",
       "         [ 0.5044,  0.5115,  0.6188,  ...,  0.3666,  0.1762,  0.7017],\n",
       "         [-0.2887,  0.5810,  0.8522,  ...,  0.3074,  0.1788,  1.1057],\n",
       "         [-0.0414,  0.5253,  0.5139,  ...,  0.1934,  0.9554,  0.1940]]),\n",
       " 'mirna': tensor([[ 0.8021,  0.7240,  0.8759,  ...,  0.7245,  0.6704,  0.8779],\n",
       "         [ 0.6343,  0.6804,  0.6110,  ...,  0.7883,  0.2520,  0.4453],\n",
       "         [ 0.4353,  0.6257,  0.7742,  ...,  0.6928,  0.4783,  0.6300],\n",
       "         ...,\n",
       "         [ 0.6901,  0.5982,  0.8047,  ...,  0.9011,  0.4010,  0.6526],\n",
       "         [ 0.7930,  0.8460,  0.6499,  ...,  0.8561,  0.7770,  0.6665],\n",
       "         [ 0.9770, -0.0268,  0.6509,  ...,  0.8369,  0.5275,  0.9227]])}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna_mrna_interactions_db = pl.read_csv(\"interaction_data/mirna_mrna_interactions_DB.csv\")\n",
    "mmirnas = mirna_mrna_interactions_db[\"mirna\"].to_list()\n",
    "mirna_gene_names = [\"\".join(mirna.split(\"-\")[1:3]).upper() for mirna in mmirnas]\n",
    "mirna_mrna_interactions_db.with_columns(\n",
    "    pl.Series(\"mirna\", mirna_gene_names)\n",
    ").select(\"mirna\", \"gene\").write_csv(\"interaction_data/mirna_genes_mrna.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
