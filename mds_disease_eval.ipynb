{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n"
     ]
    }
   ],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  1\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  2\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  3\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 48    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 13    │\n",
      "└───────┴───────┘\n",
      "fold:  4\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "for fold_idx in range(5):\n",
    "    train_df, test_df = mrna_loader.get_fold(fold_idx)\n",
    "\n",
    "    print(\"fold: \", fold_idx)\n",
    "    print(train_df[\"class\"].value_counts(), test_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    # \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.feature_dim, odm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,063] A new study created in memory with name: no-name-487dc805-fc43-418d-8a2d-a156c69f7612\n",
      "[I 2024-11-10 19:15:05,137] Trial 0 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,204] Trial 1 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,288] Trial 2 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,353] Trial 3 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,421] Trial 4 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,489] Trial 5 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,552] Trial 6 finished with value: 0.58024233114353 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,615] Trial 7 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,682] Trial 8 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,747] Trial 9 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,818] Trial 10 finished with value: 0.6723834634892419 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,886] Trial 11 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,955] Trial 12 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,042] Trial 13 finished with value: 0.5846632831891617 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,113] Trial 14 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,182] Trial 15 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,251] Trial 16 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,322] Trial 17 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,392] Trial 18 finished with value: 0.5888211266094818 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,458] Trial 19 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'n_neighbors': 16}\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,374] A new study created in memory with name: no-name-17b65a7d-695e-4e4c-9283-d9a803e51944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,465] Trial 0 finished with value: 0.6497992012967035 and parameters: {'C': 0.5912776084859577, 'class_weight': None, 'rfe_step': 0.18135798798684793, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,574] Trial 1 finished with value: 0.6497992012967035 and parameters: {'C': 1.92761937700467, 'class_weight': 'balanced', 'rfe_step': 0.12740681956658, 'rfe_n_features': 111}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,656] Trial 2 finished with value: 0.6477728936997709 and parameters: {'C': 0.04431683340011391, 'class_weight': 'balanced', 'rfe_step': 0.0561795144594292, 'rfe_n_features': 170}. Best is trial 0 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,742] Trial 3 finished with value: 0.5947745825537253 and parameters: {'C': 6.239037597620005, 'class_weight': None, 'rfe_step': 0.16137130043179504, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,804] Trial 4 finished with value: 0.6497992012967035 and parameters: {'C': 1.8024991990621209, 'class_weight': None, 'rfe_step': 0.16636180097742892, 'rfe_n_features': 200}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,914] Trial 5 finished with value: 0.5947745825537253 and parameters: {'C': 5.247154408538435, 'class_weight': None, 'rfe_step': 0.06264027199095756, 'rfe_n_features': 156}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,008] Trial 6 finished with value: 0.5947745825537253 and parameters: {'C': 9.321484902226024, 'class_weight': None, 'rfe_step': 0.16378265009504533, 'rfe_n_features': 154}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,119] Trial 7 finished with value: 0.5947745825537253 and parameters: {'C': 8.202967718129962, 'class_weight': None, 'rfe_step': 0.05914431146688243, 'rfe_n_features': 158}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,184] Trial 8 finished with value: 0.6477728936997709 and parameters: {'C': 0.04494677478861271, 'class_weight': 'balanced', 'rfe_step': 0.16643442620663507, 'rfe_n_features': 172}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,248] Trial 9 finished with value: 0.6477728936997709 and parameters: {'C': 0.019873129915673807, 'class_weight': 'balanced', 'rfe_step': 0.13255506502150974, 'rfe_n_features': 180}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,334] Trial 10 finished with value: 0.6288118975723532 and parameters: {'C': 0.2890553099749232, 'class_weight': None, 'rfe_step': 0.19984885818456594, 'rfe_n_features': 126}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,448] Trial 11 finished with value: 0.6497992012967035 and parameters: {'C': 0.7846558478244101, 'class_weight': 'balanced', 'rfe_step': 0.1008311361438369, 'rfe_n_features': 101}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,550] Trial 12 finished with value: 0.6787024344579722 and parameters: {'C': 0.25258158703203787, 'class_weight': 'balanced', 'rfe_step': 0.10914476888380255, 'rfe_n_features': 126}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,656] Trial 13 finished with value: 0.6787024344579722 and parameters: {'C': 0.21138530733394667, 'class_weight': 'balanced', 'rfe_step': 0.08776062841727579, 'rfe_n_features': 131}. Best is trial 12 with value: 0.6787024344579722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.679\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.905 ± 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:58,755] Trial 14 finished with value: 0.6787024344579722 and parameters: {'C': 0.15592096415861068, 'class_weight': 'balanced', 'rfe_step': 0.0936740719697733, 'rfe_n_features': 129}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,870] Trial 15 finished with value: 0.7331794285989254 and parameters: {'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:58,960] Trial 16 finished with value: 0.6809510924096367 and parameters: {'C': 0.050080573100068776, 'class_weight': 'balanced', 'rfe_step': 0.1057608612630386, 'rfe_n_features': 120}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,068] Trial 17 finished with value: 0.6422617001076846 and parameters: {'C': 0.07668997260540834, 'class_weight': 'balanced', 'rfe_step': 0.0832177373698388, 'rfe_n_features': 114}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.733\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:59,147] Trial 18 finished with value: 0.6477728936997709 and parameters: {'C': 0.010268450364919926, 'class_weight': 'balanced', 'rfe_step': 0.11357879132217108, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,238] Trial 19 finished with value: 0.6809510924096367 and parameters: {'C': 0.0644825018841236, 'class_weight': 'balanced', 'rfe_step': 0.14146501839591458, 'rfe_n_features': 116}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,340] Trial 20 finished with value: 0.728115464597245 and parameters: {'C': 0.10675039358903013, 'class_weight': 'balanced', 'rfe_step': 0.07651257359386716, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,445] Trial 21 finished with value: 0.728115464597245 and parameters: {'C': 0.1155640352564498, 'class_weight': 'balanced', 'rfe_step': 0.07635708884773021, 'rfe_n_features': 138}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,554] Trial 22 finished with value: 0.728115464597245 and parameters: {'C': 0.13297292036765632, 'class_weight': 'balanced', 'rfe_step': 0.0736767585819143, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,655] Trial 23 finished with value: 0.7331794285989254 and parameters: {'C': 0.0839042307438939, 'class_weight': 'balanced', 'rfe_step': 0.07312489903466748, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,748] Trial 24 finished with value: 0.6477728936997709 and parameters: {'C': 0.022068952997355576, 'class_weight': 'balanced', 'rfe_step': 0.06952300589961795, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,855] Trial 25 finished with value: 0.6497992012967035 and parameters: {'C': 0.4403890185742033, 'class_weight': 'balanced', 'rfe_step': 0.0517673718488775, 'rfe_n_features': 164}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,941] Trial 26 finished with value: 0.6477728936997709 and parameters: {'C': 0.023322190329971796, 'class_weight': 'balanced', 'rfe_step': 0.09350830825927489, 'rfe_n_features': 136}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,038] Trial 27 finished with value: 0.6926488504188454 and parameters: {'C': 0.09203815468971016, 'class_weight': 'balanced', 'rfe_step': 0.07812215855799436, 'rfe_n_features': 147}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,130] Trial 28 finished with value: 0.6809510924096367 and parameters: {'C': 0.03393875406287643, 'class_weight': 'balanced', 'rfe_step': 0.11620427238691917, 'rfe_n_features': 103}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,264] Trial 29 finished with value: 0.6497992012967035 and parameters: {'C': 0.5654000035290196, 'class_weight': 'balanced', 'rfe_step': 0.06790668927665994, 'rfe_n_features': 146}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.938 ± 0.058\n",
    "F1 Macro: 0.684 ± 0.258\n",
    "F1 Weighted: 0.924 ± 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:41,344] A new study created in memory with name: no-name-d74618ba-7c3c-4ca8-8ef7-5eab635f3ab8\n",
      "[I 2024-11-10 19:18:41,468] Trial 0 finished with value: 0.5046011800339922 and parameters: {'booster': 'gbtree', 'lambda': 1.090093549619324e-05, 'alpha': 0.0017384853973629977, 'max_depth': 6, 'eta': 0.024990822730382434, 'gamma': 0.0007448711414900602, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.5046011800339922.\n",
      "[I 2024-11-10 19:18:41,557] Trial 1 finished with value: 0.0013903478254046716 and parameters: {'booster': 'dart', 'lambda': 7.953733877278384e-07, 'alpha': 0.012311349329884858, 'max_depth': 1, 'eta': 1.3106729678135571e-08, 'gamma': 1.8912671827677967e-05, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 7.915196931299182e-06, 'skip_drop': 0.18697866330618118}. Best is trial 0 with value: 0.5046011800339922.\n",
      "[I 2024-11-10 19:18:41,664] Trial 2 finished with value: 0.5947745825537253 and parameters: {'booster': 'gbtree', 'lambda': 6.058943782920171e-06, 'alpha': 1.031488556994315e-06, 'max_depth': 7, 'eta': 7.307520855607819e-07, 'gamma': 0.00016232868881341684, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.5947745825537253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.505\n",
      "Best model performance:\n",
      "Accuracy: 0.839 ± 0.079\n",
      "F1 Macro: 0.716 ± 0.161\n",
      "F1 Weighted: 0.839 ± 0.077\n",
      "New best score: 0.595\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.077\n",
      "F1 Macro: 0.771 ± 0.177\n",
      "F1 Weighted: 0.878 ± 0.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:41,755] Trial 3 finished with value: 0.4773113900356288 and parameters: {'booster': 'dart', 'lambda': 0.12408402814102056, 'alpha': 9.110488005138047e-07, 'max_depth': 2, 'eta': 0.002647083756160064, 'gamma': 0.0003482942304648263, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.7436722774157082, 'skip_drop': 0.43481543073977125}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,833] Trial 4 finished with value: 0.553150014770979 and parameters: {'booster': 'gbtree', 'lambda': 0.2792310057749791, 'alpha': 0.8027307283253481, 'max_depth': 1, 'eta': 0.061263458984386496, 'gamma': 0.0005126645470834501, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,926] Trial 5 finished with value: 0.5201471744784413 and parameters: {'booster': 'dart', 'lambda': 0.0014880441853571502, 'alpha': 3.0784878218976924e-08, 'max_depth': 2, 'eta': 0.5338883219491826, 'gamma': 6.866987234602545e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.009400221878030034, 'skip_drop': 0.8992827497341372}. Best is trial 2 with value: 0.5947745825537253.\n",
      "[I 2024-11-10 19:18:41,992] Trial 6 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.004875944790006727, 'alpha': 8.278525432543186e-07}. Best is trial 6 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 19:18:42,080] Trial 7 finished with value: 0.5957839834409895 and parameters: {'booster': 'gbtree', 'lambda': 3.742688380242692e-05, 'alpha': 6.370361951091849e-07, 'max_depth': 3, 'eta': 0.8133082302804496, 'gamma': 5.634591381378434e-05, 'grow_policy': 'lossguide'}. Best is trial 6 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:42,204] Trial 8 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.649370961811432, 'alpha': 4.868692580741491e-07, 'max_depth': 8, 'eta': 9.401586697777986e-05, 'gamma': 1.4036135030391968e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 8.381540989037945e-07, 'skip_drop': 0.04372103568102062}. Best is trial 6 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 19:18:42,273] Trial 9 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00014034045703319477, 'alpha': 3.3108716699093745e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,344] Trial 10 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 4.068048925310283e-08, 'alpha': 8.019133438677717e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,416] Trial 11 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 9.064174582070118e-08, 'alpha': 9.451074100898181e-05}. Best is trial 9 with value: 0.7012351321451076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.701\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.053\n",
      "F1 Macro: 0.851 ± 0.082\n",
      "F1 Weighted: 0.910 ± 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:42,489] Trial 12 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 1.5959550373664294e-08, 'alpha': 0.00010698868845880366}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,562] Trial 13 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.00035866097780415574, 'alpha': 7.113577646025195e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,632] Trial 14 finished with value: 0.5636131189143786 and parameters: {'booster': 'gblinear', 'lambda': 5.831129710366558e-07, 'alpha': 0.001684214548016691}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,721] Trial 15 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.008447414817215762, 'alpha': 1.1290634951871592e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,790] Trial 16 finished with value: 0.5652708188613967 and parameters: {'booster': 'gblinear', 'lambda': 6.269592200405432e-05, 'alpha': 0.03427034067784772}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,865] Trial 17 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 2.385851657678274e-06, 'alpha': 2.4872522750497344e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:42,938] Trial 18 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.3206352393950663e-06, 'alpha': 8.95730116612407e-06}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,007] Trial 19 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.0002905407250572697, 'alpha': 3.169797980340927e-08}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,078] Trial 20 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 5.4453402105410365e-06, 'alpha': 1.0381639916892128e-05}. Best is trial 9 with value: 0.7012351321451076.\n",
      "[I 2024-11-10 19:18:43,148] Trial 21 finished with value: 0.7361252920039949 and parameters: {'booster': 'gblinear', 'lambda': 0.00045823825607512223, 'alpha': 1.673455869535136e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,222] Trial 22 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 0.0003008590890939841, 'alpha': 1.0399964313573931e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,289] Trial 23 finished with value: 0.6787024344579722 and parameters: {'booster': 'gblinear', 'lambda': 0.01211266400768025, 'alpha': 1.01203440092278e-08}. Best is trial 21 with value: 0.7361252920039949.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.736\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.868 ± 0.077\n",
      "F1 Weighted: 0.922 ± 0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:18:43,360] Trial 24 finished with value: 0.6712488553683833 and parameters: {'booster': 'gblinear', 'lambda': 3.3004014002630894e-05, 'alpha': 0.0006338456286067703}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,432] Trial 25 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0014121273544600171, 'alpha': 9.036017225149375e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,501] Trial 26 finished with value: 0.6787024344579722 and parameters: {'booster': 'gblinear', 'lambda': 0.032572906176680905, 'alpha': 0.0004046808096819849}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,568] Trial 27 finished with value: 0.612600042864821 and parameters: {'booster': 'gblinear', 'lambda': 2.7072163844552095e-07, 'alpha': 0.03251672985150403}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,690] Trial 28 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 3.2733728230848342e-06, 'alpha': 2.7390995376131084e-06, 'max_depth': 9, 'eta': 2.061457500786986e-05, 'gamma': 0.8401252851807804, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,821] Trial 29 finished with value: 0.0013903478254046716 and parameters: {'booster': 'dart', 'lambda': 1.3191777368429335e-05, 'alpha': 1.5451584673844528e-07, 'max_depth': 4, 'eta': 1.0754501284708833e-08, 'gamma': 1.3314581903620458e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.4079321561139179e-08, 'skip_drop': 1.4530117839254435e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:43,962] Trial 30 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 0.0001589567437812458, 'alpha': 3.219396774832125e-05, 'max_depth': 5, 'eta': 1.3331324483248153e-06, 'gamma': 0.881860676232723, 'grow_policy': 'lossguide'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,033] Trial 31 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0012097325177458764, 'alpha': 1.6081927283618518e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,103] Trial 32 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0005688788450926426, 'alpha': 2.95063555560621e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,173] Trial 33 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.5348054512345863e-05, 'alpha': 8.660050850899507e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,247] Trial 34 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00010019594718231664, 'alpha': 2.7384727065502e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,380] Trial 35 finished with value: 0.5844534524426352 and parameters: {'booster': 'dart', 'lambda': 0.00015652446655657268, 'alpha': 2.4574239322668134e-07, 'max_depth': 9, 'eta': 0.0023308867543869742, 'gamma': 0.02022329491796311, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0016980134341777087, 'skip_drop': 7.121720341343025e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,506] Trial 36 finished with value: 0.5587244304571011 and parameters: {'booster': 'gbtree', 'lambda': 0.004956739581441004, 'alpha': 5.56605578510018e-08, 'max_depth': 5, 'eta': 6.117033611818185e-07, 'gamma': 1.5025892054292314e-08, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,579] Trial 37 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 2.450175338402121e-06, 'alpha': 2.4430838546086436e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,711] Trial 38 finished with value: 0.5388231705365875 and parameters: {'booster': 'dart', 'lambda': 2.3541694178127426e-05, 'alpha': 0.5584646777655228, 'max_depth': 7, 'eta': 0.0005593645174947266, 'gamma': 0.01519419702938851, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.2693524381175154, 'skip_drop': 0.00024072915496283813}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,785] Trial 39 finished with value: 0.6122666381403189 and parameters: {'booster': 'gblinear', 'lambda': 0.0007983977846240928, 'alpha': 0.006144404440826941}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,903] Trial 40 finished with value: 0.5288763416697589 and parameters: {'booster': 'gbtree', 'lambda': 0.024895198820290943, 'alpha': 2.810254760556719e-07, 'max_depth': 4, 'eta': 5.9108631876170084e-06, 'gamma': 4.521538455136203e-07, 'grow_policy': 'depthwise'}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:44,976] Trial 41 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 7.056742905382381e-06, 'alpha': 1.8339921869772347e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,049] Trial 42 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 5.396304600763562e-05, 'alpha': 0.00018969516896196462}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,120] Trial 43 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.0026864412711625264, 'alpha': 2.945173429413845e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,194] Trial 44 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 4.643284043831129e-06, 'alpha': 4.0869159017792e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,269] Trial 45 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 3.0196533887378714e-07, 'alpha': 5.073582089866101e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,338] Trial 46 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00027359635947917654, 'alpha': 1.1883518471032676e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,487] Trial 47 finished with value: 0.5587244304571011 and parameters: {'booster': 'dart', 'lambda': 8.446585593490166e-06, 'alpha': 0.00026755584535787273, 'max_depth': 7, 'eta': 9.74462883298785e-08, 'gamma': 0.02272354982446128, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.064724094184567e-08, 'skip_drop': 0.00037889227065196355}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,557] Trial 48 finished with value: 0.5947745825537253 and parameters: {'booster': 'gblinear', 'lambda': 1.7109916415773722e-06, 'alpha': 0.0013398074341008051}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,626] Trial 49 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 7.855917112352985e-05, 'alpha': 2.5683469783657177e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,695] Trial 50 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 5.790427409665389e-07, 'alpha': 1.033175348350204e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,766] Trial 51 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 0.00012378111850352747, 'alpha': 5.2481560130789185e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,837] Trial 52 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 0.0005056923954403906, 'alpha': 1.932750235405262e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:45,907] Trial 53 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 3.7524617940600926e-05, 'alpha': 4.689554474205462e-07}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,005] Trial 54 finished with value: 0.6497992012967035 and parameters: {'booster': 'gblinear', 'lambda': 0.002972435146225337, 'alpha': 6.760168850393416e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,081] Trial 55 finished with value: 0.6564167377920205 and parameters: {'booster': 'gblinear', 'lambda': 0.0002893758890897774, 'alpha': 1.5356842908428545e-05}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,153] Trial 56 finished with value: 0.6663697855345576 and parameters: {'booster': 'gblinear', 'lambda': 1.8890483214727648e-05, 'alpha': 5.287869418033139e-06}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,230] Trial 57 finished with value: 0.7012351321451076 and parameters: {'booster': 'gblinear', 'lambda': 6.749349378808268e-08, 'alpha': 1.657118793967962e-08}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,303] Trial 58 finished with value: 0.689882594756958 and parameters: {'booster': 'gblinear', 'lambda': 9.034618527677589e-05, 'alpha': 0.0001563407466528211}. Best is trial 21 with value: 0.7361252920039949.\n",
      "[I 2024-11-10 19:18:46,434] Trial 59 finished with value: 0.52987054189302 and parameters: {'booster': 'dart', 'lambda': 0.0002057213154824834, 'alpha': 4.067766212756909e-05, 'max_depth': 3, 'eta': 0.04813707597036521, 'gamma': 6.687206129035324e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 9.836850868242433e-05, 'skip_drop': 4.078942122084955e-06}. Best is trial 21 with value: 0.7361252920039949.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'booster': 'gblinear', 'lambda': 0.00045823825607512223, 'alpha': 1.673455869535136e-08}\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.868 ± 0.077\n",
      "F1 Weighted: 0.922 ± 0.047\n"
     ]
    }
   ],
   "source": [
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 19:38:31,736] A new study created in memory with name: no-name-fb818b66-fdee-40b1-8f69-a558bed153e5\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:33,026] Trial 0 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00031835665015120484, 'dropout': 0.20654770214911006}. Best is trial 0 with value: 0.5814461133586074.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.581\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.032\n",
      "F1 Macro: 0.745 ± 0.154\n",
      "F1 Weighted: 0.874 ± 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:34,320] Trial 1 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0003994236638881323, 'dropout': 0.31297149922050155}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:35,641] Trial 2 finished with value: 0.5814461133586074 and parameters: {'lr': 0.0020550157776878125, 'dropout': 0.45506523137093347}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:36,932] Trial 3 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0023832168392858106, 'dropout': 0.44985818791023646}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:38,197] Trial 4 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00029998028591521244, 'dropout': 0.16352985288896082}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:39,449] Trial 5 finished with value: 0.7101929043348356 and parameters: {'lr': 0.003514759206662055, 'dropout': 0.3875206144282498}. Best is trial 5 with value: 0.7101929043348356.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.710\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:40,733] Trial 6 finished with value: 0.7290661114709509 and parameters: {'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:42,027] Trial 7 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0037000620150167396, 'dropout': 0.3974109441837972}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:43,296] Trial 8 finished with value: 0.6288118975723532 and parameters: {'lr': 0.001620528316220534, 'dropout': 0.1429762051271858}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:44,620] Trial 9 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0001195833284751336, 'dropout': 0.2314216389186731}. Best is trial 6 with value: 0.7290661114709509.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.5],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n",
      "Best hyperparameters:\n",
      "{'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"knn\")\n",
    "svm_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"svm\")\n",
    "xgb_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.923 ± 0.084\n",
      "F1 Macro: 0.825 ± 0.208\n",
      "F1 Weighted: 0.933 ± 0.072\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "knn_eval.print_best_results()\n",
    "svm_eval.print_best_results()\n",
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.235535  ┆ 0.200897  ┆ N54       ┆ 0     │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.461213  ┆ 0.061081  ┆ N58       ┆ 0     │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.114601  ┆ 0.226098  ┆ N82       ┆ 0     │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.0       ┆ 0.209648  ┆ N83       ┆ 0     │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.017143  ┆ 0.183437  ┆ N84       ┆ 0     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.390905  ┆ 0.471158  ┆ V777      ┆ 1     │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.733365  ┆ 0.766874  ┆ V806      ┆ 1     │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.093215  ┆ 0.636191  ┆ V839      ┆ 1     │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.630781  ┆ 0.495458  ┆ V883      ┆ 1     │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.637763  ┆ 0.268635  ┆ V888      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘,\n",
       " shape: (15, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.403793  ┆ 0.284774  ┆ N60       ┆ 0     │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.417659  ┆ 0.313869  ┆ N70       ┆ 0     │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.117834  ┆ 0.285621  ┆ N85       ┆ 0     │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.374325  ┆ 0.466707  ┆ V108      ┆ 1     │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 0.253248  ┆ 0.812754  ┆ V1297     ┆ 1     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.081684  ┆ 0.283214  ┆ V221      ┆ 1     │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.261181  ┆ 0.413476  ┆ V456      ┆ 1     │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.1762    ┆ 0.701686  ┆ V538      ┆ 1     │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.178761  ┆ 1.105654  ┆ V574      ┆ 1     │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.955394  ┆ 0.193979  ┆ V940      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrna_loader.get_fold(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.770044 ┆ 0.741842 ┆ 0.767158 ┆ 0.87709  │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.628628 ┆ 0.630266 ┆ 0.27247  ┆ 0.548661 │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.417187 ┆ 0.635949 ┆ 0.342004 ┆ 0.649414 │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.791224 ┆ 0.742146 ┆ 0.109209 ┆ 0.684224 │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.467998 ┆ 0.54089  ┆ 0.362278 ┆ 0.494964 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.942295 ┆ 0.816323 ┆ 0.630765 ┆ 0.78563  │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.574101 ┆ 0.724553 ┆ 0.044501 ┆ 0.687679 │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.987056 ┆ 0.689131 ┆ 0.576118 ┆ 0.892948 │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.594205 ┆ 0.57892  ┆ 0.349641 ┆ 0.600517 │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.541005 ┆ 0.44407  ┆ 0.433872 ┆ 0.336804 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " shape: (15, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.34427  ┆ 0.699313 ┆ 0.544814 ┆ 0.635649 │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.296186 ┆ 0.341609 ┆ 0.329612 ┆ 0.375117 │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.591366 ┆ 0.505929 ┆ 0.130364 ┆ 0.352736 │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.765963 ┆ 0.758159 ┆ 0.878573 ┆ 0.923286 │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 1.060318 ┆ 1.127576 ┆ 0.740619 ┆ 1.126258 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.910938 ┆ 0.703334 ┆ 0.682987 ┆ 0.731746 │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.647633 ┆ 1.039966 ┆ 0.69509  ┆ 1.039513 │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.983345 ┆ 0.899831 ┆ 0.183759 ┆ 0.94264  │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.767064 ┆ 0.753895 ┆ 0.422372 ┆ 0.772614 │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.249167 ┆ 0.635731 ┆ 0.254413 ┆ 0.622748 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.190054  ┆ 0.8589    ┆ 0.724728  ┆ 0.796341  ┆ … ┆ 0.792052  ┆ 0.724515  ┆ 0.670447  ┆ 0.877857 │\n",
       " │ 0.131677  ┆ 0.788182  ┆ 0.73847   ┆ 0.566229  ┆ … ┆ 0.570109  ┆ 0.788338  ┆ 0.25199   ┆ 0.445283 │\n",
       " │ 0.94058   ┆ 0.623225  ┆ 0.735662  ┆ 0.41357   ┆ … ┆ 0.39374   ┆ 0.692776  ┆ 0.478255  ┆ 0.630045 │\n",
       " │ 0.805491  ┆ 0.642498  ┆ 0.62087   ┆ 0.43411   ┆ … ┆ 0.656948  ┆ 0.800219  ┆ 0.68736   ┆ 0.821026 │\n",
       " │ 1.0       ┆ 0.789645  ┆ 0.52572   ┆ 0.214973  ┆ … ┆ 0.666039  ┆ 0.897311  ┆ 0.771568  ┆ 0.806792 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.226835  ┆ 0.480819  ┆ 1.0       ┆ 0.766894  ┆ … ┆ 0.52434   ┆ 0.73862   ┆ 0.457018  ┆ 0.690925 │\n",
       " │ 0.123242  ┆ 0.591142  ┆ 0.42922   ┆ 0.684059  ┆ … ┆ 0.802056  ┆ 0.921184  ┆ 0.314539  ┆ 0.94341  │\n",
       " │ 0.559905  ┆ 0.608163  ┆ 0.5361    ┆ 0.619954  ┆ … ┆ 0.674672  ┆ 0.955206  ┆ 0.787847  ┆ 0.672842 │\n",
       " │ 0.138803  ┆ 0.760565  ┆ 0.817056  ┆ 0.712027  ┆ … ┆ 0.848153  ┆ 0.876225  ┆ 0.836959  ┆ 1.0      │\n",
       " │ 0.219329  ┆ 0.813799  ┆ 0.562113  ┆ 0.573218  ┆ … ┆ 0.574968  ┆ 0.769111  ┆ 0.359642  ┆ 0.478356 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " shape: (15, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.20333   ┆ 0.738426  ┆ 0.846229  ┆ 0.688066  ┆ … ┆ 0.359969  ┆ 0.746973  ┆ 0.238121  ┆ 0.522262 │\n",
       " │ 0.191396  ┆ 0.819688  ┆ 0.91776   ┆ 0.82761   ┆ … ┆ 0.460708  ┆ 0.782432  ┆ 0.363974  ┆ 0.661165 │\n",
       " │ 0.949997  ┆ 0.784648  ┆ 0.654279  ┆ 0.11519   ┆ … ┆ 0.471986  ┆ 0.407116  ┆ 0.389313  ┆ 0.551279 │\n",
       " │ 0.047037  ┆ 0.460519  ┆ 0.601395  ┆ 0.754197  ┆ … ┆ 0.749248  ┆ 0.706021  ┆ 0.465903  ┆ 0.671848 │\n",
       " │ 0.560943  ┆ 0.237744  ┆ 0.38922   ┆ 0.4356    ┆ … ┆ 0.834129  ┆ 0.890805  ┆ 0.737512  ┆ 0.667177 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.456982  ┆ 0.455895  ┆ 0.55602   ┆ 0.528333  ┆ … ┆ 0.935791  ┆ 1.077982  ┆ 0.708828  ┆ 1.100682 │\n",
       " │ 0.015711  ┆ 0.132877  ┆ 1.11073   ┆ 0.720927  ┆ … ┆ 0.554755  ┆ 0.955612  ┆ 0.010688  ┆ 0.735628 │\n",
       " │ 0.504371  ┆ 0.511528  ┆ 0.618815  ┆ 0.492415  ┆ … ┆ 0.49578   ┆ 0.901123  ┆ 0.400956  ┆ 0.652603 │\n",
       " │ -0.288737 ┆ 0.580996  ┆ 0.852197  ┆ 0.760822  ┆ … ┆ 0.773837  ┆ 0.856068  ┆ 0.776993  ┆ 0.666524 │\n",
       " │ -0.041422 ┆ 0.525319  ┆ 0.513923  ┆ 0.752942  ┆ … ┆ 0.878347  ┆ 0.836865  ┆ 0.52754   ┆ 0.922727 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catodm = CatOmicDataManager(\n",
    "    omic_data_loaders={\n",
    "        \"mrna\": mrna_loader,\n",
    "        # \"meth\": meth_loader,\n",
    "        \"mirna\": mirna_loader,\n",
    "    },\n",
    "    n_splits=5,\n",
    ")\n",
    "\n",
    "catodm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:34:09,314] A new study created in memory with name: no-name-f369a223-3c9c-4fe4-9dfd-0047230ac178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 025:\n",
      "Train Loss: 0.5289, Train Acc: 0.6780, Train F1 Macro: 0.4040, Train F1 Weighted: 0.6711\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.3526, Train Acc: 0.7458, Train F1 Macro: 0.5295, Train F1 Weighted: 0.7404\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 075:\n",
      "Train Loss: 0.2607, Train Acc: 0.9153, Train F1 Macro: 0.8552, Train F1 Weighted: 0.9168\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.1916, Train Acc: 0.9322, Train F1 Macro: 0.8796, Train F1 Weighted: 0.9322\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7115, Val F1 Weighted: 0.8385\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7115, Test F1 Weighted: 0.8385\n",
      "##################################################\n",
      "\n",
      "Epoch: 025:\n",
      "Train Loss: 0.5062, Train Acc: 0.7119, Train F1 Macro: 0.4158, Train F1 Weighted: 0.6907\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4113, Train Acc: 0.7288, Train F1 Macro: 0.4216, Train F1 Weighted: 0.7002\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 075:\n",
      "Train Loss: 0.3272, Train Acc: 0.7966, Train F1 Macro: 0.4434, Train F1 Weighted: 0.7365\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3318, Train Acc: 0.8136, Train F1 Macro: 0.6814, Train F1 Weighted: 0.8170\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8800, Val F1 Weighted: 0.9280\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8800, Test F1 Weighted: 0.9280\n",
      "##################################################\n",
      "\n",
      "Epoch: 025:\n",
      "Train Loss: 0.5045, Train Acc: 0.6949, Train F1 Macro: 0.4100, Train F1 Weighted: 0.6810\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4253, Train Acc: 0.6610, Train F1 Macro: 0.3980, Train F1 Weighted: 0.6610\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.4444, Val F1 Weighted: 0.7111\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.4444, Test F1 Weighted: 0.7111\n",
      "##################################################\n",
      "\n",
      "Epoch: 075:\n",
      "Train Loss: 0.2890, Train Acc: 0.8983, Train F1 Macro: 0.8194, Train F1 Weighted: 0.8983\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.7917, Val F1 Weighted: 0.8667\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.7917, Test F1 Weighted: 0.8667\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2656, Train Acc: 0.8814, Train F1 Macro: 0.8224, Train F1 Weighted: 0.8900\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.8295, Val F1 Weighted: 0.8773\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.8295, Test F1 Weighted: 0.8773\n",
      "##################################################\n",
      "\n",
      "Epoch: 025:\n",
      "Train Loss: 0.6002, Train Acc: 0.6610, Train F1 Macro: 0.3980, Train F1 Weighted: 0.6475\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4390, Train Acc: 0.7119, Train F1 Macro: 0.4158, Train F1 Weighted: 0.6766\n",
      "Val Acc: 0.8667, Val F1 Macro: 0.4643, Val F1 Weighted: 0.8048\n",
      "Test Acc: 0.8667, Test F1 Macro: 0.4643, Test F1 Weighted: 0.8048\n",
      "##################################################\n",
      "\n",
      "Epoch: 075:\n",
      "Train Loss: 0.3161, Train Acc: 0.8983, Train F1 Macro: 0.8033, Train F1 Weighted: 0.8890\n",
      "Val Acc: 0.9333, Val F1 Macro: 0.8148, Val F1 Weighted: 0.9235\n",
      "Test Acc: 0.9333, Test F1 Macro: 0.8148, Test F1 Weighted: 0.9235\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2634, Train Acc: 0.8983, Train F1 Macro: 0.8595, Train F1 Weighted: 0.9058\n",
      "Val Acc: 1.0000, Val F1 Macro: 1.0000, Val F1 Weighted: 1.0000\n",
      "Test Acc: 1.0000, Test F1 Macro: 1.0000, Test F1 Weighted: 1.0000\n",
      "##################################################\n",
      "\n",
      "Epoch: 025:\n",
      "Train Loss: 0.6146, Train Acc: 0.6000, Train F1 Macro: 0.3750, Train F1 Weighted: 0.6125\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4006, Train Acc: 0.6667, Train F1 Macro: 0.4000, Train F1 Weighted: 0.6533\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.4615, Val F1 Weighted: 0.7912\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.4615, Test F1 Weighted: 0.7912\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:34:11,036] Trial 0 finished with value: 0.6882308988782632 and parameters: {}. Best is trial 0 with value: 0.6882308988782632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 075:\n",
      "Train Loss: 0.3202, Train Acc: 0.8500, Train F1 Macro: 0.7726, Train F1 Weighted: 0.8566\n",
      "Val Acc: 0.9286, Val F1 Macro: 0.8783, Val F1 Weighted: 0.9342\n",
      "Test Acc: 0.9286, Test F1 Macro: 0.8783, Test F1 Weighted: 0.9342\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2251, Train Acc: 0.8667, Train F1 Macro: 0.8137, Train F1 Weighted: 0.8766\n",
      "Val Acc: 0.8571, Val F1 Macro: 0.7879, Val F1 Weighted: 0.8745\n",
      "Test Acc: 0.8571, Test F1 Macro: 0.7879, Test F1 Weighted: 0.8745\n",
      "##################################################\n",
      "New best score: 0.688\n",
      "Best model performance:\n",
      "Accuracy: 0.905 ± 0.055\n",
      "F1 Macro: 0.842 ± 0.096\n",
      "F1 Weighted: 0.904 ± 0.056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': np.float64(0.9047619047619048),\n",
       " 'f1_macro': np.float64(0.8417925407925407),\n",
       " 'f1_weighted': np.float64(0.9036386280386282),\n",
       " 'acc_std': np.float64(0.05487565825100165),\n",
       " 'f1_macro_std': np.float64(0.09644917742011722),\n",
       " 'f1_weighted_std': np.float64(0.055988681352628106)}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders={\n",
    "            \"mrna\": mrna_loader,\n",
    "            # \"mirna\": mirna_loader,\n",
    "        },\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            # \"mirna\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"linear\",\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 100,\n",
    "        \"log_interval\": 25,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32602688670158386, tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask, data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 2, 2, 2]), tensor([3, 4, 5]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3, 4, 5])\n",
    "b = torch.tensor([0, 0, 1, 1, 1])\n",
    "a[b], a[b.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.test_mask]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
