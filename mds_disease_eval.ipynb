{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n",
      "files\n",
      "['train_0.csv', 'train_1.csv', 'train_2.csv', 'train_3.csv', 'train_4.csv']\n",
      "['test_0.csv', 'test_1.csv', 'test_2.csv', 'test_3.csv', 'test_4.csv']\n"
     ]
    }
   ],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  1\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  2\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 10    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 3     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n",
      "fold:  3\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 48    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 13    │\n",
      "└───────┴───────┘\n",
      "fold:  4\n",
      "shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 11    │\n",
      "│ 1     ┆ 49    │\n",
      "└───────┴───────┘ shape: (2, 2)\n",
      "┌───────┬───────┐\n",
      "│ class ┆ count │\n",
      "│ ---   ┆ ---   │\n",
      "│ i64   ┆ u32   │\n",
      "╞═══════╪═══════╡\n",
      "│ 0     ┆ 2     │\n",
      "│ 1     ┆ 12    │\n",
      "└───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "for fold_idx in range(5):\n",
    "    train_df, test_df = mrna_loader.get_fold(fold_idx)\n",
    "\n",
    "    print(\"fold: \", fold_idx)\n",
    "    print(train_df[\"class\"].value_counts(), test_df[\"class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.feature_dim, odm.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,063] A new study created in memory with name: no-name-487dc805-fc43-418d-8a2d-a156c69f7612\n",
      "[I 2024-11-10 19:15:05,137] Trial 0 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,204] Trial 1 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,288] Trial 2 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.716\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 19:15:05,353] Trial 3 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,421] Trial 4 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,489] Trial 5 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,552] Trial 6 finished with value: 0.58024233114353 and parameters: {'n_neighbors': 6}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,615] Trial 7 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,682] Trial 8 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 12}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,747] Trial 9 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,818] Trial 10 finished with value: 0.6723834634892419 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,886] Trial 11 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:05,955] Trial 12 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,042] Trial 13 finished with value: 0.5846632831891617 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,113] Trial 14 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,182] Trial 15 finished with value: 0.6977205806439 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,251] Trial 16 finished with value: 0.6804361055090542 and parameters: {'n_neighbors': 14}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,322] Trial 17 finished with value: 0.7155098095140549 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,392] Trial 18 finished with value: 0.5888211266094818 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.7155098095140549.\n",
      "[I 2024-11-10 19:15:06,458] Trial 19 finished with value: 0.6812403790638734 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.7155098095140549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'n_neighbors': 16}\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.050\n",
      "F1 Macro: 0.852 ± 0.097\n",
      "F1 Weighted: 0.913 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,374] A new study created in memory with name: no-name-17b65a7d-695e-4e4c-9283-d9a803e51944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,465] Trial 0 finished with value: 0.6497992012967035 and parameters: {'C': 0.5912776084859577, 'class_weight': None, 'rfe_step': 0.18135798798684793, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,574] Trial 1 finished with value: 0.6497992012967035 and parameters: {'C': 1.92761937700467, 'class_weight': 'balanced', 'rfe_step': 0.12740681956658, 'rfe_n_features': 111}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,656] Trial 2 finished with value: 0.6477728936997709 and parameters: {'C': 0.04431683340011391, 'class_weight': 'balanced', 'rfe_step': 0.0561795144594292, 'rfe_n_features': 170}. Best is trial 0 with value: 0.6497992012967035.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.650\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.814 ± 0.093\n",
      "F1 Weighted: 0.895 ± 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:57,742] Trial 3 finished with value: 0.5947745825537253 and parameters: {'C': 6.239037597620005, 'class_weight': None, 'rfe_step': 0.16137130043179504, 'rfe_n_features': 150}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,804] Trial 4 finished with value: 0.6497992012967035 and parameters: {'C': 1.8024991990621209, 'class_weight': None, 'rfe_step': 0.16636180097742892, 'rfe_n_features': 200}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:57,914] Trial 5 finished with value: 0.5947745825537253 and parameters: {'C': 5.247154408538435, 'class_weight': None, 'rfe_step': 0.06264027199095756, 'rfe_n_features': 156}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,008] Trial 6 finished with value: 0.5947745825537253 and parameters: {'C': 9.321484902226024, 'class_weight': None, 'rfe_step': 0.16378265009504533, 'rfe_n_features': 154}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,119] Trial 7 finished with value: 0.5947745825537253 and parameters: {'C': 8.202967718129962, 'class_weight': None, 'rfe_step': 0.05914431146688243, 'rfe_n_features': 158}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,184] Trial 8 finished with value: 0.6477728936997709 and parameters: {'C': 0.04494677478861271, 'class_weight': 'balanced', 'rfe_step': 0.16643442620663507, 'rfe_n_features': 172}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,248] Trial 9 finished with value: 0.6477728936997709 and parameters: {'C': 0.019873129915673807, 'class_weight': 'balanced', 'rfe_step': 0.13255506502150974, 'rfe_n_features': 180}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,334] Trial 10 finished with value: 0.6288118975723532 and parameters: {'C': 0.2890553099749232, 'class_weight': None, 'rfe_step': 0.19984885818456594, 'rfe_n_features': 126}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,448] Trial 11 finished with value: 0.6497992012967035 and parameters: {'C': 0.7846558478244101, 'class_weight': 'balanced', 'rfe_step': 0.1008311361438369, 'rfe_n_features': 101}. Best is trial 0 with value: 0.6497992012967035.\n",
      "[I 2024-11-10 15:04:58,550] Trial 12 finished with value: 0.6787024344579722 and parameters: {'C': 0.25258158703203787, 'class_weight': 'balanced', 'rfe_step': 0.10914476888380255, 'rfe_n_features': 126}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,656] Trial 13 finished with value: 0.6787024344579722 and parameters: {'C': 0.21138530733394667, 'class_weight': 'balanced', 'rfe_step': 0.08776062841727579, 'rfe_n_features': 131}. Best is trial 12 with value: 0.6787024344579722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.679\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.905 ± 0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:58,755] Trial 14 finished with value: 0.6787024344579722 and parameters: {'C': 0.15592096415861068, 'class_weight': 'balanced', 'rfe_step': 0.0936740719697733, 'rfe_n_features': 129}. Best is trial 12 with value: 0.6787024344579722.\n",
      "[I 2024-11-10 15:04:58,870] Trial 15 finished with value: 0.7331794285989254 and parameters: {'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:58,960] Trial 16 finished with value: 0.6809510924096367 and parameters: {'C': 0.050080573100068776, 'class_weight': 'balanced', 'rfe_step': 0.1057608612630386, 'rfe_n_features': 120}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,068] Trial 17 finished with value: 0.6422617001076846 and parameters: {'C': 0.07668997260540834, 'class_weight': 'balanced', 'rfe_step': 0.0832177373698388, 'rfe_n_features': 114}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.733\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-10 15:04:59,147] Trial 18 finished with value: 0.6477728936997709 and parameters: {'C': 0.010268450364919926, 'class_weight': 'balanced', 'rfe_step': 0.11357879132217108, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,238] Trial 19 finished with value: 0.6809510924096367 and parameters: {'C': 0.0644825018841236, 'class_weight': 'balanced', 'rfe_step': 0.14146501839591458, 'rfe_n_features': 116}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,340] Trial 20 finished with value: 0.728115464597245 and parameters: {'C': 0.10675039358903013, 'class_weight': 'balanced', 'rfe_step': 0.07651257359386716, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,445] Trial 21 finished with value: 0.728115464597245 and parameters: {'C': 0.1155640352564498, 'class_weight': 'balanced', 'rfe_step': 0.07635708884773021, 'rfe_n_features': 138}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,554] Trial 22 finished with value: 0.728115464597245 and parameters: {'C': 0.13297292036765632, 'class_weight': 'balanced', 'rfe_step': 0.0736767585819143, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,655] Trial 23 finished with value: 0.7331794285989254 and parameters: {'C': 0.0839042307438939, 'class_weight': 'balanced', 'rfe_step': 0.07312489903466748, 'rfe_n_features': 139}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,748] Trial 24 finished with value: 0.6477728936997709 and parameters: {'C': 0.022068952997355576, 'class_weight': 'balanced', 'rfe_step': 0.06952300589961795, 'rfe_n_features': 141}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,855] Trial 25 finished with value: 0.6497992012967035 and parameters: {'C': 0.4403890185742033, 'class_weight': 'balanced', 'rfe_step': 0.0517673718488775, 'rfe_n_features': 164}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:04:59,941] Trial 26 finished with value: 0.6477728936997709 and parameters: {'C': 0.023322190329971796, 'class_weight': 'balanced', 'rfe_step': 0.09350830825927489, 'rfe_n_features': 136}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,038] Trial 27 finished with value: 0.6926488504188454 and parameters: {'C': 0.09203815468971016, 'class_weight': 'balanced', 'rfe_step': 0.07812215855799436, 'rfe_n_features': 147}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,130] Trial 28 finished with value: 0.6809510924096367 and parameters: {'C': 0.03393875406287643, 'class_weight': 'balanced', 'rfe_step': 0.11620427238691917, 'rfe_n_features': 103}. Best is trial 15 with value: 0.7331794285989254.\n",
      "[I 2024-11-10 15:05:00,264] Trial 29 finished with value: 0.6497992012967035 and parameters: {'C': 0.5654000035290196, 'class_weight': 'balanced', 'rfe_step': 0.06790668927665994, 'rfe_n_features': 146}. Best is trial 15 with value: 0.7331794285989254.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'C': 0.10243547401908666, 'class_weight': 'balanced', 'rfe_step': 0.09295207471213159, 'rfe_n_features': 131}\n",
      "Best model performance:\n",
      "Accuracy: 0.918 ± 0.030\n",
      "F1 Macro: 0.867 ± 0.041\n",
      "F1 Weighted: 0.921 ± 0.024\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=30,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"rfe_step_range\": (0.05, 0.2),\n",
    "        \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model performance:\n",
    "Accuracy: 0.938 ± 0.058\n",
    "F1 Macro: 0.684 ± 0.258\n",
    "F1 Weighted: 0.924 ± 0.064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:48,987] A new study created in memory with name: no-name-078ffa6d-2c7a-496a-b139-dae54416a30c\n",
      "[I 2024-11-12 23:57:49,181] Trial 0 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 0.2012630144698972, 'alpha': 4.341615091163527e-08, 'max_depth': 3, 'eta': 2.332756805332267e-06, 'gamma': 1.2412516726806618e-06, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.5036718571125454.\n",
      "[I 2024-11-12 23:57:49,288] Trial 1 finished with value: 0.6219707163863691 and parameters: {'booster': 'gblinear', 'lambda': 8.646955492392989e-07, 'alpha': 1.5506307885311386e-07}. Best is trial 1 with value: 0.6219707163863691.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.504\n",
      "Best model performance:\n",
      "Accuracy: 0.825 ± 0.123\n",
      "F1 Macro: 0.729 ± 0.187\n",
      "F1 Weighted: 0.838 ± 0.104\n",
      "New best score: 0.622\n",
      "Best model performance:\n",
      "Accuracy: 0.879 ± 0.049\n",
      "F1 Macro: 0.804 ± 0.088\n",
      "F1 Weighted: 0.880 ± 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:49,497] Trial 2 finished with value: 0.5325885489025592 and parameters: {'booster': 'dart', 'lambda': 2.5211337235291564e-07, 'alpha': 0.0007655003241959716, 'max_depth': 3, 'eta': 3.1708961722098076e-07, 'gamma': 0.6434137367112158, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 2.4656097396901352e-08, 'skip_drop': 1.394809407147541e-05}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,604] Trial 3 finished with value: 0.5652269064579075 and parameters: {'booster': 'gblinear', 'lambda': 9.852933249626684e-05, 'alpha': 0.0011541940286663782}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,784] Trial 4 finished with value: 0.5844534524426352 and parameters: {'booster': 'gbtree', 'lambda': 6.894009654657362e-05, 'alpha': 0.013098965460206181, 'max_depth': 4, 'eta': 3.221944198197507e-05, 'gamma': 2.1300756208798475e-07, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:49,977] Trial 5 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.06827875535860767, 'alpha': 0.0013947811316405605, 'max_depth': 8, 'eta': 8.226228187567583e-07, 'gamma': 2.1496589291743587e-06, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,156] Trial 6 finished with value: 0.5036718571125454 and parameters: {'booster': 'gbtree', 'lambda': 1.586758017056682e-08, 'alpha': 0.00027096780439986665, 'max_depth': 3, 'eta': 1.639910100371045e-05, 'gamma': 3.083038221040991e-05, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,335] Trial 7 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.01441133265718125, 'alpha': 1.517997363432716e-07, 'max_depth': 3, 'eta': 6.05186690982422e-06, 'gamma': 1.493397804642125e-05, 'grow_policy': 'depthwise'}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,444] Trial 8 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 5.154325644399204e-06, 'alpha': 7.218531093288851e-07}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,602] Trial 9 finished with value: 0.49872960448032255 and parameters: {'booster': 'dart', 'lambda': 0.012793603702235803, 'alpha': 2.1551367986267964e-07, 'max_depth': 1, 'eta': 7.765576996118632e-07, 'gamma': 0.07662326036468593, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.03260828154492218, 'skip_drop': 0.002823072031521814}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,713] Trial 10 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 2.42729620598781e-06, 'alpha': 7.409878421702675e-06}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,825] Trial 11 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 3.2574254119276433e-06, 'alpha': 1.7501251014935886e-06}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:50,933] Trial 12 finished with value: 0.27794757275052406 and parameters: {'booster': 'gblinear', 'lambda': 3.031008780600074e-06, 'alpha': 0.9313765259674033}. Best is trial 1 with value: 0.6219707163863691.\n",
      "[I 2024-11-12 23:57:51,046] Trial 13 finished with value: 0.6522278748969422 and parameters: {'booster': 'gblinear', 'lambda': 6.837093128801486e-08, 'alpha': 1.0154079836126915e-08}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,160] Trial 14 finished with value: 0.5844131317615832 and parameters: {'booster': 'gblinear', 'lambda': 1.3457712092301079e-08, 'alpha': 1.3358060209942263e-05}. Best is trial 13 with value: 0.6522278748969422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.652\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.053\n",
      "F1 Macro: 0.819 ± 0.098\n",
      "F1 Weighted: 0.892 ± 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:51,277] Trial 15 finished with value: 0.5868005745549648 and parameters: {'booster': 'gblinear', 'lambda': 1.9861036182191637e-07, 'alpha': 1.5363778417735748e-08}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,498] Trial 16 finished with value: 0.5675043225090064 and parameters: {'booster': 'dart', 'lambda': 9.193201467048045e-08, 'alpha': 1.1675521747082847e-08, 'max_depth': 9, 'eta': 0.13753571594079478, 'gamma': 0.0009921462222416356, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.9079710789742155, 'skip_drop': 0.45513656784906026}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,609] Trial 17 finished with value: 0.6119104221152568 and parameters: {'booster': 'gblinear', 'lambda': 2.706796288224791e-05, 'alpha': 2.5731722360039313e-05}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,725] Trial 18 finished with value: 0.5575009739708995 and parameters: {'booster': 'gblinear', 'lambda': 1.2472205789904531e-07, 'alpha': 1.1038914553322574e-07}. Best is trial 13 with value: 0.6522278748969422.\n",
      "[I 2024-11-12 23:57:51,837] Trial 19 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0015864259951147818, 'alpha': 1.67898986538854e-06}. Best is trial 19 with value: 0.6760709742693581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.676\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.828 ± 0.068\n",
      "F1 Weighted: 0.901 ± 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:52,063] Trial 20 finished with value: 0.48162878238196427 and parameters: {'booster': 'dart', 'lambda': 0.0013594253713011379, 'alpha': 1.0427442190553258e-06, 'max_depth': 7, 'eta': 0.008224588783981643, 'gamma': 0.002546554304199964, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 8.656537424825594e-07, 'skip_drop': 1.5164876825299935e-08}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,175] Trial 21 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0008033446327792338, 'alpha': 3.3395862525915787e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,290] Trial 22 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.00096864069113249, 'alpha': 3.8063602619995724e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,420] Trial 23 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0011847480928647528, 'alpha': 7.568955495245484e-05}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,534] Trial 24 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.0008321191051562027, 'alpha': 4.150398302279131e-06}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,645] Trial 25 finished with value: 0.6760709742693581 and parameters: {'booster': 'gblinear', 'lambda': 0.00047924279684625316, 'alpha': 7.922261415903151e-05}. Best is trial 19 with value: 0.6760709742693581.\n",
      "[I 2024-11-12 23:57:52,757] Trial 26 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.006115702581713739, 'alpha': 2.403489779945619e-06}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:52,870] Trial 27 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.007068348306687501, 'alpha': 4.983613495089214e-07}. Best is trial 26 with value: 0.7172503758053912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.717\n",
      "Best model performance:\n",
      "Accuracy: 0.920 ± 0.050\n",
      "F1 Macro: 0.853 ± 0.097\n",
      "F1 Weighted: 0.914 ± 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:53,080] Trial 28 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.8723166811962394, 'alpha': 6.643695005025739e-07, 'max_depth': 7, 'eta': 0.0006642969678843825, 'gamma': 1.141093687755603e-08, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0003347653951284447, 'skip_drop': 5.2559008104433786e-08}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,213] Trial 29 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.04065379049808371, 'alpha': 2.641676480443951e-05}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,349] Trial 30 finished with value: 0.5297617257950828 and parameters: {'booster': 'gbtree', 'lambda': 0.009681052052635123, 'alpha': 2.5948563926624134e-05, 'max_depth': 1, 'eta': 0.609774250373914, 'gamma': 0.004724513014616042, 'grow_policy': 'depthwise'}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,463] Trial 31 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.04696806092777378, 'alpha': 6.424328677092413e-07}. Best is trial 26 with value: 0.7172503758053912.\n",
      "[I 2024-11-12 23:57:53,577] Trial 32 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.09785783345067849, 'alpha': 2.787118060386497e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:53,689] Trial 33 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.3370343582549276, 'alpha': 7.704536972626795e-08}. Best is trial 32 with value: 0.7581635155117249.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.758\n",
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:57:53,807] Trial 34 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.29095253999994614, 'alpha': 5.459898539694326e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:53,920] Trial 35 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.5839108883101183, 'alpha': 2.947158047877257e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,056] Trial 36 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.19041924107375766, 'alpha': 5.650857249421197e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,251] Trial 37 finished with value: 0.5325885489025592 and parameters: {'booster': 'gbtree', 'lambda': 0.1825115817608083, 'alpha': 5.490275888289266e-08, 'max_depth': 6, 'eta': 2.9138188559070594e-08, 'gamma': 3.665815759431605e-08, 'grow_policy': 'lossguide'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,364] Trial 38 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.23237081073936433, 'alpha': 5.200403554206324e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,578] Trial 39 finished with value: 0.5591177623513526 and parameters: {'booster': 'dart', 'lambda': 0.20060425023437314, 'alpha': 2.2221898294984027e-07, 'max_depth': 5, 'eta': 0.0011024198828807463, 'gamma': 0.00017176625380473714, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.00011970591669768955, 'skip_drop': 0.8994962401548576}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,763] Trial 40 finished with value: 0.6122666381403188 and parameters: {'booster': 'gbtree', 'lambda': 0.08924863241922468, 'alpha': 3.959013021333259e-08, 'max_depth': 5, 'eta': 0.06359289930152, 'gamma': 0.035590451980013614, 'grow_policy': 'lossguide'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,875] Trial 41 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.393045881889716, 'alpha': 7.246586679112614e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:54,996] Trial 42 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.11429081190472298, 'alpha': 3.325976396678091e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,110] Trial 43 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.02895913419064581, 'alpha': 2.6466726982642934e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,242] Trial 44 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.33206185593540327, 'alpha': 1.0279611622279887e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,357] Trial 45 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.8084812424635968, 'alpha': 2.488301670965524e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,471] Trial 46 finished with value: 0.612600042864821 and parameters: {'booster': 'gblinear', 'lambda': 0.018410742614269726, 'alpha': 0.04114472206767246}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,587] Trial 47 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.1311768696826609, 'alpha': 3.155217924149402e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,790] Trial 48 finished with value: 0.0013903478254046716 and parameters: {'booster': 'gbtree', 'lambda': 0.003822733448220564, 'alpha': 0.003043334464338783, 'max_depth': 9, 'eta': 1.250294721559774e-08, 'gamma': 0.696011003606078, 'grow_policy': 'depthwise'}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:55,903] Trial 49 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.33706584829752156, 'alpha': 1.0880696612540967e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,079] Trial 50 finished with value: 0.455156407362528 and parameters: {'booster': 'dart', 'lambda': 0.06496017185526001, 'alpha': 2.1804140721146447e-08, 'max_depth': 2, 'eta': 0.00017650468706026422, 'gamma': 0.0004694151091426447, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 1.029915736178884e-08, 'skip_drop': 2.1034039629328684e-05}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,192] Trial 51 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.11254929472527452, 'alpha': 5.765544771367046e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,331] Trial 52 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.02135396069531604, 'alpha': 3.50965584514197e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,449] Trial 53 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.2662621023740773, 'alpha': 1.114150190734843e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,562] Trial 54 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.07386493670993022, 'alpha': 1.768235462229777e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,677] Trial 55 finished with value: 0.7039337563024542 and parameters: {'booster': 'gblinear', 'lambda': 0.5383927785796644, 'alpha': 5.753867875186309e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,789] Trial 56 finished with value: 0.7581635155117249 and parameters: {'booster': 'gblinear', 'lambda': 0.15499852309094572, 'alpha': 4.1219665763946747e-07}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:56,902] Trial 57 finished with value: 0.6219707163863691 and parameters: {'booster': 'gblinear', 'lambda': 1.980988818564679e-05, 'alpha': 1.0987664608778963e-06}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:57,017] Trial 58 finished with value: 0.7172503758053912 and parameters: {'booster': 'gblinear', 'lambda': 0.003515674321615172, 'alpha': 2.1239611426827974e-08}. Best is trial 32 with value: 0.7581635155117249.\n",
      "[I 2024-11-12 23:57:57,130] Trial 59 finished with value: 0.6466999579380653 and parameters: {'booster': 'gblinear', 'lambda': 0.00022066623709189373, 'alpha': 1.329806772968925e-07}. Best is trial 32 with value: 0.7581635155117249.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.933 ± 0.060\n",
      "F1 Macro: 0.877 ± 0.114\n",
      "F1 Weighted: 0.927 ± 0.067\n"
     ]
    }
   ],
   "source": [
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 19:38:31,736] A new study created in memory with name: no-name-fb818b66-fdee-40b1-8f69-a558bed153e5\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:33,026] Trial 0 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00031835665015120484, 'dropout': 0.20654770214911006}. Best is trial 0 with value: 0.5814461133586074.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.581\n",
      "Best model performance:\n",
      "Accuracy: 0.892 ± 0.032\n",
      "F1 Macro: 0.745 ± 0.154\n",
      "F1 Weighted: 0.874 ± 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:34,320] Trial 1 finished with value: 0.6631836987350632 and parameters: {'lr': 0.0003994236638881323, 'dropout': 0.31297149922050155}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.663\n",
      "Best model performance:\n",
      "Accuracy: 0.906 ± 0.032\n",
      "F1 Macro: 0.815 ± 0.062\n",
      "F1 Weighted: 0.898 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:35,641] Trial 2 finished with value: 0.5814461133586074 and parameters: {'lr': 0.0020550157776878125, 'dropout': 0.45506523137093347}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:36,932] Trial 3 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0023832168392858106, 'dropout': 0.44985818791023646}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:38,197] Trial 4 finished with value: 0.5814461133586074 and parameters: {'lr': 0.00029998028591521244, 'dropout': 0.16352985288896082}. Best is trial 1 with value: 0.6631836987350632.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:39,449] Trial 5 finished with value: 0.7101929043348356 and parameters: {'lr': 0.003514759206662055, 'dropout': 0.3875206144282498}. Best is trial 5 with value: 0.7101929043348356.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.710\n",
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:40,733] Trial 6 finished with value: 0.7290661114709509 and parameters: {'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:42,027] Trial 7 finished with value: 0.6614210409097354 and parameters: {'lr': 0.0037000620150167396, 'dropout': 0.3974109441837972}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:43,296] Trial 8 finished with value: 0.6288118975723532 and parameters: {'lr': 0.001620528316220534, 'dropout': 0.1429762051271858}. Best is trial 6 with value: 0.7290661114709509.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-12 19:38:44,620] Trial 9 finished with value: 0.27794757275052406 and parameters: {'lr': 0.0001195833284751336, 'dropout': 0.2314216389186731}. Best is trial 6 with value: 0.7290661114709509.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.5],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.919 ± 0.026\n",
      "F1 Macro: 0.846 ± 0.067\n",
      "F1 Weighted: 0.913 ± 0.038\n",
      "Best hyperparameters:\n",
      "{'lr': 0.007039917108469391, 'dropout': 0.44376947156025015}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"knn\")\n",
    "svm_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"svm\")\n",
    "xgb_eval.save_results(results_file=\"logs/mds_disese_eval.csv\", row_name=\"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.923 ± 0.084\n",
      "F1 Macro: 0.825 ± 0.208\n",
      "F1 Weighted: 0.933 ± 0.072\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n",
      "Best model performance:\n",
      "Accuracy: 0.954 ± 0.038\n",
      "F1 Macro: 0.688 ± 0.255\n",
      "F1 Weighted: 0.932 ± 0.056\n"
     ]
    }
   ],
   "source": [
    "knn_eval.print_best_results()\n",
    "svm_eval.print_best_results()\n",
    "xgb_eval.print_best_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.235535  ┆ 0.200897  ┆ N54       ┆ 0     │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.461213  ┆ 0.061081  ┆ N58       ┆ 0     │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.114601  ┆ 0.226098  ┆ N82       ┆ 0     │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.0       ┆ 0.209648  ┆ N83       ┆ 0     │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.017143  ┆ 0.183437  ┆ N84       ┆ 0     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.390905  ┆ 0.471158  ┆ V777      ┆ 1     │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.733365  ┆ 0.766874  ┆ V806      ┆ 1     │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.093215  ┆ 0.636191  ┆ V839      ┆ 1     │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.630781  ┆ 0.495458  ┆ V883      ┆ 1     │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.637763  ┆ 0.268635  ┆ V888      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘,\n",
       " shape: (15, 202)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ sample_id ┆ class │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ 130821    ┆ 172159    ┆ s         ┆ ---   │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ i64   │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ f64       ┆ str       ┆       │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.403793  ┆ 0.284774  ┆ N60       ┆ 0     │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.417659  ┆ 0.313869  ┆ N70       ┆ 0     │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.117834  ┆ 0.285621  ┆ N85       ┆ 0     │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.374325  ┆ 0.466707  ┆ V108      ┆ 1     │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 0.253248  ┆ 0.812754  ┆ V1297     ┆ 1     │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …     │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.081684  ┆ 0.283214  ┆ V221      ┆ 1     │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.261181  ┆ 0.413476  ┆ V456      ┆ 1     │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.1762    ┆ 0.701686  ┆ V538      ┆ 1     │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.178761  ┆ 1.105654  ┆ V574      ┆ 1     │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.955394  ┆ 0.193979  ┆ V940      ┆ 1     │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────┘)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrna_loader.get_fold(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.190054   ┆ 0.8589     ┆ 0.724728   ┆ 0.796341  ┆ … ┆ 0.770044 ┆ 0.741842 ┆ 0.767158 ┆ 0.87709  │\n",
       " │ 0.131677   ┆ 0.788182   ┆ 0.73847    ┆ 0.566229  ┆ … ┆ 0.628628 ┆ 0.630266 ┆ 0.27247  ┆ 0.548661 │\n",
       " │ 0.94058    ┆ 0.623225   ┆ 0.735662   ┆ 0.41357   ┆ … ┆ 0.417187 ┆ 0.635949 ┆ 0.342004 ┆ 0.649414 │\n",
       " │ 0.805491   ┆ 0.642498   ┆ 0.62087    ┆ 0.43411   ┆ … ┆ 0.791224 ┆ 0.742146 ┆ 0.109209 ┆ 0.684224 │\n",
       " │ 1.0        ┆ 0.789645   ┆ 0.52572    ┆ 0.214973  ┆ … ┆ 0.467998 ┆ 0.54089  ┆ 0.362278 ┆ 0.494964 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.226835   ┆ 0.480819   ┆ 1.0        ┆ 0.766894  ┆ … ┆ 0.942295 ┆ 0.816323 ┆ 0.630765 ┆ 0.78563  │\n",
       " │ 0.123242   ┆ 0.591142   ┆ 0.42922    ┆ 0.684059  ┆ … ┆ 0.574101 ┆ 0.724553 ┆ 0.044501 ┆ 0.687679 │\n",
       " │ 0.559905   ┆ 0.608163   ┆ 0.5361     ┆ 0.619954  ┆ … ┆ 0.987056 ┆ 0.689131 ┆ 0.576118 ┆ 0.892948 │\n",
       " │ 0.138803   ┆ 0.760565   ┆ 0.817056   ┆ 0.712027  ┆ … ┆ 0.594205 ┆ 0.57892  ┆ 0.349641 ┆ 0.600517 │\n",
       " │ 0.219329   ┆ 0.813799   ┆ 0.562113   ┆ 0.573218  ┆ … ┆ 0.541005 ┆ 0.44407  ┆ 0.433872 ┆ 0.336804 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " shape: (15, 800)\n",
       " ┌────────────┬────────────┬────────────┬───────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       " │ ENSG000001 ┆ ENSG000002 ┆ ENSG000001 ┆ ENSG00000 ┆ … ┆ L1M3DE_5 ┆ HERV19I  ┆ LTR40C   ┆ L1MA8    │\n",
       " │ 81826      ┆ 78588      ┆ 20594      ┆ 121797    ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       " │ ---        ┆ ---        ┆ ---        ┆ ---       ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       " │ f64        ┆ f64        ┆ f64        ┆ f64       ┆   ┆          ┆          ┆          ┆          │\n",
       " ╞════════════╪════════════╪════════════╪═══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       " │ 0.20333    ┆ 0.738426   ┆ 0.846229   ┆ 0.688066  ┆ … ┆ 0.34427  ┆ 0.699313 ┆ 0.544814 ┆ 0.635649 │\n",
       " │ 0.191396   ┆ 0.819688   ┆ 0.91776    ┆ 0.82761   ┆ … ┆ 0.296186 ┆ 0.341609 ┆ 0.329612 ┆ 0.375117 │\n",
       " │ 0.949997   ┆ 0.784648   ┆ 0.654279   ┆ 0.11519   ┆ … ┆ 0.591366 ┆ 0.505929 ┆ 0.130364 ┆ 0.352736 │\n",
       " │ 0.047037   ┆ 0.460519   ┆ 0.601395   ┆ 0.754197  ┆ … ┆ 0.765963 ┆ 0.758159 ┆ 0.878573 ┆ 0.923286 │\n",
       " │ 0.560943   ┆ 0.237744   ┆ 0.38922    ┆ 0.4356    ┆ … ┆ 1.060318 ┆ 1.127576 ┆ 0.740619 ┆ 1.126258 │\n",
       " │ …          ┆ …          ┆ …          ┆ …         ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       " │ 0.456982   ┆ 0.455895   ┆ 0.55602    ┆ 0.528333  ┆ … ┆ 0.910938 ┆ 0.703334 ┆ 0.682987 ┆ 0.731746 │\n",
       " │ 0.015711   ┆ 0.132877   ┆ 1.11073    ┆ 0.720927  ┆ … ┆ 0.647633 ┆ 1.039966 ┆ 0.69509  ┆ 1.039513 │\n",
       " │ 0.504371   ┆ 0.511528   ┆ 0.618815   ┆ 0.492415  ┆ … ┆ 0.983345 ┆ 0.899831 ┆ 0.183759 ┆ 0.94264  │\n",
       " │ -0.288737  ┆ 0.580996   ┆ 0.852197   ┆ 0.760822  ┆ … ┆ 0.767064 ┆ 0.753895 ┆ 0.422372 ┆ 0.772614 │\n",
       " │ -0.041422  ┆ 0.525319   ┆ 0.513923   ┆ 0.752942  ┆ … ┆ 0.249167 ┆ 0.635731 ┆ 0.254413 ┆ 0.622748 │\n",
       " └────────────┴────────────┴────────────┴───────────┴───┴──────────┴──────────┴──────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (59, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.190054  ┆ 0.8589    ┆ 0.724728  ┆ 0.796341  ┆ … ┆ 0.792052  ┆ 0.724515  ┆ 0.670447  ┆ 0.877857 │\n",
       " │ 0.131677  ┆ 0.788182  ┆ 0.73847   ┆ 0.566229  ┆ … ┆ 0.570109  ┆ 0.788338  ┆ 0.25199   ┆ 0.445283 │\n",
       " │ 0.94058   ┆ 0.623225  ┆ 0.735662  ┆ 0.41357   ┆ … ┆ 0.39374   ┆ 0.692776  ┆ 0.478255  ┆ 0.630045 │\n",
       " │ 0.805491  ┆ 0.642498  ┆ 0.62087   ┆ 0.43411   ┆ … ┆ 0.656948  ┆ 0.800219  ┆ 0.68736   ┆ 0.821026 │\n",
       " │ 1.0       ┆ 0.789645  ┆ 0.52572   ┆ 0.214973  ┆ … ┆ 0.666039  ┆ 0.897311  ┆ 0.771568  ┆ 0.806792 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.226835  ┆ 0.480819  ┆ 1.0       ┆ 0.766894  ┆ … ┆ 0.52434   ┆ 0.73862   ┆ 0.457018  ┆ 0.690925 │\n",
       " │ 0.123242  ┆ 0.591142  ┆ 0.42922   ┆ 0.684059  ┆ … ┆ 0.802056  ┆ 0.921184  ┆ 0.314539  ┆ 0.94341  │\n",
       " │ 0.559905  ┆ 0.608163  ┆ 0.5361    ┆ 0.619954  ┆ … ┆ 0.674672  ┆ 0.955206  ┆ 0.787847  ┆ 0.672842 │\n",
       " │ 0.138803  ┆ 0.760565  ┆ 0.817056  ┆ 0.712027  ┆ … ┆ 0.848153  ┆ 0.876225  ┆ 0.836959  ┆ 1.0      │\n",
       " │ 0.219329  ┆ 0.813799  ┆ 0.562113  ┆ 0.573218  ┆ … ┆ 0.574968  ┆ 0.769111  ┆ 0.359642  ┆ 0.478356 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " shape: (15, 400)\n",
       " ┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       " │ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ … ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG00000 ┆ ENSG0000 │\n",
       " │ 181826    ┆ 278588    ┆ 120594    ┆ 121797    ┆   ┆ 276404    ┆ 207820    ┆ 252695    ┆ 0263831  │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       " │ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       " │ 0.20333   ┆ 0.738426  ┆ 0.846229  ┆ 0.688066  ┆ … ┆ 0.359969  ┆ 0.746973  ┆ 0.238121  ┆ 0.522262 │\n",
       " │ 0.191396  ┆ 0.819688  ┆ 0.91776   ┆ 0.82761   ┆ … ┆ 0.460708  ┆ 0.782432  ┆ 0.363974  ┆ 0.661165 │\n",
       " │ 0.949997  ┆ 0.784648  ┆ 0.654279  ┆ 0.11519   ┆ … ┆ 0.471986  ┆ 0.407116  ┆ 0.389313  ┆ 0.551279 │\n",
       " │ 0.047037  ┆ 0.460519  ┆ 0.601395  ┆ 0.754197  ┆ … ┆ 0.749248  ┆ 0.706021  ┆ 0.465903  ┆ 0.671848 │\n",
       " │ 0.560943  ┆ 0.237744  ┆ 0.38922   ┆ 0.4356    ┆ … ┆ 0.834129  ┆ 0.890805  ┆ 0.737512  ┆ 0.667177 │\n",
       " │ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       " │ 0.456982  ┆ 0.455895  ┆ 0.55602   ┆ 0.528333  ┆ … ┆ 0.935791  ┆ 1.077982  ┆ 0.708828  ┆ 1.100682 │\n",
       " │ 0.015711  ┆ 0.132877  ┆ 1.11073   ┆ 0.720927  ┆ … ┆ 0.554755  ┆ 0.955612  ┆ 0.010688  ┆ 0.735628 │\n",
       " │ 0.504371  ┆ 0.511528  ┆ 0.618815  ┆ 0.492415  ┆ … ┆ 0.49578   ┆ 0.901123  ┆ 0.400956  ┆ 0.652603 │\n",
       " │ -0.288737 ┆ 0.580996  ┆ 0.852197  ┆ 0.760822  ┆ … ┆ 0.773837  ┆ 0.856068  ┆ 0.776993  ┆ 0.666524 │\n",
       " │ -0.041422 ┆ 0.525319  ┆ 0.513923  ┆ 0.752942  ┆ … ┆ 0.878347  ┆ 0.836865  ┆ 0.52754   ┆ 0.922727 │\n",
       " └───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘,\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catodm = CatOmicDataManager(\n",
    "    omic_data_loaders={\n",
    "        \"mrna\": mrna_loader,\n",
    "        # \"meth\": meth_loader,\n",
    "        \"mirna\": mirna_loader,\n",
    "    },\n",
    "    n_splits=5,\n",
    ")\n",
    "\n",
    "catodm.get_split(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 23:58:27,453] A new study created in memory with name: no-name-88f2a81c-17ae-464a-9309-9250ee9deaed\n",
      "[I 2024-11-12 23:58:31,864] Trial 0 finished with value: 0.8225991965179064 and parameters: {}. Best is trial 0 with value: 0.8225991965179064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.823\n",
      "Best model performance:\n",
      "Accuracy: 0.947 ± 0.050\n",
      "F1 Macro: 0.918 ± 0.070\n",
      "F1 Weighted: 0.947 ± 0.047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': np.float64(0.9466666666666667),\n",
       " 'f1_macro': np.float64(0.917909090909091),\n",
       " 'f1_weighted': np.float64(0.9466545454545454),\n",
       " 'acc_std': np.float64(0.04988876515698587),\n",
       " 'f1_macro_std': np.float64(0.06951282657072173),\n",
       " 'f1_weighted_std': np.float64(0.047331376205998074)}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders={\n",
    "            \"mrna\": mrna_loader,\n",
    "            \"mirna\": mirna_loader,\n",
    "        },\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"attention\",\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 100,\n",
    "        \"log_interval\": 101,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
