{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_risk/mrna_mirna_circrna_te.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_risk/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-12-08 14:23:27,114] A new study created in memory with name: no-name-22380468-d6e7-49ec-ae99-4e46d9857a5d\n",
      "[I 2024-12-08 14:23:27,289] Trial 0 finished with value: 0.1489648158535142 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.1489648158535142.\n",
      "[I 2024-12-08 14:23:27,440] Trial 1 finished with value: 0.2631199598914357 and parameters: {'n_neighbors': 13}. Best is trial 1 with value: 0.2631199598914357.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.149\n",
      "Best model performance:\n",
      "Accuracy: 0.584 ± 0.053\n",
      "F1 Macro: 0.496 ± 0.121\n",
      "F1 Weighted: 0.515 ± 0.112\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6071428571428572), 'f1_weighted': np.float64(0.6168831168831169)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5376845376845377)}, {'acc': 0.6, 'f1_macro': np.float64(0.375), 'f1_weighted': np.float64(0.45)}, {'acc': 0.5, 'f1_macro': np.float64(0.3333333333333333), 'f1_weighted': np.float64(0.3333333333333333)}]\n",
      "New best score: 0.263\n",
      "Best model performance:\n",
      "Accuracy: 0.647 ± 0.181\n",
      "F1 Macro: 0.637 ± 0.187\n",
      "F1 Weighted: 0.638 ± 0.191\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5376845376845377)}, {'acc': 0.36363636363636365, 'f1_macro': np.float64(0.3418803418803419), 'f1_weighted': np.float64(0.33100233100233106)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.696969696969697), 'f1_weighted': np.float64(0.696969696969697)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:27,590] Trial 2 finished with value: 0.1784699439018282 and parameters: {'n_neighbors': 1}. Best is trial 1 with value: 0.2631199598914357.\n",
      "[I 2024-12-08 14:23:27,765] Trial 3 finished with value: 0.27761979806498105 and parameters: {'n_neighbors': 17}. Best is trial 3 with value: 0.27761979806498105.\n",
      "[I 2024-12-08 14:23:27,913] Trial 4 finished with value: 0.27673149849051043 and parameters: {'n_neighbors': 10}. Best is trial 3 with value: 0.27761979806498105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.278\n",
      "Best model performance:\n",
      "Accuracy: 0.664 ± 0.117\n",
      "F1 Macro: 0.646 ± 0.125\n",
      "F1 Weighted: 0.648 ± 0.134\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.45454545454545453, 'f1_macro': np.float64(0.4107142857142857), 'f1_weighted': np.float64(0.39610389610389607)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.696969696969697), 'f1_weighted': np.float64(0.696969696969697)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:28,060] Trial 5 finished with value: 0.2631199598914357 and parameters: {'n_neighbors': 13}. Best is trial 3 with value: 0.27761979806498105.\n",
      "[I 2024-12-08 14:23:28,213] Trial 6 finished with value: 0.26062065170661974 and parameters: {'n_neighbors': 18}. Best is trial 3 with value: 0.27761979806498105.\n",
      "[I 2024-12-08 14:23:28,363] Trial 7 finished with value: 0.30594317401183274 and parameters: {'n_neighbors': 3}. Best is trial 7 with value: 0.30594317401183274.\n",
      "[I 2024-12-08 14:23:28,515] Trial 8 finished with value: 0.2297615558373419 and parameters: {'n_neighbors': 14}. Best is trial 7 with value: 0.30594317401183274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.306\n",
      "Best model performance:\n",
      "Accuracy: 0.682 ± 0.086\n",
      "F1 Macro: 0.668 ± 0.076\n",
      "F1 Weighted: 0.672 ± 0.080\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5454545454545454), 'f1_weighted': np.float64(0.5454545454545454)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:28,663] Trial 9 finished with value: 0.2243592972482265 and parameters: {'n_neighbors': 12}. Best is trial 7 with value: 0.30594317401183274.\n",
      "[I 2024-12-08 14:23:28,811] Trial 10 finished with value: 0.25712933228654355 and parameters: {'n_neighbors': 6}. Best is trial 7 with value: 0.30594317401183274.\n",
      "[I 2024-12-08 14:23:28,960] Trial 11 finished with value: 0.3454232445389504 and parameters: {'n_neighbors': 20}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:29,110] Trial 12 finished with value: 0.33724377229164443 and parameters: {'n_neighbors': 7}. Best is trial 11 with value: 0.3454232445389504.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.345\n",
      "Best model performance:\n",
      "Accuracy: 0.704 ± 0.152\n",
      "F1 Macro: 0.700 ± 0.151\n",
      "F1 Weighted: 0.701 ± 0.154\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.45454545454545453, 'f1_macro': np.float64(0.45), 'f1_weighted': np.float64(0.4454545454545455)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:29,276] Trial 13 finished with value: 0.2725434934796483 and parameters: {'n_neighbors': 8}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:29,449] Trial 14 finished with value: 0.3454232445389504 and parameters: {'n_neighbors': 20}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:29,600] Trial 15 finished with value: 0.3454232445389504 and parameters: {'n_neighbors': 20}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:29,754] Trial 16 finished with value: 0.3454232445389504 and parameters: {'n_neighbors': 20}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:29,910] Trial 17 finished with value: 0.27761979806498105 and parameters: {'n_neighbors': 17}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:30,066] Trial 18 finished with value: 0.25999989956398933 and parameters: {'n_neighbors': 15}. Best is trial 11 with value: 0.3454232445389504.\n",
      "[I 2024-12-08 14:23:30,215] Trial 19 finished with value: 0.2297615558373419 and parameters: {'n_neighbors': 16}. Best is trial 11 with value: 0.3454232445389504.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:30,239] A new study created in memory with name: no-name-0273cf7f-393a-4dbe-891a-c6723008dac3\n",
      "[I 2024-12-08 14:23:30,443] Trial 0 finished with value: 0.3079454857016296 and parameters: {'C': 0.04043896525427254, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:30,622] Trial 1 finished with value: 0.2736963980298857 and parameters: {'C': 5.040436237172311, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.308\n",
      "Best model performance:\n",
      "Accuracy: 0.684 ± 0.101\n",
      "F1 Macro: 0.670 ± 0.095\n",
      "F1 Weighted: 0.672 ± 0.102\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:30,791] Trial 2 finished with value: 0.2736963980298857 and parameters: {'C': 0.574511404393049, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:30,959] Trial 3 finished with value: 0.2736963980298857 and parameters: {'C': 2.014838488402247, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,144] Trial 4 finished with value: 0.2736963980298857 and parameters: {'C': 7.820792777698033, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,312] Trial 5 finished with value: 0.2736963980298857 and parameters: {'C': 0.14511806512871692, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,489] Trial 6 finished with value: 0.2736963980298857 and parameters: {'C': 0.5521233551088864, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,657] Trial 7 finished with value: 0.2736963980298857 and parameters: {'C': 1.9946041641448853, 'class_weight': None}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,809] Trial 8 finished with value: 0.282303085828288 and parameters: {'C': 0.05719894104549766, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.3079454857016296.\n",
      "[I 2024-12-08 14:23:31,949] Trial 9 finished with value: 0.33450122790480763 and parameters: {'C': 0.023051966343890815, 'class_weight': None}. Best is trial 9 with value: 0.33450122790480763.\n",
      "[I 2024-12-08 14:23:32,084] Trial 10 finished with value: 0.33450122790480763 and parameters: {'C': 0.011660911872902694, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.33450122790480763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.335\n",
      "Best model performance:\n",
      "Accuracy: 0.702 ± 0.117\n",
      "F1 Macro: 0.690 ± 0.119\n",
      "F1 Weighted: 0.691 ± 0.123\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:32,225] Trial 11 finished with value: 0.33450122790480763 and parameters: {'C': 0.010679356518277073, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.33450122790480763.\n",
      "[I 2024-12-08 14:23:32,362] Trial 12 finished with value: 0.33450122790480763 and parameters: {'C': 0.01032057519571862, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.33450122790480763.\n",
      "[I 2024-12-08 14:23:32,520] Trial 13 finished with value: 0.340415895850237 and parameters: {'C': 0.03395356753532771, 'class_weight': 'balanced'}. Best is trial 13 with value: 0.340415895850237.\n",
      "[I 2024-12-08 14:23:32,687] Trial 14 finished with value: 0.3079454857016296 and parameters: {'C': 0.049117640022859184, 'class_weight': None}. Best is trial 13 with value: 0.340415895850237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.340\n",
      "Best model performance:\n",
      "Accuracy: 0.704 ± 0.128\n",
      "F1 Macro: 0.696 ± 0.128\n",
      "F1 Weighted: 0.695 ± 0.132\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:32,847] Trial 15 finished with value: 0.24895242453069985 and parameters: {'C': 0.1405444632775134, 'class_weight': 'balanced'}. Best is trial 13 with value: 0.340415895850237.\n",
      "[I 2024-12-08 14:23:33,000] Trial 16 finished with value: 0.36864492846001523 and parameters: {'C': 0.02603121521958259, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:33,158] Trial 17 finished with value: 0.282303085828288 and parameters: {'C': 0.10959705771103749, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.369\n",
      "Best model performance:\n",
      "Accuracy: 0.722 ± 0.124\n",
      "F1 Macro: 0.714 ± 0.125\n",
      "F1 Weighted: 0.715 ± 0.128\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:33,314] Trial 18 finished with value: 0.36864492846001523 and parameters: {'C': 0.025893587418638158, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:33,480] Trial 19 finished with value: 0.2736963980298857 and parameters: {'C': 0.2545315091428662, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:33,640] Trial 20 finished with value: 0.282303085828288 and parameters: {'C': 0.08010724380335828, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:33,786] Trial 21 finished with value: 0.36864492846001523 and parameters: {'C': 0.02217742350025568, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:33,933] Trial 22 finished with value: 0.36864492846001523 and parameters: {'C': 0.019390280164933504, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,083] Trial 23 finished with value: 0.36864492846001523 and parameters: {'C': 0.021003182110520897, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,231] Trial 24 finished with value: 0.36864492846001523 and parameters: {'C': 0.026949790774983974, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,396] Trial 25 finished with value: 0.2736963980298857 and parameters: {'C': 0.24742255112930037, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,576] Trial 26 finished with value: 0.282303085828288 and parameters: {'C': 0.07275676465877647, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,716] Trial 27 finished with value: 0.33450122790480763 and parameters: {'C': 0.014117437192302448, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:34,864] Trial 28 finished with value: 0.340415895850237 and parameters: {'C': 0.035085779012923896, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,020] Trial 29 finished with value: 0.282303085828288 and parameters: {'C': 0.04641984501142506, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,159] Trial 30 finished with value: 0.36864492846001523 and parameters: {'C': 0.018631686268064367, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,317] Trial 31 finished with value: 0.36864492846001523 and parameters: {'C': 0.018757912217426943, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,513] Trial 32 finished with value: 0.36864492846001523 and parameters: {'C': 0.02810647202352672, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,656] Trial 33 finished with value: 0.36864492846001523 and parameters: {'C': 0.01587634686281333, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,808] Trial 34 finished with value: 0.3129736242864627 and parameters: {'C': 0.04072894041154189, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:35,968] Trial 35 finished with value: 0.282303085828288 and parameters: {'C': 0.08374106472668312, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:36,148] Trial 36 finished with value: 0.2736963980298857 and parameters: {'C': 1.3509197374070907, 'class_weight': None}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:36,326] Trial 37 finished with value: 0.2736963980298857 and parameters: {'C': 0.41396291042233513, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:36,500] Trial 38 finished with value: 0.2736963980298857 and parameters: {'C': 0.182129306988896, 'class_weight': None}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:36,681] Trial 39 finished with value: 0.2736963980298857 and parameters: {'C': 1.1978725339008842, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:36,871] Trial 40 finished with value: 0.2736963980298857 and parameters: {'C': 5.243663495127243, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,019] Trial 41 finished with value: 0.36864492846001523 and parameters: {'C': 0.02077061861753985, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,174] Trial 42 finished with value: 0.36864492846001523 and parameters: {'C': 0.029367226764196033, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,318] Trial 43 finished with value: 0.36864492846001523 and parameters: {'C': 0.015583847893428497, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,474] Trial 44 finished with value: 0.282303085828288 and parameters: {'C': 0.05500581807683385, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,617] Trial 45 finished with value: 0.33450122790480763 and parameters: {'C': 0.023133633803897734, 'class_weight': None}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,753] Trial 46 finished with value: 0.33450122790480763 and parameters: {'C': 0.012890235342740126, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:37,907] Trial 47 finished with value: 0.282303085828288 and parameters: {'C': 0.06022764706759005, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:38,066] Trial 48 finished with value: 0.2736963980298857 and parameters: {'C': 0.10430054776777016, 'class_weight': None}. Best is trial 16 with value: 0.36864492846001523.\n",
      "[I 2024-12-08 14:23:38,216] Trial 49 finished with value: 0.340415895850237 and parameters: {'C': 0.0392373691982082, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.36864492846001523.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:38,244] A new study created in memory with name: no-name-5980eb2d-8db2-47dd-a767-30fe15ad5427\n",
      "[I 2024-12-08 14:23:38,529] Trial 0 finished with value: 0.2523998390074793 and parameters: {'booster': 'gbtree', 'lambda': 1.787796554210756e-08, 'alpha': 1.5669476991936122e-07, 'max_depth': 1, 'eta': 0.0001256276065307056, 'gamma': 4.748876822166894e-08, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.2523998390074793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.252\n",
      "Best model performance:\n",
      "Accuracy: 0.642 ± 0.032\n",
      "F1 Macro: 0.625 ± 0.029\n",
      "F1 Weighted: 0.629 ± 0.037\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6071428571428572), 'f1_weighted': np.float64(0.5974025974025974)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6901098901098901)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.5833333333333333)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:38,793] Trial 1 finished with value: 0.3848078301355351 and parameters: {'booster': 'gblinear', 'lambda': 0.00017080461061764587, 'alpha': 0.0006315489859677015}. Best is trial 1 with value: 0.3848078301355351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.385\n",
      "Best model performance:\n",
      "Accuracy: 0.736 ± 0.105\n",
      "F1 Macro: 0.722 ± 0.105\n",
      "F1 Weighted: 0.724 ± 0.108\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:39,183] Trial 2 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 0.275884541241094, 'alpha': 0.0004103906948738958, 'max_depth': 5, 'eta': 4.192753064407911e-07, 'gamma': 0.0014709912206730198, 'grow_policy': 'lossguide'}. Best is trial 1 with value: 0.3848078301355351.\n",
      "[I 2024-12-08 14:23:39,382] Trial 3 finished with value: 0.3024066173284228 and parameters: {'booster': 'gblinear', 'lambda': 0.047275771665989716, 'alpha': 3.988074153576865e-06}. Best is trial 1 with value: 0.3848078301355351.\n",
      "[I 2024-12-08 14:23:39,745] Trial 4 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 7.852302200309452e-05, 'alpha': 0.005900315342066362, 'max_depth': 6, 'eta': 4.894206204126081e-05, 'gamma': 6.269086041679486e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.0010755368520089276, 'skip_drop': 0.865532014401339}. Best is trial 1 with value: 0.3848078301355351.\n",
      "[I 2024-12-08 14:23:40,177] Trial 5 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 5.2765197971865843e-05, 'alpha': 1.163367501161678e-08, 'max_depth': 8, 'eta': 0.0014621789836524542, 'gamma': 3.3719203108626186e-08, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.3480746159292089, 'skip_drop': 4.507579667021128e-06}. Best is trial 1 with value: 0.3848078301355351.\n",
      "[I 2024-12-08 14:23:40,398] Trial 6 finished with value: 0.3933985308620883 and parameters: {'booster': 'gblinear', 'lambda': 0.12946538585000017, 'alpha': 0.007946136358742981}. Best is trial 6 with value: 0.3933985308620883.\n",
      "[I 2024-12-08 14:23:40,568] Trial 7 finished with value: 0.44759261053984506 and parameters: {'booster': 'gblinear', 'lambda': 0.00020283251885010514, 'alpha': 0.02547680414853258}. Best is trial 7 with value: 0.44759261053984506.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.393\n",
      "Best model performance:\n",
      "Accuracy: 0.738 ± 0.087\n",
      "F1 Macro: 0.730 ± 0.088\n",
      "F1 Weighted: 0.730 ± 0.091\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n",
      "New best score: 0.448\n",
      "Best model performance:\n",
      "Accuracy: 0.773 ± 0.076\n",
      "F1 Macro: 0.759 ± 0.081\n",
      "F1 Weighted: 0.763 ± 0.081\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:40,859] Trial 8 finished with value: 0.3305099711477334 and parameters: {'booster': 'gblinear', 'lambda': 0.002593935856426547, 'alpha': 9.898551955159433e-06}. Best is trial 7 with value: 0.44759261053984506.\n",
      "[I 2024-12-08 14:23:41,210] Trial 9 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 7.371047043706629e-08, 'alpha': 0.009323589909557747, 'max_depth': 5, 'eta': 2.293586264248889e-05, 'gamma': 0.0004770227197490919, 'grow_policy': 'depthwise'}. Best is trial 7 with value: 0.44759261053984506.\n",
      "[I 2024-12-08 14:23:41,422] Trial 10 finished with value: 0.07498688884758245 and parameters: {'booster': 'gblinear', 'lambda': 9.671574366463863e-07, 'alpha': 0.5059826861273617}. Best is trial 7 with value: 0.44759261053984506.\n",
      "[I 2024-12-08 14:23:41,612] Trial 11 finished with value: 0.07498688884758245 and parameters: {'booster': 'gblinear', 'lambda': 0.6392443022675349, 'alpha': 0.6811692665726407}. Best is trial 7 with value: 0.44759261053984506.\n",
      "[I 2024-12-08 14:23:41,792] Trial 12 finished with value: 0.45335432573662515 and parameters: {'booster': 'gblinear', 'lambda': 0.006597649669844469, 'alpha': 0.03070541369787911}. Best is trial 12 with value: 0.45335432573662515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.453\n",
      "Best model performance:\n",
      "Accuracy: 0.773 ± 0.076\n",
      "F1 Macro: 0.765 ± 0.082\n",
      "F1 Weighted: 0.767 ± 0.082\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.8)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:42,087] Trial 13 finished with value: 0.3815258176026134 and parameters: {'booster': 'gblinear', 'lambda': 0.0036614149092414165, 'alpha': 0.07398262363414136}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:42,408] Trial 14 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 0.002665016700103263, 'alpha': 0.054131099955047585, 'max_depth': 2, 'eta': 0.14785724129923306, 'gamma': 0.8809208014105002, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.6503029178614795e-08, 'skip_drop': 1.276793698635265e-07}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:42,616] Trial 15 finished with value: 0.3565112016188938 and parameters: {'booster': 'gblinear', 'lambda': 6.131723732940182e-06, 'alpha': 4.0420212326480565e-05}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:42,826] Trial 16 finished with value: 0.3575850210960192 and parameters: {'booster': 'gblinear', 'lambda': 0.011707268224397514, 'alpha': 0.0007025971682994684}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:43,018] Trial 17 finished with value: 0.3815258176026134 and parameters: {'booster': 'gblinear', 'lambda': 0.0004159804999247489, 'alpha': 0.08286746893295327}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:43,345] Trial 18 finished with value: 0.07498688884758245 and parameters: {'booster': 'gbtree', 'lambda': 1.1145331601645583e-05, 'alpha': 0.0029730013421439953, 'max_depth': 8, 'eta': 1.0184175236323827e-08, 'gamma': 0.19077882790704115, 'grow_policy': 'depthwise'}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:43,600] Trial 19 finished with value: 0.3013298819924665 and parameters: {'booster': 'dart', 'lambda': 0.0004924036494530268, 'alpha': 8.704270220060722e-05, 'max_depth': 3, 'eta': 0.9935895355927489, 'gamma': 6.747019499556274e-06, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 1.1977734136995268e-08, 'skip_drop': 0.023959966238475468}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:43,777] Trial 20 finished with value: 0.3464285732685259 and parameters: {'booster': 'gblinear', 'lambda': 0.02428979684057092, 'alpha': 0.08431121815797232}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:43,983] Trial 21 finished with value: 0.3933985308620883 and parameters: {'booster': 'gblinear', 'lambda': 0.1803867695670995, 'alpha': 0.011582806254075846}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:44,175] Trial 22 finished with value: 0.3575850210960192 and parameters: {'booster': 'gblinear', 'lambda': 0.044410812805512886, 'alpha': 0.015676211168159464}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:44,429] Trial 23 finished with value: 0.3917914266036992 and parameters: {'booster': 'gblinear', 'lambda': 0.001256633109394148, 'alpha': 0.0018268467145119102}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:44,616] Trial 24 finished with value: 0.25173527371090454 and parameters: {'booster': 'gblinear', 'lambda': 0.11663060642819788, 'alpha': 0.24389282776638932}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:44,842] Trial 25 finished with value: 0.44759261053984506 and parameters: {'booster': 'gblinear', 'lambda': 0.011381031916544732, 'alpha': 0.028517252579091164}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:45,062] Trial 26 finished with value: 0.44759261053984506 and parameters: {'booster': 'gblinear', 'lambda': 0.012878459502919739, 'alpha': 0.030124114263036142}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:45,433] Trial 27 finished with value: 0.21534604040404037 and parameters: {'booster': 'dart', 'lambda': 1.9318419860932968e-05, 'alpha': 0.2288018156195402, 'max_depth': 9, 'eta': 0.008985471041384658, 'gamma': 0.017892423017951586, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 2.3993101290079103e-05, 'skip_drop': 0.00048569255509244534}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:45,781] Trial 28 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 0.0005443006614018623, 'alpha': 0.0016695778498972986, 'max_depth': 3, 'eta': 5.547318856185757e-07, 'gamma': 2.101849295895865e-05, 'grow_policy': 'lossguide'}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:46,021] Trial 29 finished with value: 0.3626351287947816 and parameters: {'booster': 'gblinear', 'lambda': 2.4382884692540874e-06, 'alpha': 9.157210076752506e-07}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:46,434] Trial 30 finished with value: 0.07498688884758245 and parameters: {'booster': 'gbtree', 'lambda': 0.007176140435331902, 'alpha': 0.00023489307932755032, 'max_depth': 6, 'eta': 1.160007443585855e-08, 'gamma': 0.008982049683013683, 'grow_policy': 'depthwise'}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:46,608] Trial 31 finished with value: 0.44759261053984506 and parameters: {'booster': 'gblinear', 'lambda': 0.013623987830349826, 'alpha': 0.029277059472137948}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:46,790] Trial 32 finished with value: 0.41535727108964055 and parameters: {'booster': 'gblinear', 'lambda': 0.001209334171363717, 'alpha': 0.019893670466005087}. Best is trial 12 with value: 0.45335432573662515.\n",
      "[I 2024-12-08 14:23:47,010] Trial 33 finished with value: 0.45492440897231284 and parameters: {'booster': 'gblinear', 'lambda': 0.0001795641562171208, 'alpha': 0.0026768584197248927}. Best is trial 33 with value: 0.45492440897231284.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.455\n",
      "Best model performance:\n",
      "Accuracy: 0.775 ± 0.110\n",
      "F1 Macro: 0.766 ± 0.113\n",
      "F1 Weighted: 0.767 ± 0.115\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:47,257] Trial 34 finished with value: 0.41541020440321136 and parameters: {'booster': 'gblinear', 'lambda': 0.00010131342013193048, 'alpha': 0.0028664032717327023}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:47,499] Trial 35 finished with value: 0.3562051632128554 and parameters: {'booster': 'gblinear', 'lambda': 0.00024258726892130184, 'alpha': 0.0005546735850844923}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:47,769] Trial 36 finished with value: 0.3075229188952905 and parameters: {'booster': 'gblinear', 'lambda': 3.9399422254524904e-05, 'alpha': 0.2502650600561481}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:48,034] Trial 37 finished with value: 0.41541020440321136 and parameters: {'booster': 'gblinear', 'lambda': 4.6778208042398377e-07, 'alpha': 0.0061826086718971655}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:48,371] Trial 38 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 0.00017434578991264103, 'alpha': 0.0011425460905455545, 'max_depth': 4, 'eta': 9.960272227366825e-07, 'gamma': 1.1188081733652729e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.6448263084870132, 'skip_drop': 1.9548321915066055e-08}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:48,604] Trial 39 finished with value: 0.2523998390074793 and parameters: {'booster': 'gbtree', 'lambda': 0.0011144410946273622, 'alpha': 0.00018625153878438336, 'max_depth': 1, 'eta': 0.011098219861014851, 'gamma': 7.462515785261394e-05, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:48,884] Trial 40 finished with value: 0.28066162560332936 and parameters: {'booster': 'gblinear', 'lambda': 0.8485821712891365, 'alpha': 2.4987379155463132e-08}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:49,186] Trial 41 finished with value: 0.4107768252204106 and parameters: {'booster': 'gblinear', 'lambda': 0.04169832415350921, 'alpha': 0.04644293387416521}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:49,406] Trial 42 finished with value: 0.3917914266036992 and parameters: {'booster': 'gblinear', 'lambda': 0.00500215299309976, 'alpha': 0.004279875952129539}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:49,619] Trial 43 finished with value: 0.3734019041622159 and parameters: {'booster': 'gblinear', 'lambda': 0.018950329112726035, 'alpha': 0.13226164293655396}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:49,822] Trial 44 finished with value: 0.44629065572976356 and parameters: {'booster': 'gblinear', 'lambda': 1.2535078933072317e-08, 'alpha': 0.01798890515579538}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:50,037] Trial 45 finished with value: 0.07498688884758245 and parameters: {'booster': 'gblinear', 'lambda': 3.152908126209056e-05, 'alpha': 0.890508007064418}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:50,301] Trial 46 finished with value: 0.3933985308620883 and parameters: {'booster': 'gblinear', 'lambda': 0.09203985180836126, 'alpha': 0.007883332578989814}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:50,532] Trial 47 finished with value: 0.45335432573662515 and parameters: {'booster': 'gblinear', 'lambda': 0.0018178175396253553, 'alpha': 0.03353573286567184}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:50,870] Trial 48 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 8.543296892438267e-05, 'alpha': 0.12504861596371095, 'max_depth': 9, 'eta': 4.019085203117492e-06, 'gamma': 1.3433958792332714e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 9.41251725173581e-06, 'skip_drop': 8.357380710932232e-05}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:51,065] Trial 49 finished with value: 0.3050129034313449 and parameters: {'booster': 'gblinear', 'lambda': 0.0020441899508005187, 'alpha': 1.1567399689083906e-05}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:51,437] Trial 50 finished with value: 0.21534604040404037 and parameters: {'booster': 'gbtree', 'lambda': 0.0006938544338957492, 'alpha': 0.5163339002646257, 'max_depth': 7, 'eta': 0.0006024820354907007, 'gamma': 0.036582866515201894, 'grow_policy': 'lossguide'}. Best is trial 33 with value: 0.45492440897231284.\n",
      "[I 2024-12-08 14:23:51,632] Trial 51 finished with value: 0.48908878329371996 and parameters: {'booster': 'gblinear', 'lambda': 0.007738786046325792, 'alpha': 0.022052039394924505}. Best is trial 51 with value: 0.48908878329371996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.489\n",
      "Best model performance:\n",
      "Accuracy: 0.793 ± 0.092\n",
      "F1 Macro: 0.785 ± 0.096\n",
      "F1 Weighted: 0.786 ± 0.098\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:51,838] Trial 52 finished with value: 0.48908878329371996 and parameters: {'booster': 'gblinear', 'lambda': 0.006277096687942411, 'alpha': 0.04078986741166531}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:52,039] Trial 53 finished with value: 0.4214075724280765 and parameters: {'booster': 'gblinear', 'lambda': 0.004205885265738726, 'alpha': 0.04897463689357716}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:52,276] Trial 54 finished with value: 0.48908878329371996 and parameters: {'booster': 'gblinear', 'lambda': 0.0022674617732888346, 'alpha': 0.010882531641958567}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:52,490] Trial 55 finished with value: 0.44759261053984506 and parameters: {'booster': 'gblinear', 'lambda': 0.002496405853817425, 'alpha': 0.009939386438943956}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:52,706] Trial 56 finished with value: 0.363770052622702 and parameters: {'booster': 'gblinear', 'lambda': 0.005491433248459564, 'alpha': 0.0010130879965618992}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:52,924] Trial 57 finished with value: 0.3645229022486005 and parameters: {'booster': 'gblinear', 'lambda': 0.34396300992081424, 'alpha': 0.003854408990561067}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:53,103] Trial 58 finished with value: 0.3734019041622159 and parameters: {'booster': 'gblinear', 'lambda': 0.03189224750743207, 'alpha': 0.13366821252073405}. Best is trial 51 with value: 0.48908878329371996.\n",
      "[I 2024-12-08 14:23:53,482] Trial 59 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 0.0018314138575610008, 'alpha': 0.01347113820525802, 'max_depth': 3, 'eta': 1.0496079768259403e-07, 'gamma': 0.0007764499778931368, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.003650270235565476, 'skip_drop': 0.0069633934055005845}. Best is trial 51 with value: 0.48908878329371996.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.793 ± 0.092\n",
      "F1 Macro: 0.785 ± 0.096\n",
      "F1 Weighted: 0.786 ± 0.098\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:23:55,618] A new study created in memory with name: no-name-194542d0-a0a1-4b04-bc1c-7f13a21dc36a\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:23:57,188] Trial 0 finished with value: 0.282303085828288 and parameters: {'lr': 0.0027695208160206258, 'dropout': 0.10153956806916167}. Best is trial 0 with value: 0.282303085828288.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.282\n",
      "Best model performance:\n",
      "Accuracy: 0.664 ± 0.084\n",
      "F1 Macro: 0.651 ± 0.077\n",
      "F1 Weighted: 0.653 ± 0.085\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.696969696969697), 'f1_weighted': np.float64(0.696969696969697)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:23:58,806] Trial 1 finished with value: 0.3616579244897626 and parameters: {'lr': 0.0006830292732341623, 'dropout': 0.31324067855895477}. Best is trial 1 with value: 0.3616579244897626.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.362\n",
      "Best model performance:\n",
      "Accuracy: 0.720 ± 0.093\n",
      "F1 Macro: 0.708 ± 0.092\n",
      "F1 Weighted: 0.710 ± 0.098\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:00,336] Trial 2 finished with value: 0.2799947488214131 and parameters: {'lr': 0.0019465836121142707, 'dropout': 0.51995450929341}. Best is trial 1 with value: 0.3616579244897626.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:01,888] Trial 3 finished with value: 0.33307077092811355 and parameters: {'lr': 0.009267994936540151, 'dropout': 0.36544423756812505}. Best is trial 1 with value: 0.3616579244897626.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:03,505] Trial 4 finished with value: 0.3933985308620883 and parameters: {'lr': 0.0002645149001288009, 'dropout': 0.39579389129506637}. Best is trial 4 with value: 0.3933985308620883.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.393\n",
      "Best model performance:\n",
      "Accuracy: 0.738 ± 0.087\n",
      "F1 Macro: 0.730 ± 0.088\n",
      "F1 Weighted: 0.730 ± 0.091\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:05,101] Trial 5 finished with value: 0.3933985308620883 and parameters: {'lr': 0.00014644694779745074, 'dropout': 0.5843343722405275}. Best is trial 4 with value: 0.3933985308620883.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:06,660] Trial 6 finished with value: 0.4313670326166885 and parameters: {'lr': 0.00027126096795881055, 'dropout': 0.5490774328143101}. Best is trial 6 with value: 0.4313670326166885.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.431\n",
      "Best model performance:\n",
      "Accuracy: 0.758 ± 0.088\n",
      "F1 Macro: 0.754 ± 0.085\n",
      "F1 Weighted: 0.755 ± 0.088\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:08,227] Trial 7 finished with value: 0.3354049175009829 and parameters: {'lr': 0.0018766412839409633, 'dropout': 0.38089903703500594}. Best is trial 6 with value: 0.4313670326166885.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:09,814] Trial 8 finished with value: 0.3129736242864627 and parameters: {'lr': 0.0010598477246416972, 'dropout': 0.17899238391964314}. Best is trial 6 with value: 0.4313670326166885.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaThesis2/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-12-08 14:24:11,396] Trial 9 finished with value: 0.3933985308620883 and parameters: {'lr': 0.00013181063932985956, 'dropout': 0.27556860068081357}. Best is trial 6 with value: 0.4313670326166885.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.132\n",
      "F1 Macro: 0.755 ± 0.135\n",
      "F1 Weighted: 0.754 ± 0.139\n",
      "Best hyperparameters:\n",
      "{'lr': 0.00015545789103861535, 'dropout': 0.5038754072096114}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:24:11,711] A new study created in memory with name: no-name-a8c399e4-23d5-445e-9cff-82aa6c4be7bf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:26:17,065] Trial 0 finished with value: 0.6117195708786672 and parameters: {}. Best is trial 0 with value: 0.6117195708786672.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.612\n",
      "Best model performance:\n",
      "Accuracy: 0.849 ± 0.046\n",
      "F1 Macro: 0.848 ± 0.046\n",
      "F1 Weighted: 0.849 ± 0.046\n",
      "[{'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.9, 'f1_macro': np.float64(0.898989898989899), 'f1_weighted': np.float64(0.901010101010101)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.849 ± 0.046\n",
      "F1 Macro: 0.848 ± 0.046\n",
      "F1 Weighted: 0.849 ± 0.046\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:31:36,856] A new study created in memory with name: no-name-8e2df972-fed7-44da-b2b8-24af414554fa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5288, Train Acc: 0.7143, Train F1 Macro: 0.6971, Train F1 Weighted: 0.7040\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.3529, Val F1 Weighted: 0.3850, Val Geometric Mean: 0.4201\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.3529, Test F1 Weighted: 0.3850\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2804, Train Acc: 0.9524, Train F1 Macro: 0.9523, Train F1 Weighted: 0.9525\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6303, Val Geometric Mean: 0.6333\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6303\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.0640, Train Acc: 0.9762, Train F1 Macro: 0.9761, Train F1 Weighted: 0.9762\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6364, Val Geometric Mean: 0.6354\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6364\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.6160, Train Acc: 0.6429, Train F1 Macro: 0.6427, Train F1 Weighted: 0.6435\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.4762, Val F1 Weighted: 0.4589, Val Geometric Mean: 0.4921\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.4762, Test F1 Weighted: 0.4589\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.3564, Train Acc: 0.8810, Train F1 Macro: 0.8776, Train F1 Weighted: 0.8795\n",
      "Val Acc: 0.8182, Val F1 Macro: 0.8167, Val F1 Weighted: 0.8182, Val Geometric Mean: 0.8177\n",
      "Test Acc: 0.8182, Test F1 Macro: 0.8167, Test F1 Weighted: 0.8182\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1651, Train Acc: 0.9048, Train F1 Macro: 0.9039, Train F1 Weighted: 0.9048\n",
      "Val Acc: 0.7273, Val F1 Macro: 0.7179, Val F1 Weighted: 0.7133, Val Geometric Mean: 0.7195\n",
      "Test Acc: 0.7273, Test F1 Macro: 0.7179, Test F1 Weighted: 0.7133\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.4335, Train Acc: 0.7381, Train F1 Macro: 0.6998, Train F1 Weighted: 0.7100\n",
      "Val Acc: 0.7273, Val F1 Macro: 0.6857, Val F1 Weighted: 0.6961, Val Geometric Mean: 0.7028\n",
      "Test Acc: 0.7273, Test F1 Macro: 0.6857, Test F1 Weighted: 0.6961\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.6226)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.5472)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(10) tensor(15.0189)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6415)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5740, Train Acc: 0.6429, Train F1 Macro: 0.6427, Train F1 Weighted: 0.6418\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.4762, Val F1 Weighted: 0.4589, Val Geometric Mean: 0.4921\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.4762, Test F1 Weighted: 0.4589\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.5082, Train Acc: 0.7381, Train F1 Macro: 0.7379, Train F1 Weighted: 0.7385\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4455, Val Geometric Mean: 0.4500\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4455\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4269, Train Acc: 0.7619, Train F1 Macro: 0.7569, Train F1 Weighted: 0.7536\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4455, Val Geometric Mean: 0.4500\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4455\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3104, Train Acc: 0.8095, Train F1 Macro: 0.8091, Train F1 Weighted: 0.8082\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.2552, Train Acc: 0.9286, Train F1 Macro: 0.9282, Train F1 Weighted: 0.9287\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.2920, Train Acc: 0.9048, Train F1 Macro: 0.9045, Train F1 Weighted: 0.9050\n",
      "Val Acc: 0.3636, Val F1 Macro: 0.3419, Val F1 Weighted: 0.3528, Val Geometric Mean: 0.3526\n",
      "Test Acc: 0.3636, Test F1 Macro: 0.3419, Test F1 Weighted: 0.3528\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.2735, Train Acc: 0.8333, Train F1 Macro: 0.8325, Train F1 Weighted: 0.8313\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4455, Val Geometric Mean: 0.4500\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4455\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.6038)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.0377)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(10) tensor(16.5094)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(2) tensor(0) tensor(15.9245)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4286, Train Acc: 0.8571, Train F1 Macro: 0.8568, Train F1 Weighted: 0.8575\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6071, Val F1 Weighted: 0.6169, Val Geometric Mean: 0.6200\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6071, Test F1 Weighted: 0.6169\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3845, Train Acc: 0.7381, Train F1 Macro: 0.7343, Train F1 Weighted: 0.7313\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.5154, Train Acc: 0.7143, Train F1 Macro: 0.7035, Train F1 Weighted: 0.7089\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5299, Val F1 Weighted: 0.5377, Val Geometric Mean: 0.5376\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5299, Test F1 Weighted: 0.5377\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2396, Train Acc: 0.8571, Train F1 Macro: 0.8486, Train F1 Weighted: 0.8520\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6071, Val F1 Weighted: 0.6169, Val Geometric Mean: 0.6200\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6071, Test F1 Weighted: 0.6169\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.5557, Train Acc: 0.6667, Train F1 Macro: 0.6667, Train F1 Weighted: 0.6667\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6364, Val Geometric Mean: 0.6354\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6364\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.3291, Train Acc: 0.8095, Train F1 Macro: 0.8078, Train F1 Weighted: 0.8095\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.6612, Train Acc: 0.5952, Train F1 Macro: 0.4603, Train F1 Weighted: 0.4860\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.3529, Val F1 Weighted: 0.3850, Val Geometric Mean: 0.4201\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.3529, Test F1 Weighted: 0.3850\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.8491)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1698)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(8) tensor(13.1509)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(3) tensor(0) tensor(15.5283)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6684, Train Acc: 0.5349, Train F1 Macro: 0.3485, Train F1 Weighted: 0.3728\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6504, Train Acc: 0.5349, Train F1 Macro: 0.3485, Train F1 Weighted: 0.3728\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.6221, Train Acc: 0.5349, Train F1 Macro: 0.5023, Train F1 Weighted: 0.4934\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.3500, Val Geometric Mean: 0.3744\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.3500\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.6101, Train Acc: 0.6279, Train F1 Macro: 0.6019, Train F1 Weighted: 0.5947\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.2788, Val Geometric Mean: 0.2904\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.2788\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.3125, Train Acc: 0.7674, Train F1 Macro: 0.7568, Train F1 Weighted: 0.7603\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.3915\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.4209, Train Acc: 0.7442, Train F1 Macro: 0.7391, Train F1 Weighted: 0.7366\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.4000, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.4000\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.4000, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0744, Train Acc: 0.9767, Train F1 Macro: 0.9765, Train F1 Weighted: 0.9767\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.1698)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.5660)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(14.8491)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(3) tensor(0) tensor(15.6981)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5796, Train Acc: 0.6047, Train F1 Macro: 0.5905, Train F1 Weighted: 0.5993\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.7917, Val F1 Weighted: 0.7917, Val Geometric Mean: 0.7944\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.7917, Test F1 Weighted: 0.7917\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3527, Train Acc: 0.8140, Train F1 Macro: 0.8139, Train F1 Weighted: 0.8133\n",
      "Val Acc: 0.7000, Val F1 Macro: 0.6970, Val F1 Weighted: 0.6970, Val Geometric Mean: 0.6980\n",
      "Test Acc: 0.7000, Test F1 Macro: 0.6970, Test F1 Weighted: 0.6970\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4297, Train Acc: 0.9070, Train F1 Macro: 0.9069, Train F1 Weighted: 0.9072\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2053, Train Acc: 0.9070, Train F1 Macro: 0.9065, Train F1 Weighted: 0.9073\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5238, Val F1 Weighted: 0.5238, Val Geometric Mean: 0.5481\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5238, Test F1 Weighted: 0.5238\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.2064, Train Acc: 0.9070, Train F1 Macro: 0.9065, Train F1 Weighted: 0.9073\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.7917, Val F1 Weighted: 0.7917, Val Geometric Mean: 0.7944\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.7917, Test F1 Weighted: 0.7917\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1489, Train Acc: 0.9302, Train F1 Macro: 0.9301, Train F1 Weighted: 0.9305\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5238, Val F1 Weighted: 0.5238, Val Geometric Mean: 0.5481\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5238, Test F1 Weighted: 0.5238\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 14:36:19,545] Trial 0 finished with value: 0.5847507094155515 and parameters: {}. Best is trial 0 with value: 0.5847507094155515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0956, Train Acc: 0.9302, Train F1 Macro: 0.9296, Train F1 Weighted: 0.9304\n",
      "Val Acc: 0.7000, Val F1 Macro: 0.6703, Val F1 Weighted: 0.6703, Val Geometric Mean: 0.6801\n",
      "Test Acc: 0.7000, Test F1 Macro: 0.6703, Test F1 Weighted: 0.6703\n",
      "##################################################\n",
      "New best score: 0.585\n",
      "Best model performance:\n",
      "Accuracy: 0.847 ± 0.099\n",
      "F1 Macro: 0.826 ± 0.132\n",
      "F1 Weighted: 0.835 ± 0.118\n",
      "[{'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.7, 'f1_macro': np.float64(0.6000000000000001), 'f1_weighted': np.float64(0.64)}, {'acc': 0.9, 'f1_macro': np.float64(0.898989898989899), 'f1_weighted': np.float64(0.898989898989899)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.847 ± 0.099\n",
      "F1 Macro: 0.826 ± 0.132\n",
      "F1 Weighted: 0.835 ± 0.118\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": False,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
