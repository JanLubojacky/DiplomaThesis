{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_risk/mrna_mirna_circrna.csv'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    \"mrna\": mrna_loader,\n",
    "    \"mirna\": mirna_loader,\n",
    "    \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    # \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_risk/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:21:58,103] A new study created in memory with name: no-name-7e40e229-d887-4ae3-86ed-36c75469c619\n",
      "[I 2024-11-21 00:21:58,247] Trial 0 finished with value: 0.34204924042989987 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:58,371] Trial 1 finished with value: 0.19388513670751556 and parameters: {'n_neighbors': 1}. Best is trial 0 with value: 0.34204924042989987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.342\n",
      "Best model performance:\n",
      "Accuracy: 0.704 ± 0.128\n",
      "F1 Macro: 0.697 ± 0.130\n",
      "F1 Weighted: 0.697 ± 0.134\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:21:58,494] Trial 2 finished with value: 0.3049770148266652 and parameters: {'n_neighbors': 11}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:58,620] Trial 3 finished with value: 0.31018182119467835 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:58,760] Trial 4 finished with value: 0.25476976237074866 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:58,881] Trial 5 finished with value: 0.29005021306735596 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,002] Trial 6 finished with value: 0.25476976237074866 and parameters: {'n_neighbors': 13}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,128] Trial 7 finished with value: 0.29005021306735596 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,251] Trial 8 finished with value: 0.34204924042989987 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,375] Trial 9 finished with value: 0.2546081508254172 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,501] Trial 10 finished with value: 0.29071061175533214 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,626] Trial 11 finished with value: 0.29071061175533214 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,751] Trial 12 finished with value: 0.2546081508254172 and parameters: {'n_neighbors': 7}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:21:59,877] Trial 13 finished with value: 0.2873852494518468 and parameters: {'n_neighbors': 18}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,010] Trial 14 finished with value: 0.31018182119467835 and parameters: {'n_neighbors': 17}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,149] Trial 15 finished with value: 0.23176748213289852 and parameters: {'n_neighbors': 8}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,272] Trial 16 finished with value: 0.17818492776788214 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,397] Trial 17 finished with value: 0.29071061175533214 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,522] Trial 18 finished with value: 0.29005021306735596 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: 0.34204924042989987.\n",
      "[I 2024-11-21 00:22:00,651] Trial 19 finished with value: 0.23792856403695398 and parameters: {'n_neighbors': 4}. Best is trial 0 with value: 0.34204924042989987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_risk/mrna_mirna_circrna.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:00,683] A new study created in memory with name: no-name-b7ab9e38-cd1d-4a1b-a616-cf073cc6f28e\n",
      "[I 2024-11-21 00:22:00,811] Trial 0 finished with value: 0.33450122790480763 and parameters: {'C': 0.01945349170115929, 'class_weight': None}. Best is trial 0 with value: 0.33450122790480763.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.335\n",
      "Best model performance:\n",
      "Accuracy: 0.702 ± 0.117\n",
      "F1 Macro: 0.690 ± 0.119\n",
      "F1 Weighted: 0.691 ± 0.123\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:00,921] Trial 1 finished with value: 0.33450122790480763 and parameters: {'C': 0.026158729824112388, 'class_weight': None}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,034] Trial 2 finished with value: 0.33450122790480763 and parameters: {'C': 0.01678306741875054, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,163] Trial 3 finished with value: 0.2520688516904583 and parameters: {'C': 4.063671185117358, 'class_weight': None}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,296] Trial 4 finished with value: 0.2520688516904583 and parameters: {'C': 1.5977743899288857, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,420] Trial 5 finished with value: 0.22866732496688394 and parameters: {'C': 0.15189199699495332, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,549] Trial 6 finished with value: 0.2520688516904583 and parameters: {'C': 0.5028877497698824, 'class_weight': None}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,671] Trial 7 finished with value: 0.22866732496688394 and parameters: {'C': 0.13868216945884151, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,803] Trial 8 finished with value: 0.2520688516904583 and parameters: {'C': 6.5843842059459226, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.33450122790480763.\n",
      "[I 2024-11-21 00:22:01,918] Trial 9 finished with value: 0.340415895850237 and parameters: {'C': 0.03894542502997774, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,039] Trial 10 finished with value: 0.282303085828288 and parameters: {'C': 0.0683839851811138, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.340\n",
      "Best model performance:\n",
      "Accuracy: 0.704 ± 0.128\n",
      "F1 Macro: 0.696 ± 0.128\n",
      "F1 Weighted: 0.695 ± 0.132\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:02,170] Trial 11 finished with value: 0.2777096180173103 and parameters: {'C': 0.03170500460409983, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,278] Trial 12 finished with value: 0.33450122790480763 and parameters: {'C': 0.010828198106658107, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,399] Trial 13 finished with value: 0.33638068415134637 and parameters: {'C': 0.06426839282912879, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,528] Trial 14 finished with value: 0.2520688516904583 and parameters: {'C': 0.38182704984874594, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,648] Trial 15 finished with value: 0.282303085828288 and parameters: {'C': 0.05808661948912825, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,773] Trial 16 finished with value: 0.22866732496688394 and parameters: {'C': 0.12041263332240129, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:02,911] Trial 17 finished with value: 0.2520688516904583 and parameters: {'C': 0.9573585468707587, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,034] Trial 18 finished with value: 0.3042861753630985 and parameters: {'C': 0.05921435694908405, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,160] Trial 19 finished with value: 0.2520688516904583 and parameters: {'C': 0.23191204130943224, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,286] Trial 20 finished with value: 0.2777096180173103 and parameters: {'C': 0.03435040835255738, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,394] Trial 21 finished with value: 0.33450122790480763 and parameters: {'C': 0.01120340955683628, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,508] Trial 22 finished with value: 0.33450122790480763 and parameters: {'C': 0.020950905995940232, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,628] Trial 23 finished with value: 0.2777096180173103 and parameters: {'C': 0.049671621897707796, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,749] Trial 24 finished with value: 0.2758712018115035 and parameters: {'C': 0.09929400381602248, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,859] Trial 25 finished with value: 0.33450122790480763 and parameters: {'C': 0.016944550998822333, 'class_weight': None}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:03,978] Trial 26 finished with value: 0.340415895850237 and parameters: {'C': 0.03845378944073488, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,111] Trial 27 finished with value: 0.2520688516904583 and parameters: {'C': 0.2323998944997291, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,231] Trial 28 finished with value: 0.340415895850237 and parameters: {'C': 0.040663734377237595, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,348] Trial 29 finished with value: 0.340415895850237 and parameters: {'C': 0.034141481791351506, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,483] Trial 30 finished with value: 0.282303085828288 and parameters: {'C': 0.09190372803765222, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,600] Trial 31 finished with value: 0.340415895850237 and parameters: {'C': 0.035002221924738786, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,717] Trial 32 finished with value: 0.340415895850237 and parameters: {'C': 0.041115098648227766, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,832] Trial 33 finished with value: 0.340415895850237 and parameters: {'C': 0.024466199776858738, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:04,943] Trial 34 finished with value: 0.33450122790480763 and parameters: {'C': 0.015699435763756, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,059] Trial 35 finished with value: 0.340415895850237 and parameters: {'C': 0.028770572552788153, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,185] Trial 36 finished with value: 0.2520688516904583 and parameters: {'C': 0.1879717798748229, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,309] Trial 37 finished with value: 0.282303085828288 and parameters: {'C': 0.08854056737080677, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,443] Trial 38 finished with value: 0.2520688516904583 and parameters: {'C': 0.683594887354548, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,575] Trial 39 finished with value: 0.2520688516904583 and parameters: {'C': 2.180088990224263, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,703] Trial 40 finished with value: 0.33450122790480763 and parameters: {'C': 0.014745092373967313, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,823] Trial 41 finished with value: 0.340415895850237 and parameters: {'C': 0.04320308770752007, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:05,937] Trial 42 finished with value: 0.340415895850237 and parameters: {'C': 0.02398461806319494, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,057] Trial 43 finished with value: 0.340415895850237 and parameters: {'C': 0.03626879166081412, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,179] Trial 44 finished with value: 0.282303085828288 and parameters: {'C': 0.07202412857972805, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,296] Trial 45 finished with value: 0.33450122790480763 and parameters: {'C': 0.020675238134043376, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,414] Trial 46 finished with value: 0.340415895850237 and parameters: {'C': 0.04278035536384624, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,522] Trial 47 finished with value: 0.33450122790480763 and parameters: {'C': 0.01005921040539476, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,647] Trial 48 finished with value: 0.22866732496688394 and parameters: {'C': 0.12663714365892306, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n",
      "[I 2024-11-21 00:22:06,778] Trial 49 finished with value: 0.340415895850237 and parameters: {'C': 0.028316710992828054, 'class_weight': 'balanced'}. Best is trial 9 with value: 0.340415895850237.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:06,818] A new study created in memory with name: no-name-47b88404-50f5-4548-9b0a-70e28eef2d2f\n",
      "[I 2024-11-21 00:22:07,026] Trial 0 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 8.172875198129406e-06, 'alpha': 0.7230665708592489, 'max_depth': 7, 'eta': 2.87048416293221e-05, 'gamma': 4.0730159441985344e-07, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.2149085674931129.\n",
      "[I 2024-11-21 00:22:07,197] Trial 1 finished with value: 0.07498688884758245 and parameters: {'booster': 'gblinear', 'lambda': 0.07783228967457771, 'alpha': 0.8453666208490124}. Best is trial 0 with value: 0.2149085674931129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.215\n",
      "Best model performance:\n",
      "Accuracy: 0.620 ± 0.073\n",
      "F1 Macro: 0.582 ± 0.133\n",
      "F1 Weighted: 0.595 ± 0.108\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.5, 'f1_macro': np.float64(0.3333333333333333), 'f1_weighted': np.float64(0.4)}, {'acc': 0.6, 'f1_macro': np.float64(0.5833333333333333), 'f1_weighted': np.float64(0.5833333333333333)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:07,460] Trial 2 finished with value: 0.2149085674931129 and parameters: {'booster': 'dart', 'lambda': 0.00043585617404054455, 'alpha': 0.0008872109723103269, 'max_depth': 7, 'eta': 6.53338875723601e-07, 'gamma': 8.694938435372058e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 5.87859555689667e-08, 'skip_drop': 0.013546539462348014}. Best is trial 0 with value: 0.2149085674931129.\n",
      "[I 2024-11-21 00:22:07,595] Trial 3 finished with value: 0.38524225167290765 and parameters: {'booster': 'gblinear', 'lambda': 8.738544374904245e-05, 'alpha': 0.02683811674420172}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:07,729] Trial 4 finished with value: 0.3735629973133255 and parameters: {'booster': 'gblinear', 'lambda': 0.00575093052129023, 'alpha': 0.07409413658924324}. Best is trial 3 with value: 0.38524225167290765.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.385\n",
      "Best model performance:\n",
      "Accuracy: 0.735 ± 0.044\n",
      "F1 Macro: 0.722 ± 0.054\n",
      "F1 Weighted: 0.726 ± 0.050\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8151515151515153)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6901098901098901)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:07,926] Trial 5 finished with value: 0.3515032983971951 and parameters: {'booster': 'gblinear', 'lambda': 0.02285844265284716, 'alpha': 0.0014514746742740501}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:08,103] Trial 6 finished with value: 0.3305099711477334 and parameters: {'booster': 'gblinear', 'lambda': 0.22160869972866146, 'alpha': 2.4009291906845667e-05}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:08,336] Trial 7 finished with value: 0.21579444444444446 and parameters: {'booster': 'gbtree', 'lambda': 1.3281230837856426e-06, 'alpha': 1.4432248477778299e-08, 'max_depth': 3, 'eta': 0.13570205881812453, 'gamma': 1.779558141715992e-07, 'grow_policy': 'depthwise'}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:08,548] Trial 8 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 3.662802334223968e-08, 'alpha': 1.4421894972027642e-08, 'max_depth': 9, 'eta': 5.021516525536252e-06, 'gamma': 0.4077322299482791, 'grow_policy': 'lossguide'}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:08,788] Trial 9 finished with value: 0.21534604040404037 and parameters: {'booster': 'dart', 'lambda': 0.08034679928058436, 'alpha': 1.5924598588001089e-06, 'max_depth': 7, 'eta': 0.006697065247576348, 'gamma': 0.12076422739057922, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'tree', 'rate_drop': 0.0069553563282198265, 'skip_drop': 0.0004390334541445934}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:08,971] Trial 10 finished with value: 0.35677091719566123 and parameters: {'booster': 'gblinear', 'lambda': 0.00015715467930687757, 'alpha': 0.010410747074126314}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:09,123] Trial 11 finished with value: 0.38524225167290765 and parameters: {'booster': 'gblinear', 'lambda': 0.0018606475645805347, 'alpha': 0.03298634178309379}. Best is trial 3 with value: 0.38524225167290765.\n",
      "[I 2024-11-21 00:22:09,278] Trial 12 finished with value: 0.4146797854017747 and parameters: {'booster': 'gblinear', 'lambda': 0.0015484831892314133, 'alpha': 0.020522335743483593}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:09,461] Trial 13 finished with value: 0.3630055188416464 and parameters: {'booster': 'gblinear', 'lambda': 1.1578260105517985e-05, 'alpha': 5.0862849117564816e-05}. Best is trial 12 with value: 0.4146797854017747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.415\n",
      "Best model performance:\n",
      "Accuracy: 0.753 ± 0.054\n",
      "F1 Macro: 0.740 ± 0.066\n",
      "F1 Weighted: 0.744 ± 0.062\n",
      "[{'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8151515151515153)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6901098901098901)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:09,693] Trial 14 finished with value: 0.1001661456337518 and parameters: {'booster': 'dart', 'lambda': 1.4405104927187283e-05, 'alpha': 0.0019003423904554832, 'max_depth': 1, 'eta': 1.9771563941942304e-08, 'gamma': 0.0014831231125841304, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.7512019471770593, 'skip_drop': 1.8505151444302425e-08}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:09,850] Trial 15 finished with value: 0.3734019041622159 and parameters: {'booster': 'gblinear', 'lambda': 2.402857428956379e-07, 'alpha': 0.12555588764290232}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:09,996] Trial 16 finished with value: 0.304836957913881 and parameters: {'booster': 'gblinear', 'lambda': 0.0011444108242948513, 'alpha': 2.903695575916224e-06}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:10,203] Trial 17 finished with value: 0.38529159843335303 and parameters: {'booster': 'gblinear', 'lambda': 5.434916654765522e-05, 'alpha': 0.005176210880453682}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:10,446] Trial 18 finished with value: 0.21534604040404037 and parameters: {'booster': 'gbtree', 'lambda': 0.0067751574925013795, 'alpha': 0.003585553662705043, 'max_depth': 4, 'eta': 0.001687236186172424, 'gamma': 0.0009155719711495607, 'grow_policy': 'lossguide'}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:10,657] Trial 19 finished with value: 0.3836809469904056 and parameters: {'booster': 'dart', 'lambda': 5.9313397805005163e-05, 'alpha': 0.0003642484244521419, 'max_depth': 1, 'eta': 0.25381042661595415, 'gamma': 1.9419531581347154e-08, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 3.8953907026793994e-08, 'skip_drop': 8.420912556376811e-08}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:10,871] Trial 20 finished with value: 0.3932585665986905 and parameters: {'booster': 'gblinear', 'lambda': 1.5640926404577533e-06, 'alpha': 0.00019523863276160397}. Best is trial 12 with value: 0.4146797854017747.\n",
      "[I 2024-11-21 00:22:11,094] Trial 21 finished with value: 0.41541020440321136 and parameters: {'booster': 'gblinear', 'lambda': 1.0354382672556374e-06, 'alpha': 0.0001509358576885963}. Best is trial 21 with value: 0.41541020440321136.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.415\n",
      "Best model performance:\n",
      "Accuracy: 0.755 ± 0.093\n",
      "F1 Macro: 0.740 ± 0.095\n",
      "F1 Weighted: 0.744 ± 0.097\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.7, 'f1_macro': np.float64(0.6703296703296704), 'f1_weighted': np.float64(0.6703296703296703)}]\n",
      "New best score: 0.458\n",
      "Best model performance:\n",
      "Accuracy: 0.775 ± 0.110\n",
      "F1 Macro: 0.769 ± 0.110\n",
      "F1 Weighted: 0.769 ± 0.113\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.717948717948718), 'f1_weighted': np.float64(0.7132867132867133)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.7, 'f1_macro': np.float64(0.696969696969697), 'f1_weighted': np.float64(0.696969696969697)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:11,299] Trial 22 finished with value: 0.45848949058461286 and parameters: {'booster': 'gblinear', 'lambda': 1.431635732487221e-08, 'alpha': 4.050786097843031e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:11,516] Trial 23 finished with value: 0.42037358084130233 and parameters: {'booster': 'gblinear', 'lambda': 1.2756470106316192e-08, 'alpha': 3.929754725425964e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:11,673] Trial 24 finished with value: 0.36984636486390743 and parameters: {'booster': 'gblinear', 'lambda': 1.0022074321863063e-08, 'alpha': 3.062810332032032e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:11,860] Trial 25 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 1.2744454399998134e-07, 'alpha': 2.552902247046118e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:12,039] Trial 26 finished with value: 0.3935860978617341 and parameters: {'booster': 'gblinear', 'lambda': 7.862608416472454e-08, 'alpha': 1.032147044104111e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:12,213] Trial 27 finished with value: 0.3888154709698619 and parameters: {'booster': 'gblinear', 'lambda': 1.3924874700772243e-08, 'alpha': 3.943569140759895e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:12,481] Trial 28 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 1.5761542845945133e-07, 'alpha': 1.6808226937119917e-07, 'max_depth': 9, 'eta': 3.70062768691324e-08, 'gamma': 1.868714441065166e-05, 'grow_policy': 'depthwise'}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:12,755] Trial 29 finished with value: 0.21534604040404037 and parameters: {'booster': 'dart', 'lambda': 3.357731066358495e-07, 'alpha': 7.536759515768791e-07, 'max_depth': 3, 'eta': 0.0008580026380116419, 'gamma': 0.012013053930089224, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 5.6692120459373994e-05, 'skip_drop': 0.8774955183326946}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:12,967] Trial 30 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 3.775859850870146e-08, 'alpha': 1.1197611754315116e-05, 'max_depth': 5, 'eta': 6.860974883394195e-07, 'gamma': 0.00014550458879833026, 'grow_policy': 'depthwise'}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:13,120] Trial 31 finished with value: 0.42412821098146236 and parameters: {'booster': 'gblinear', 'lambda': 1.5834472473825452e-06, 'alpha': 7.573693874088786e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:13,290] Trial 32 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 2.9620340892558615e-08, 'alpha': 5.7038087549366626e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:13,456] Trial 33 finished with value: 0.45675081171642196 and parameters: {'booster': 'gblinear', 'lambda': 2.86859143514952e-06, 'alpha': 4.416682803243646e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:13,672] Trial 34 finished with value: 0.4220058097169786 and parameters: {'booster': 'gblinear', 'lambda': 5.7749775458489315e-08, 'alpha': 2.845437573526792e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:13,837] Trial 35 finished with value: 0.42412821098146236 and parameters: {'booster': 'gblinear', 'lambda': 5.517671128878345e-07, 'alpha': 3.3000592588181413e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:14,083] Trial 36 finished with value: 0.39114991643539465 and parameters: {'booster': 'gblinear', 'lambda': 3.233049742555051e-06, 'alpha': 9.262268656715029e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:14,260] Trial 37 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 1.2019520432451957e-07, 'alpha': 4.944229488928747e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:14,445] Trial 38 finished with value: 0.33371510242671076 and parameters: {'booster': 'gblinear', 'lambda': 4.368139475489706e-06, 'alpha': 1.3323204038068938e-05}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:14,766] Trial 39 finished with value: 0.21534604040404037 and parameters: {'booster': 'dart', 'lambda': 2.8579708498911784e-08, 'alpha': 2.6239268381945784e-07, 'max_depth': 6, 'eta': 0.031711963077649424, 'gamma': 2.0812447783387815e-06, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 1.886467912620434e-05, 'skip_drop': 6.3659061452190606e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:14,921] Trial 40 finished with value: 0.3293863788159891 and parameters: {'booster': 'gblinear', 'lambda': 5.355363521723894e-07, 'alpha': 6.277669062894264e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:15,126] Trial 41 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 1.180231959512386e-07, 'alpha': 3.976779745776919e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:15,341] Trial 42 finished with value: 0.4220058097169786 and parameters: {'booster': 'gblinear', 'lambda': 1.054226218817968e-07, 'alpha': 1.0598744598842957e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:15,533] Trial 43 finished with value: 0.4220058097169786 and parameters: {'booster': 'gblinear', 'lambda': 1.820818956925332e-08, 'alpha': 7.550338443284728e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:15,691] Trial 44 finished with value: 0.42224234977196473 and parameters: {'booster': 'gblinear', 'lambda': 6.192638337997665e-08, 'alpha': 7.108091480144156e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:15,892] Trial 45 finished with value: 0.3935860978617341 and parameters: {'booster': 'gblinear', 'lambda': 2.325592783512807e-07, 'alpha': 1.3726264780958043e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:16,080] Trial 46 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 2.797294515228938e-08, 'alpha': 2.6249123947572517e-08, 'max_depth': 2, 'eta': 0.00010218922652990075, 'gamma': 1.5493202829747926e-08, 'grow_policy': 'lossguide'}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:16,226] Trial 47 finished with value: 0.33565535878263153 and parameters: {'booster': 'gblinear', 'lambda': 2.3653189825259876e-05, 'alpha': 5.3075393175331684e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:16,398] Trial 48 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 6.351358914504601e-07, 'alpha': 1.164395893582019e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:16,659] Trial 49 finished with value: 0.2006470534660145 and parameters: {'booster': 'dart', 'lambda': 5.595543049518124e-06, 'alpha': 2.7365960091259767e-06, 'max_depth': 5, 'eta': 0.753468418942618, 'gamma': 0.031385379435637024, 'grow_policy': 'depthwise', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.7135794569997488, 'skip_drop': 5.6998101185037895e-06}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:16,888] Trial 50 finished with value: 0.33742429500569354 and parameters: {'booster': 'gblinear', 'lambda': 1.93646385994825e-07, 'alpha': 5.191880709612849e-05}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,097] Trial 51 finished with value: 0.42464295101461563 and parameters: {'booster': 'gblinear', 'lambda': 9.952940047877762e-08, 'alpha': 1.8260360919064866e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,241] Trial 52 finished with value: 0.4220058097169786 and parameters: {'booster': 'gblinear', 'lambda': 4.5319604418016944e-08, 'alpha': 4.9086421549969174e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,407] Trial 53 finished with value: 0.3097143981559566 and parameters: {'booster': 'gblinear', 'lambda': 0.0002123032750618629, 'alpha': 4.2145338857803393e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,590] Trial 54 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 2.3486544442949273e-08, 'alpha': 1.0301592968147263e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,762] Trial 55 finished with value: 0.3305099711477334 and parameters: {'booster': 'gblinear', 'lambda': 0.9609887462388602, 'alpha': 1.772662841386333e-07}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:17,960] Trial 56 finished with value: 0.4317137893461849 and parameters: {'booster': 'gblinear', 'lambda': 1.2837734675751805e-07, 'alpha': 5.5791571398003715e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:18,197] Trial 57 finished with value: 0.2149085674931129 and parameters: {'booster': 'gbtree', 'lambda': 3.0859118304127665e-07, 'alpha': 1.7594134475369776e-06, 'max_depth': 8, 'eta': 5.41807726267145e-07, 'gamma': 0.00012210635808849435, 'grow_policy': 'depthwise'}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:18,409] Trial 58 finished with value: 0.453983092332606 and parameters: {'booster': 'gblinear', 'lambda': 2.058077789599459e-06, 'alpha': 1.9939686814046223e-08}. Best is trial 22 with value: 0.45848949058461286.\n",
      "[I 2024-11-21 00:22:18,618] Trial 59 finished with value: 0.4901031903211952 and parameters: {'booster': 'gblinear', 'lambda': 1.0801377988023472e-08, 'alpha': 5.03877881619746e-07}. Best is trial 59 with value: 0.4901031903211952.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.490\n",
      "Best model performance:\n",
      "Accuracy: 0.795 ± 0.104\n",
      "F1 Macro: 0.784 ± 0.106\n",
      "F1 Weighted: 0.787 ± 0.107\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.717948717948718), 'f1_weighted': np.float64(0.7132867132867133)}, {'acc': 0.9090909090909091, 'f1_macro': np.float64(0.9090909090909091), 'f1_weighted': np.float64(0.9090909090909091)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6303030303030303)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.9, 'f1_macro': np.float64(0.898989898989899), 'f1_weighted': np.float64(0.898989898989899)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.795 ± 0.104\n",
      "F1 Macro: 0.784 ± 0.106\n",
      "F1 Weighted: 0.787 ± 0.107\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:18,653] A new study created in memory with name: no-name-1ceb2680-e83c-440b-8cf3-0a0547387d49\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:20,163] Trial 0 finished with value: 0.37003309090909087 and parameters: {'lr': 0.0008732686223895885, 'dropout': 0.3346528849486463}. Best is trial 0 with value: 0.37003309090909087.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.370\n",
      "Best model performance:\n",
      "Accuracy: 0.720 ± 0.073\n",
      "F1 Macro: 0.715 ± 0.071\n",
      "F1 Weighted: 0.718 ± 0.072\n",
      "[{'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.8)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:21,644] Trial 1 finished with value: 0.36864492846001523 and parameters: {'lr': 0.00014045160975316605, 'dropout': 0.2105618742955104}. Best is trial 0 with value: 0.37003309090909087.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:23,102] Trial 2 finished with value: 0.33450122790480763 and parameters: {'lr': 0.0001109814420816853, 'dropout': 0.5988389288874266}. Best is trial 0 with value: 0.37003309090909087.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:24,620] Trial 3 finished with value: 0.33522415474191697 and parameters: {'lr': 0.0014229896019549149, 'dropout': 0.5484404860349028}. Best is trial 0 with value: 0.37003309090909087.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:26,072] Trial 4 finished with value: 0.2682723226109673 and parameters: {'lr': 0.00010403690908600127, 'dropout': 0.41800990750395584}. Best is trial 0 with value: 0.37003309090909087.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:27,574] Trial 5 finished with value: 0.42567137933560617 and parameters: {'lr': 0.0006036760617085977, 'dropout': 0.19056985178216437}. Best is trial 5 with value: 0.42567137933560617.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.426\n",
      "Best model performance:\n",
      "Accuracy: 0.756 ± 0.068\n",
      "F1 Macro: 0.748 ± 0.065\n",
      "F1 Weighted: 0.753 ± 0.066\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.8, 'f1_macro': np.float64(0.7619047619047619), 'f1_weighted': np.float64(0.7809523809523808)}, {'acc': 0.8, 'f1_macro': np.float64(0.8), 'f1_weighted': np.float64(0.8)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:29,034] Trial 6 finished with value: 0.3024066173284228 and parameters: {'lr': 0.00011049446814599399, 'dropout': 0.41426406015013306}. Best is trial 5 with value: 0.42567137933560617.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:30,516] Trial 7 finished with value: 0.36315701252702437 and parameters: {'lr': 0.002102505307784831, 'dropout': 0.320187437466917}. Best is trial 5 with value: 0.42567137933560617.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:32,026] Trial 8 finished with value: 0.43275352704867187 and parameters: {'lr': 0.00015545789103861535, 'dropout': 0.5038754072096114}. Best is trial 8 with value: 0.43275352704867187.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.433\n",
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.132\n",
      "F1 Macro: 0.755 ± 0.135\n",
      "F1 Weighted: 0.754 ± 0.139\n",
      "[{'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.7272727272727273), 'f1_weighted': np.float64(0.7272727272727273)}, {'acc': 0.5454545454545454, 'f1_macro': np.float64(0.5299145299145299), 'f1_weighted': np.float64(0.5221445221445221)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.9, 'f1_macro': np.float64(0.898989898989899), 'f1_weighted': np.float64(0.898989898989899)}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-21 00:22:33,502] Trial 9 finished with value: 0.42870837431264336 and parameters: {'lr': 0.0014694665632082925, 'dropout': 0.23420539133322252}. Best is trial 8 with value: 0.43275352704867187.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.760 ± 0.132\n",
      "F1 Macro: 0.755 ± 0.135\n",
      "F1 Weighted: 0.754 ± 0.139\n",
      "Best hyperparameters:\n",
      "{'lr': 0.00015545789103861535, 'dropout': 0.5038754072096114}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:33,665] A new study created in memory with name: no-name-4bc71976-03a4-46cf-b54d-717fd6feda0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:57,682] Trial 0 finished with value: 0.5685106677310369 and parameters: {}. Best is trial 0 with value: 0.5685106677310369.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.569\n",
      "Best model performance:\n",
      "Accuracy: 0.831 ± 0.035\n",
      "F1 Macro: 0.826 ± 0.033\n",
      "F1 Weighted: 0.828 ± 0.036\n",
      "[{'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8151515151515153)}, {'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.9, 'f1_macro': np.float64(0.8901098901098901), 'f1_weighted': np.float64(0.8967032967032967)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.7916666666666667)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.831 ± 0.035\n",
      "F1 Macro: 0.826 ± 0.033\n",
      "F1 Weighted: 0.828 ± 0.036\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:22:58,018] A new study created in memory with name: no-name-8ad8310d-d31b-4759-977d-a51ba78f603a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(25.0943)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.4528)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(15.1887)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.4761, Train Acc: 0.7619, Train F1 Macro: 0.7597, Train F1 Weighted: 0.7619\n",
      "Val Acc: 0.7273, Val F1 Macro: 0.7273, Val F1 Weighted: 0.7273, Val Geometric Mean: 0.7273\n",
      "Test Acc: 0.7273, Test F1 Macro: 0.7273, Test F1 Weighted: 0.7273\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3980, Train Acc: 0.8810, Train F1 Macro: 0.8809, Train F1 Weighted: 0.8806\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6071, Val F1 Weighted: 0.5974, Val Geometric Mean: 0.6134\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6071, Test F1 Weighted: 0.5974\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2218, Train Acc: 0.9048, Train F1 Macro: 0.9048, Train F1 Weighted: 0.9048\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6364, Val Geometric Mean: 0.6354\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6364\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2456, Train Acc: 0.8810, Train F1 Macro: 0.8809, Train F1 Weighted: 0.8806\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5299, Val F1 Weighted: 0.5221, Val Geometric Mean: 0.5324\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5299, Test F1 Weighted: 0.5221\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0000, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4455, Val Geometric Mean: 0.4500\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4455\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1303, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.3636, Val F1 Macro: 0.3419, Val F1 Weighted: 0.3528, Val Geometric Mean: 0.3526\n",
      "Test Acc: 0.3636, Test F1 Macro: 0.3419, Test F1 Weighted: 0.3528\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0032, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.6226)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.5472)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(10) tensor(15.0189)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5099, Train Acc: 0.7143, Train F1 Macro: 0.7136, Train F1 Weighted: 0.7123\n",
      "Val Acc: 0.3636, Val F1 Macro: 0.3419, Val F1 Weighted: 0.3310, Val Geometric Mean: 0.3452\n",
      "Test Acc: 0.3636, Test F1 Macro: 0.3419, Test F1 Weighted: 0.3310\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3899, Train Acc: 0.8333, Train F1 Macro: 0.8325, Train F1 Weighted: 0.8313\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2971, Train Acc: 0.8095, Train F1 Macro: 0.8095, Train F1 Weighted: 0.8095\n",
      "Val Acc: 0.3636, Val F1 Macro: 0.3419, Val F1 Weighted: 0.3528, Val Geometric Mean: 0.3526\n",
      "Test Acc: 0.3636, Test F1 Macro: 0.3419, Test F1 Weighted: 0.3528\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1541, Train Acc: 0.9524, Train F1 Macro: 0.9523, Train F1 Weighted: 0.9525\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1280, Train Acc: 0.9286, Train F1 Macro: 0.9282, Train F1 Weighted: 0.9287\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1494, Train Acc: 0.9286, Train F1 Macro: 0.9285, Train F1 Weighted: 0.9287\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0530, Train Acc: 0.9762, Train F1 Macro: 0.9761, Train F1 Weighted: 0.9762\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.6038)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.0377)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(10) tensor(16.5094)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.7500, Train Acc: 0.5952, Train F1 Macro: 0.5524, Train F1 Weighted: 0.5392\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.2712, Train Acc: 0.9286, Train F1 Macro: 0.9285, Train F1 Weighted: 0.9287\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4369, Train Acc: 0.8571, Train F1 Macro: 0.8568, Train F1 Weighted: 0.8562\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.0009, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.0259, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0201, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.4253, Val Geometric Mean: 0.4298\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.4253\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0494, Train Acc: 0.9762, Train F1 Macro: 0.9761, Train F1 Weighted: 0.9762\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5299, Val F1 Weighted: 0.5377, Val Geometric Mean: 0.5376\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5299, Test F1 Weighted: 0.5377\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.8491)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.1698)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(8) tensor(13.1509)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6074, Train Acc: 0.6047, Train F1 Macro: 0.5713, Train F1 Weighted: 0.5796\n",
      "Val Acc: 0.7000, Val F1 Macro: 0.6000, Val F1 Weighted: 0.6400, Val Geometric Mean: 0.6454\n",
      "Test Acc: 0.7000, Test F1 Macro: 0.6000, Test F1 Weighted: 0.6400\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.3416, Train Acc: 0.8837, Train F1 Macro: 0.8835, Train F1 Weighted: 0.8831\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2412, Train Acc: 0.9302, Train F1 Macro: 0.9288, Train F1 Weighted: 0.9295\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.4505, Val F1 Weighted: 0.4835, Val Geometric Mean: 0.4776\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.4505, Test F1 Weighted: 0.4835\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.1480, Train Acc: 0.9535, Train F1 Macro: 0.9533, Train F1 Weighted: 0.9535\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.3915\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1669, Train Acc: 0.9535, Train F1 Macro: 0.9529, Train F1 Weighted: 0.9532\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5238, Val F1 Weighted: 0.5619, Val Geometric Mean: 0.5610\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5238, Test F1 Weighted: 0.5619\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.0253, Train Acc: 0.9767, Train F1 Macro: 0.9767, Train F1 Weighted: 0.9768\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.3915\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.0017, Train Acc: 1.0000, Train F1 Macro: 1.0000, Train F1 Weighted: 1.0000\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(24.1698)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(0) tensor(21.5660)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(0) tensor(8) tensor(14.8491)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.5671, Train Acc: 0.7674, Train F1 Macro: 0.7643, Train F1 Weighted: 0.7611\n",
      "Val Acc: 0.7000, Val F1 Macro: 0.6703, Val F1 Weighted: 0.6703, Val Geometric Mean: 0.6801\n",
      "Test Acc: 0.7000, Test F1 Macro: 0.6703, Test F1 Weighted: 0.6703\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.5794, Train Acc: 0.6279, Train F1 Macro: 0.6109, Train F1 Weighted: 0.6014\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.8000, Val F1 Weighted: 0.8000, Val Geometric Mean: 0.8000\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.8000, Test F1 Weighted: 0.8000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.2477, Train Acc: 0.9070, Train F1 Macro: 0.9069, Train F1 Weighted: 0.9072\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5833, Val F1 Weighted: 0.5833, Val Geometric Mean: 0.5888\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5833, Test F1 Weighted: 0.5833\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2373, Train Acc: 0.9070, Train F1 Macro: 0.9044, Train F1 Weighted: 0.9063\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.7917, Val F1 Weighted: 0.7917, Val Geometric Mean: 0.7944\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.7917, Test F1 Weighted: 0.7917\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.1295, Train Acc: 0.9535, Train F1 Macro: 0.9533, Train F1 Weighted: 0.9536\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.5238, Val F1 Weighted: 0.5238, Val Geometric Mean: 0.5481\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.5238, Test F1 Weighted: 0.5238\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.2373, Train Acc: 0.9302, Train F1 Macro: 0.9296, Train F1 Weighted: 0.9304\n",
      "Val Acc: 0.7000, Val F1 Macro: 0.6703, Val F1 Weighted: 0.6703, Val Geometric Mean: 0.6801\n",
      "Test Acc: 0.7000, Test F1 Macro: 0.6703, Test F1 Weighted: 0.6703\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-21 00:24:54,048] Trial 0 finished with value: 0.5016456616949483 and parameters: {}. Best is trial 0 with value: 0.5016456616949483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1253, Train Acc: 0.9767, Train F1 Macro: 0.9765, Train F1 Weighted: 0.9768\n",
      "Val Acc: 0.8000, Val F1 Macro: 0.7917, Val F1 Weighted: 0.7917, Val Geometric Mean: 0.7944\n",
      "Test Acc: 0.8000, Test F1 Macro: 0.7917, Test F1 Weighted: 0.7917\n",
      "##################################################\n",
      "New best score: 0.502\n",
      "Best model performance:\n",
      "Accuracy: 0.796 ± 0.120\n",
      "F1 Macro: 0.792 ± 0.122\n",
      "F1 Weighted: 0.795 ± 0.121\n",
      "[{'acc': 0.8181818181818182, 'f1_macro': np.float64(0.8166666666666667), 'f1_weighted': np.float64(0.8181818181818182)}, {'acc': 0.7272727272727273, 'f1_macro': np.float64(0.717948717948718), 'f1_weighted': np.float64(0.7226107226107227)}, {'acc': 0.6363636363636364, 'f1_macro': np.float64(0.6333333333333333), 'f1_weighted': np.float64(0.6363636363636364)}, {'acc': 0.8, 'f1_macro': np.float64(0.7916666666666667), 'f1_weighted': np.float64(0.8)}, {'acc': 1.0, 'f1_macro': np.float64(1.0), 'f1_weighted': np.float64(1.0)}]\n",
      "Best model performance:\n",
      "Accuracy: 0.796 ± 0.120\n",
      "F1 Macro: 0.792 ± 0.122\n",
      "F1 Weighted: 0.795 ± 0.121\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": True,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
