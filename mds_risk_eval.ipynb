{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.base_classes.omic_data_loader import OmicDataLoader\n",
    "from src.data_managers.concat import CatOmicDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mrna\",\n",
    ")\n",
    "mirna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/mirna_genes\",\n",
    ")\n",
    "circrna_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/circrna\",\n",
    ")\n",
    "# pirna_loader = OmicDataLoader(\n",
    "#     data_dir=\"mds_data/splits_74/pirna\",\n",
    "# )\n",
    "te_loader = OmicDataLoader(\n",
    "    data_dir=\"mds_data/splits_74_risk/te_counts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/mds_risk/te.csv'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omic_data_loaders = {\n",
    "    # \"mrna\": mrna_loader,\n",
    "    # \"mirna\": mirna_loader,\n",
    "    # \"circrna\": circrna_loader,\n",
    "    # \"pirna\": pirna_loader,\n",
    "    \"te\": te_loader,\n",
    "}\n",
    "odm = CatOmicDataManager(omic_data_loaders, n_splits=5)\n",
    "\n",
    "save_folder = f\"logs/mds_risk/{'_'.join(omic_data_loaders.keys())}.csv\"\n",
    "save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:17,779] A new study created in memory with name: no-name-83effbb1-018e-4424-8279-33c16a3f1e42\n",
      "[I 2024-11-18 00:15:17,864] Trial 0 finished with value: 0.2511994569978806 and parameters: {'n_neighbors': 16}. Best is trial 0 with value: 0.2511994569978806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.251\n",
      "Best model performance:\n",
      "Accuracy: 0.644 ± 0.099\n",
      "F1 Macro: 0.621 ± 0.108\n",
      "F1 Weighted: 0.629 ± 0.106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:17,925] Trial 1 finished with value: 0.2778022044039014 and parameters: {'n_neighbors': 19}. Best is trial 1 with value: 0.2778022044039014.\n",
      "[I 2024-11-18 00:15:17,982] Trial 2 finished with value: 0.2993829200738212 and parameters: {'n_neighbors': 12}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,036] Trial 3 finished with value: 0.2567724091116706 and parameters: {'n_neighbors': 9}. Best is trial 2 with value: 0.2993829200738212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.278\n",
      "Best model performance:\n",
      "Accuracy: 0.662 ± 0.065\n",
      "F1 Macro: 0.644 ± 0.063\n",
      "F1 Weighted: 0.651 ± 0.063\n",
      "New best score: 0.299\n",
      "Best model performance:\n",
      "Accuracy: 0.680 ± 0.037\n",
      "F1 Macro: 0.660 ± 0.038\n",
      "F1 Weighted: 0.667 ± 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:18,092] Trial 4 finished with value: 0.2567724091116706 and parameters: {'n_neighbors': 9}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,177] Trial 5 finished with value: 0.2778022044039014 and parameters: {'n_neighbors': 20}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,230] Trial 6 finished with value: 0.22002017543695068 and parameters: {'n_neighbors': 4}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,284] Trial 7 finished with value: 0.2778022044039014 and parameters: {'n_neighbors': 19}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,337] Trial 8 finished with value: 0.2765059639424474 and parameters: {'n_neighbors': 8}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,390] Trial 9 finished with value: 0.2567724091116706 and parameters: {'n_neighbors': 9}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,447] Trial 10 finished with value: 0.269464714194896 and parameters: {'n_neighbors': 14}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,502] Trial 11 finished with value: 0.2778022044039014 and parameters: {'n_neighbors': 15}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,557] Trial 12 finished with value: 0.2741908271344041 and parameters: {'n_neighbors': 3}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,611] Trial 13 finished with value: 0.2765654626585536 and parameters: {'n_neighbors': 13}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,666] Trial 14 finished with value: 0.2778022044039014 and parameters: {'n_neighbors': 17}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,724] Trial 15 finished with value: 0.2993829200738212 and parameters: {'n_neighbors': 12}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,782] Trial 16 finished with value: 0.2993829200738212 and parameters: {'n_neighbors': 12}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,837] Trial 17 finished with value: 0.2675451207094603 and parameters: {'n_neighbors': 6}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,907] Trial 18 finished with value: 0.27527381620577224 and parameters: {'n_neighbors': 11}. Best is trial 2 with value: 0.2993829200738212.\n",
      "[I 2024-11-18 00:15:18,974] Trial 19 finished with value: 0.2675451207094603 and parameters: {'n_neighbors': 6}. Best is trial 2 with value: 0.2993829200738212.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to logs/mds_risk/te.csv\n"
     ]
    }
   ],
   "source": [
    "from src.evals.knn import KNNEvaluator\n",
    "\n",
    "knn_eval = KNNEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=20,\n",
    "    verbose=True,\n",
    "    params={\"k_lb\": 1, \"k_ub\": 20},\n",
    ")\n",
    "_ = knn_eval.evaluate()\n",
    "knn_eval.save_results(results_file=save_folder, row_name=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:19,005] A new study created in memory with name: no-name-a9cb552e-7e16-464f-be0a-6a110a6d07e7\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,095] Trial 0 finished with value: 0.3533582281013471 and parameters: {'C': 1.4679288090378353, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.3533582281013471.\n",
      "[I 2024-11-18 00:15:19,141] Trial 1 finished with value: 0.27950092796580706 and parameters: {'C': 0.0450071152389281, 'class_weight': None}. Best is trial 0 with value: 0.3533582281013471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.353\n",
      "Best model performance:\n",
      "Accuracy: 0.716 ± 0.059\n",
      "F1 Macro: 0.699 ± 0.059\n",
      "F1 Weighted: 0.706 ± 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,214] Trial 2 finished with value: 0.27769649776486144 and parameters: {'C': 0.3717063182747438, 'class_weight': None}. Best is trial 0 with value: 0.3533582281013471.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,299] Trial 3 finished with value: 0.3554227859869068 and parameters: {'C': 6.59663904986276, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:19,350] Trial 4 finished with value: 0.27950092796580706 and parameters: {'C': 0.04553720052764493, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:19,417] Trial 5 finished with value: 0.30625176947978955 and parameters: {'C': 0.12407515465193299, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:19,494] Trial 6 finished with value: 0.2822835610229991 and parameters: {'C': 0.1934135097439394, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model performance:\n",
      "Accuracy: 0.718 ± 0.078\n",
      "F1 Macro: 0.700 ± 0.070\n",
      "F1 Weighted: 0.706 ± 0.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,581] Trial 7 finished with value: 0.3554227859869068 and parameters: {'C': 2.836014083773884, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:19,627] Trial 8 finished with value: 0.3320016194771636 and parameters: {'C': 0.05117253098050185, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:19,677] Trial 9 finished with value: 0.30625176947978955 and parameters: {'C': 0.11028904989111978, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,769] Trial 10 finished with value: 0.3554227859869068 and parameters: {'C': 6.553759155771302, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,860] Trial 11 finished with value: 0.3554227859869068 and parameters: {'C': 9.722901972257432, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:19,953] Trial 12 finished with value: 0.3533582281013471 and parameters: {'C': 1.9742157716294781, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,047] Trial 13 finished with value: 0.3533582281013471 and parameters: {'C': 2.032975368969214, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:20,092] Trial 14 finished with value: 0.3088003130172081 and parameters: {'C': 0.010371889155461169, 'class_weight': None}. Best is trial 3 with value: 0.3554227859869068.\n",
      "[I 2024-11-18 00:15:20,177] Trial 15 finished with value: 0.3006297781363935 and parameters: {'C': 0.8487443964857727, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.3554227859869068.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,266] Trial 16 finished with value: 0.38233731164439955 and parameters: {'C': 4.04259155343055, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,356] Trial 17 finished with value: 0.3554227859869068 and parameters: {'C': 3.835991298737011, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,439] Trial 18 finished with value: 0.3006297781363935 and parameters: {'C': 0.5526038787561984, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.382\n",
      "Best model performance:\n",
      "Accuracy: 0.736 ± 0.067\n",
      "F1 Macro: 0.717 ± 0.061\n",
      "F1 Weighted: 0.724 ± 0.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,528] Trial 19 finished with value: 0.3554227859869068 and parameters: {'C': 5.125624641843633, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,610] Trial 20 finished with value: 0.32708018203453687 and parameters: {'C': 1.0586894359904704, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,699] Trial 21 finished with value: 0.3554227859869068 and parameters: {'C': 3.2734943049580183, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,785] Trial 22 finished with value: 0.3554227859869068 and parameters: {'C': 9.617320929600902, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,873] Trial 23 finished with value: 0.3554227859869068 and parameters: {'C': 2.6364559244516417, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:20,959] Trial 24 finished with value: 0.3554227859869068 and parameters: {'C': 4.783956749591383, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,039] Trial 25 finished with value: 0.3006297781363935 and parameters: {'C': 0.7534711769097218, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,127] Trial 26 finished with value: 0.38233731164439955 and parameters: {'C': 6.69022028540513, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,213] Trial 27 finished with value: 0.3554227859869068 and parameters: {'C': 6.48756417904097, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,298] Trial 28 finished with value: 0.3533582281013471 and parameters: {'C': 1.57400841641884, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,394] Trial 29 finished with value: 0.3554227859869068 and parameters: {'C': 9.934099397532522, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,475] Trial 30 finished with value: 0.3533582281013471 and parameters: {'C': 1.1598308293459687, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,565] Trial 31 finished with value: 0.38233731164439955 and parameters: {'C': 3.08124712452952, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,652] Trial 32 finished with value: 0.3554227859869068 and parameters: {'C': 5.767355927551497, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,735] Trial 33 finished with value: 0.3554227859869068 and parameters: {'C': 3.4311968708259735, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,827] Trial 34 finished with value: 0.3533582281013471 and parameters: {'C': 2.0931373680627243, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:21,940] Trial 35 finished with value: 0.3554227859869068 and parameters: {'C': 6.805498817468508, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,018] Trial 36 finished with value: 0.27769649776486144 and parameters: {'C': 0.3970800772270978, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,104] Trial 37 finished with value: 0.3554227859869068 and parameters: {'C': 4.262340416605392, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,186] Trial 38 finished with value: 0.3533582281013471 and parameters: {'C': 1.3865482311533335, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,274] Trial 39 finished with value: 0.3554227859869068 and parameters: {'C': 2.2732355045245156, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,359] Trial 40 finished with value: 0.3554227859869068 and parameters: {'C': 7.201699395496861, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,448] Trial 41 finished with value: 0.3554227859869068 and parameters: {'C': 3.0057382369338477, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "[I 2024-11-18 00:15:22,510] Trial 42 finished with value: 0.2822835610229991 and parameters: {'C': 0.19792042727106546, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,597] Trial 43 finished with value: 0.3554227859869068 and parameters: {'C': 4.318350918963146, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,685] Trial 44 finished with value: 0.3533582281013471 and parameters: {'C': 1.7167964870911854, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,772] Trial 45 finished with value: 0.3554227859869068 and parameters: {'C': 2.9361704778314137, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:22,860] Trial 46 finished with value: 0.3554227859869068 and parameters: {'C': 8.02862887712578, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n",
      "[I 2024-11-18 00:15:22,903] Trial 47 finished with value: 0.3088003130172081 and parameters: {'C': 0.010411297194680801, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "[I 2024-11-18 00:15:22,951] Trial 48 finished with value: 0.3320016194771636 and parameters: {'C': 0.0712950889033904, 'class_weight': None}. Best is trial 16 with value: 0.38233731164439955.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "[I 2024-11-18 00:15:23,041] Trial 49 finished with value: 0.3554227859869068 and parameters: {'C': 5.6468144483551015, 'class_weight': 'balanced'}. Best is trial 16 with value: 0.38233731164439955.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.svm import SVMEvaluator\n",
    "\n",
    "svm_eval = SVMEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=50,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"C_lb\": 0.01,\n",
    "        \"C_ub\": 10,\n",
    "        \"no_rfe\": True,\n",
    "        # \"rfe_step_range\": (0.05, 0.2),\n",
    "        # \"rfe_n_features_range\": (100, 200),\n",
    "    },\n",
    "    mode=\"linear\",\n",
    ")\n",
    "_ = svm_eval.evaluate()\n",
    "svm_eval.save_results(results_file=save_folder, row_name=\"Linear SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:23,075] A new study created in memory with name: no-name-02c969d1-6245-4638-ad77-d6cbfda5aa4c\n",
      "[I 2024-11-18 00:15:23,215] Trial 0 finished with value: 0.07498688884758245 and parameters: {'lambda': 1.6429371831045578e-08, 'alpha': 0.8320550236530617}. Best is trial 0 with value: 0.07498688884758245.\n",
      "[I 2024-11-18 00:15:23,308] Trial 1 finished with value: 0.3028024714191348 and parameters: {'lambda': 0.12672876915326495, 'alpha': 0.006491647295887068}. Best is trial 1 with value: 0.3028024714191348.\n",
      "[I 2024-11-18 00:15:23,394] Trial 2 finished with value: 0.3533582281013471 and parameters: {'lambda': 1.5482219717269852e-08, 'alpha': 0.00031085553490061476}. Best is trial 2 with value: 0.3533582281013471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.075\n",
      "Best model performance:\n",
      "Accuracy: 0.547 ± 0.032\n",
      "F1 Macro: 0.353 ± 0.013\n",
      "F1 Weighted: 0.388 ± 0.037\n",
      "New best score: 0.303\n",
      "Best model performance:\n",
      "Accuracy: 0.680 ± 0.068\n",
      "F1 Macro: 0.664 ± 0.063\n",
      "F1 Weighted: 0.670 ± 0.066\n",
      "New best score: 0.353\n",
      "Best model performance:\n",
      "Accuracy: 0.716 ± 0.059\n",
      "F1 Macro: 0.699 ± 0.059\n",
      "F1 Weighted: 0.706 ± 0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:23,462] Trial 3 finished with value: 0.07498688884758245 and parameters: {'lambda': 0.004249677273720232, 'alpha': 0.35338318022165094}. Best is trial 2 with value: 0.3533582281013471.\n",
      "[I 2024-11-18 00:15:23,560] Trial 4 finished with value: 0.4105771253471802 and parameters: {'lambda': 0.002492791230890914, 'alpha': 0.0003972197861400822}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:23,689] Trial 5 finished with value: 0.3028024714191348 and parameters: {'lambda': 0.1270440893960908, 'alpha': 2.7083524526375607e-07}. Best is trial 4 with value: 0.4105771253471802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.411\n",
      "Best model performance:\n",
      "Accuracy: 0.755 ± 0.046\n",
      "F1 Macro: 0.734 ± 0.045\n",
      "F1 Weighted: 0.741 ± 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:23,866] Trial 6 finished with value: 0.2930684811820436 and parameters: {'lambda': 0.0030646935892124174, 'alpha': 1.7507502816551927e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,128] Trial 7 finished with value: 0.30176297924066064 and parameters: {'lambda': 0.0031784319189296674, 'alpha': 0.0038403030428259483}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,207] Trial 8 finished with value: 0.32708018203453687 and parameters: {'lambda': 0.0834619573672111, 'alpha': 1.5304039519658182e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,292] Trial 9 finished with value: 0.4105771253471802 and parameters: {'lambda': 1.3768514885276428e-07, 'alpha': 1.0923237178355021e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,412] Trial 10 finished with value: 0.3533582281013471 and parameters: {'lambda': 1.0213416395460216e-05, 'alpha': 2.3120893484373963e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,516] Trial 11 finished with value: 0.4105771253471802 and parameters: {'lambda': 3.3929561126637473e-06, 'alpha': 3.70304500670002e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,628] Trial 12 finished with value: 0.3533582281013471 and parameters: {'lambda': 9.820688287020917e-07, 'alpha': 1.0082980288806663e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,723] Trial 13 finished with value: 0.3533582281013471 and parameters: {'lambda': 0.00010839225939477397, 'alpha': 1.289220926095179e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,796] Trial 14 finished with value: 0.32668437912337417 and parameters: {'lambda': 0.00010195672656027224, 'alpha': 0.0007258680994611436}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,875] Trial 15 finished with value: 0.29419713414016807 and parameters: {'lambda': 2.412561265832152e-07, 'alpha': 1.04467018594027e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:24,975] Trial 16 finished with value: 0.15047574550701828 and parameters: {'lambda': 0.0005314283826515501, 'alpha': 0.0248578460548552}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,086] Trial 17 finished with value: 0.3533582281013471 and parameters: {'lambda': 2.622591308292876e-07, 'alpha': 8.098735029899589e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,289] Trial 18 finished with value: 0.38233731164439955 and parameters: {'lambda': 1.6320939935023545e-05, 'alpha': 0.0002593099188522022}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,390] Trial 19 finished with value: 0.3076571809675507 and parameters: {'lambda': 0.7078584441064064, 'alpha': 3.544216203809911e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,498] Trial 20 finished with value: 0.250928945986354 and parameters: {'lambda': 0.017690949438324916, 'alpha': 0.0878871774866867}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,574] Trial 21 finished with value: 0.4105771253471802 and parameters: {'lambda': 2.8962855094997134e-06, 'alpha': 6.11603813996966e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,674] Trial 22 finished with value: 0.32668437912337417 and parameters: {'lambda': 1.393659018141858e-07, 'alpha': 0.0020281965928824054}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,758] Trial 23 finished with value: 0.38016989181758415 and parameters: {'lambda': 7.275563849610752e-06, 'alpha': 2.2642613895050442e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,853] Trial 24 finished with value: 0.38016989181758415 and parameters: {'lambda': 0.0005527501141239093, 'alpha': 6.070590204003502e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:25,949] Trial 25 finished with value: 0.38016989181758415 and parameters: {'lambda': 6.776885032645177e-08, 'alpha': 1.832136707031843e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,025] Trial 26 finished with value: 0.38233731164439955 and parameters: {'lambda': 1.739274188491251e-06, 'alpha': 7.807881339568267e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,120] Trial 27 finished with value: 0.32668437912337417 and parameters: {'lambda': 2.552601946242847e-05, 'alpha': 0.0008516770395211805}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,203] Trial 28 finished with value: 0.20482050443811936 and parameters: {'lambda': 7.953866298493945e-07, 'alpha': 0.017657747215790904}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,276] Trial 29 finished with value: 0.3533582281013471 and parameters: {'lambda': 3.332945515610173e-08, 'alpha': 0.00014073318608097356}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,379] Trial 30 finished with value: 0.38016989181758415 and parameters: {'lambda': 0.00044406508097478926, 'alpha': 5.28971308497307e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,557] Trial 31 finished with value: 0.38016989181758415 and parameters: {'lambda': 2.117302013777889e-06, 'alpha': 5.265179445542512e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,643] Trial 32 finished with value: 0.32668437912337417 and parameters: {'lambda': 3.6908823168079067e-06, 'alpha': 0.0010650962471469962}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,735] Trial 33 finished with value: 0.32708018203453687 and parameters: {'lambda': 4.9892168032602e-07, 'alpha': 8.221811955977349e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,819] Trial 34 finished with value: 0.38233731164439955 and parameters: {'lambda': 1.1652748939529925e-08, 'alpha': 0.00012749666394642957}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,902] Trial 35 finished with value: 0.2822835610229991 and parameters: {'lambda': 6.25297484681754e-05, 'alpha': 0.00922279188788711}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:26,989] Trial 36 finished with value: 0.38233731164439955 and parameters: {'lambda': 5.496180306764643e-06, 'alpha': 3.7642584717180696e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,069] Trial 37 finished with value: 0.3533582281013471 and parameters: {'lambda': 6.373607842786142e-08, 'alpha': 0.00022244176558406767}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,149] Trial 38 finished with value: 0.250928945986354 and parameters: {'lambda': 0.0013212777303191682, 'alpha': 0.08017456772233009}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,252] Trial 39 finished with value: 0.3533582281013471 and parameters: {'lambda': 0.008463671870488756, 'alpha': 4.897642446135288e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,346] Trial 40 finished with value: 0.30176297924066064 and parameters: {'lambda': 2.636251088587171e-06, 'alpha': 0.000553051640754624}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,517] Trial 41 finished with value: 0.26990356430217477 and parameters: {'lambda': 1.1905276557408432e-05, 'alpha': 0.0003133951081612899}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,595] Trial 42 finished with value: 0.3533582281013471 and parameters: {'lambda': 2.5692698247363118e-05, 'alpha': 1.4058032214350658e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,700] Trial 43 finished with value: 0.07498688884758245 and parameters: {'lambda': 3.22703943908501e-05, 'alpha': 0.9881990468847279}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,785] Trial 44 finished with value: 0.3542068346168205 and parameters: {'lambda': 0.0003018554636419171, 'alpha': 0.0024483293094808463}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,892] Trial 45 finished with value: 0.3533582281013471 and parameters: {'lambda': 5.06806615003881e-07, 'alpha': 0.0003040603777822643}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:27,988] Trial 46 finished with value: 0.38016989181758415 and parameters: {'lambda': 1.2731564675823998e-06, 'alpha': 7.24915601200722e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,091] Trial 47 finished with value: 0.38233731164439955 and parameters: {'lambda': 1.186578413829763e-05, 'alpha': 7.364869809980379e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,195] Trial 48 finished with value: 0.3221628269974105 and parameters: {'lambda': 0.0002211326198901209, 'alpha': 0.00354031455056945}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,273] Trial 49 finished with value: 0.3533582281013471 and parameters: {'lambda': 5.3874611229805064e-05, 'alpha': 3.1931576286854476e-05}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,369] Trial 50 finished with value: 0.3533582281013471 and parameters: {'lambda': 0.015659724982518, 'alpha': 5.761636081491835e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,469] Trial 51 finished with value: 0.38016989181758415 and parameters: {'lambda': 2.4250048213440174e-07, 'alpha': 3.074760504619106e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,545] Trial 52 finished with value: 0.38016989181758415 and parameters: {'lambda': 1.6821229256679511e-06, 'alpha': 1.8949399415644015e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,637] Trial 53 finished with value: 0.3533582281013471 and parameters: {'lambda': 4.107499958717034e-06, 'alpha': 2.4146470731466522e-08}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,746] Trial 54 finished with value: 0.3533582281013471 and parameters: {'lambda': 1.189376527206748e-07, 'alpha': 1.044511393641387e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,833] Trial 55 finished with value: 0.3533582281013471 and parameters: {'lambda': 0.062277210635564954, 'alpha': 3.998149920066154e-07}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:28,968] Trial 56 finished with value: 0.3533582281013471 and parameters: {'lambda': 6.747412948845849e-07, 'alpha': 2.2265593211829203e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:29,061] Trial 57 finished with value: 0.38016989181758415 and parameters: {'lambda': 1.7495464370066347e-05, 'alpha': 7.440934925640266e-06}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:29,134] Trial 58 finished with value: 0.3521464971292444 and parameters: {'lambda': 2.7918231030133422e-08, 'alpha': 0.0016046777074288881}. Best is trial 4 with value: 0.4105771253471802.\n",
      "[I 2024-11-18 00:15:29,262] Trial 59 finished with value: 0.38233731164439955 and parameters: {'lambda': 0.0014094963168559606, 'alpha': 1.0131482557758055e-08}. Best is trial 4 with value: 0.4105771253471802.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.755 ± 0.046\n",
      "F1 Macro: 0.734 ± 0.045\n",
      "F1 Weighted: 0.741 ± 0.049\n"
     ]
    }
   ],
   "source": [
    "from src.evals.xgboost import XGBoostEvaluator\n",
    "\n",
    "xgb_eval = XGBoostEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=60,\n",
    "    verbose=True,\n",
    ")\n",
    "_ = xgb_eval.evaluate()\n",
    "xgb_eval.print_best_results()\n",
    "xgb_eval.save_results(results_file=save_folder, row_name=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:29,294] A new study created in memory with name: no-name-2b6f55ce-2341-4b5a-87fe-e6b498c7b7d9\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:30,609] Trial 0 finished with value: 0.2771075311053424 and parameters: {'lr': 0.00037334390959603526, 'dropout': 0.13563045565689993}. Best is trial 0 with value: 0.2771075311053424.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.277\n",
      "Best model performance:\n",
      "Accuracy: 0.664 ± 0.102\n",
      "F1 Macro: 0.644 ± 0.121\n",
      "F1 Weighted: 0.648 ± 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:31,837] Trial 1 finished with value: 0.26797065461627445 and parameters: {'lr': 0.00031435126504789475, 'dropout': 0.1686326160102216}. Best is trial 0 with value: 0.2771075311053424.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:33,189] Trial 2 finished with value: 0.25286609380923725 and parameters: {'lr': 0.0004341247404744546, 'dropout': 0.11648731312622287}. Best is trial 0 with value: 0.2771075311053424.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:34,481] Trial 3 finished with value: 0.3183781593304391 and parameters: {'lr': 0.002945439763174446, 'dropout': 0.40947191135843664}. Best is trial 3 with value: 0.3183781593304391.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.318\n",
      "Best model performance:\n",
      "Accuracy: 0.696 ± 0.111\n",
      "F1 Macro: 0.673 ± 0.120\n",
      "F1 Weighted: 0.680 ± 0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:35,796] Trial 4 finished with value: 0.280332216389671 and parameters: {'lr': 0.0005265822626079916, 'dropout': 0.4644932203625811}. Best is trial 3 with value: 0.3183781593304391.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:37,178] Trial 5 finished with value: 0.2583682580535473 and parameters: {'lr': 0.00026403316207361765, 'dropout': 0.45950922147600715}. Best is trial 3 with value: 0.3183781593304391.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:38,542] Trial 6 finished with value: 0.24436145454545458 and parameters: {'lr': 0.0003105974241629549, 'dropout': 0.567797029654837}. Best is trial 3 with value: 0.3183781593304391.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:39,886] Trial 7 finished with value: 0.33293482865489865 and parameters: {'lr': 0.0012212125769189902, 'dropout': 0.5403336512373779}. Best is trial 7 with value: 0.33293482865489865.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.333\n",
      "Best model performance:\n",
      "Accuracy: 0.700 ± 0.102\n",
      "F1 Macro: 0.688 ± 0.109\n",
      "F1 Weighted: 0.691 ± 0.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:41,167] Trial 8 finished with value: 0.1885951157223285 and parameters: {'lr': 0.003108397004173857, 'dropout': 0.24583407886170108}. Best is trial 7 with value: 0.33293482865489865.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/lubojjan/DiplomaGeneral/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "[I 2024-11-18 00:15:42,456] Trial 9 finished with value: 0.27193468218461864 and parameters: {'lr': 0.00019981176236523842, 'dropout': 0.28602311473705977}. Best is trial 7 with value: 0.33293482865489865.\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mlp import MLPEvaluator\n",
    "\n",
    "mlp_eval = MLPEvaluator(\n",
    "    data_manager=odm,\n",
    "    n_trials=10,\n",
    "    verbose=True,\n",
    "    params={\n",
    "        \"lr_range\": [1e-4, 1e-2],\n",
    "        \"l2_lambda\": 5e-4,\n",
    "        \"dropout_range\": [0.1, 0.6],\n",
    "        \"hidden_channels\": 64,\n",
    "        \"proj_dim\": 64,\n",
    "        \"batch_size\": 32,\n",
    "        \"max_epochs\": 50,\n",
    "    },\n",
    ")\n",
    "_ = mlp_eval.evaluate()\n",
    "mlp_eval.save_results(results_file=save_folder, row_name=\"MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model performance:\n",
      "Accuracy: 0.700 ± 0.102\n",
      "F1 Macro: 0.688 ± 0.109\n",
      "F1 Weighted: 0.691 ± 0.111\n",
      "Best hyperparameters:\n",
      "{'lr': 0.0012212125769189902, 'dropout': 0.5403336512373779}\n"
     ]
    }
   ],
   "source": [
    "mlp_eval.print_best_results()\n",
    "mlp_eval.print_best_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:42,570] A new study created in memory with name: no-name-258b29e4-c045-4335-8a59-e051f877e821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n",
      "Using: vcdn integrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:52,284] Trial 0 finished with value: 0.5224244183647915 and parameters: {}. Best is trial 0 with value: 0.5224244183647915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 0.522\n",
      "Best model performance:\n",
      "Accuracy: 0.811 ± 0.058\n",
      "F1 Macro: 0.801 ± 0.071\n",
      "F1 Weighted: 0.805 ± 0.068\n",
      "Best model performance:\n",
      "Accuracy: 0.811 ± 0.058\n",
      "F1 Macro: 0.801 ± 0.071\n",
      "F1 Weighted: 0.805 ± 0.068\n"
     ]
    }
   ],
   "source": [
    "from src.evals.mogonet import MOGONETEvaluator\n",
    "from src.data_managers.sample_graph import SampleGraphDataManager\n",
    "\n",
    "mogonet_eval = MOGONETEvaluator(\n",
    "    data_manager=SampleGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"graph_style\": \"threshold\",\n",
    "            \"self_connections\": True,\n",
    "            \"avg_degree\": 5,\n",
    "        },\n",
    "    ),\n",
    "    n_trials=1,\n",
    "    params={\n",
    "        \"encoder_hidden_channels\": {\n",
    "            \"mrna\": 64,\n",
    "            \"mirna\": 64,\n",
    "            \"circrna\": 64,\n",
    "            \"te\": 64,\n",
    "        },\n",
    "        \"encoder_type\": \"gat\",\n",
    "        \"dropout\": 0.2,\n",
    "        \"integrator_type\": \"vcdn\", # vcdn seems to work better on mds disease\n",
    "        \"integration_in_dim\": 16,\n",
    "        \"vcdn_hidden_channels\": 16,\n",
    "        \"epochs\": 250,\n",
    "        \"log_interval\": 251,\n",
    "    }\n",
    ")\n",
    "mogonet_eval.evaluate()\n",
    "mogonet_eval.print_best_results()\n",
    "mogonet_eval.save_results(results_file=save_folder, row_name=\"MOGONET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "# vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.825 ± 0.030\n",
    "F1 Macro: 0.452 ± 0.009\n",
    "F1 Weighted: 0.746 ± 0.043\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.957 ± 0.053\n",
    "F1 Weighted: 0.973 ± 0.033\n",
    "- integration dim = 16\n",
    "Accuracy: 0.973 ± 0.053\n",
    "F1 Macro: 0.958 ± 0.083\n",
    "F1 Weighted: 0.973 ± 0.053\n",
    "# attention - faster than vcdn\n",
    "- integration dim = 2\n",
    "Accuracy: 0.933 ± 0.060\n",
    "F1 Macro: 0.877 ± 0.114\n",
    "F1 Weighted: 0.927 ± 0.067\n",
    "- integration dim = 8\n",
    "Accuracy: 0.973 ± 0.033\n",
    "F1 Macro: 0.952 ± 0.059\n",
    "F1 Weighted: 0.971 ± 0.035\n",
    "- integration dim = 12\n",
    "Accuracy: 0.947 ± 0.050\n",
    "F1 Macro: 0.910 ± 0.080\n",
    "F1 Weighted: 0.945 ± 0.051\n",
    "- integration dim = 16\n",
    "Accuracy: 0.960 ± 0.053\n",
    "F1 Macro: 0.934 ± 0.085\n",
    "F1 Weighted: 0.959 ± 0.054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:15:52,370] A new study created in memory with name: no-name-4896e082-0872-4bbe-9781-4737519fd685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(1) tensor(0) tensor(15.9623)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6851, Train Acc: 0.5476, Train F1 Macro: 0.3538, Train F1 Weighted: 0.3875\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.3529, Val F1 Weighted: 0.3850, Val Geometric Mean: 0.4201\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.3529, Test F1 Weighted: 0.3850\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6978, Train Acc: 0.4286, Train F1 Macro: 0.4233, Train F1 Weighted: 0.4181\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.3125, Val F1 Weighted: 0.2841, Val Geometric Mean: 0.3430\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.3125, Test F1 Weighted: 0.2841\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.6928, Train Acc: 0.5238, Train F1 Macro: 0.3438, Train F1 Weighted: 0.3765\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.3529, Val F1 Weighted: 0.3850, Val Geometric Mean: 0.4201\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.3529, Test F1 Weighted: 0.3850\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.6265, Train Acc: 0.6667, Train F1 Macro: 0.6636, Train F1 Weighted: 0.6667\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.4762, Val F1 Weighted: 0.4589, Val Geometric Mean: 0.4921\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.4762, Test F1 Weighted: 0.4589\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.5125, Train Acc: 0.7857, Train F1 Macro: 0.7796, Train F1 Weighted: 0.7831\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.3961, Val Geometric Mean: 0.4197\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.3961\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.4902, Train Acc: 0.7857, Train F1 Macro: 0.7796, Train F1 Weighted: 0.7831\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.3961, Val Geometric Mean: 0.4197\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.3961\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.3164, Train Acc: 0.8571, Train F1 Macro: 0.8542, Train F1 Weighted: 0.8562\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.3961, Val Geometric Mean: 0.4197\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.3961\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(4) tensor(0) tensor(15.6415)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6390, Train Acc: 0.6190, Train F1 Macro: 0.6047, Train F1 Weighted: 0.6119\n",
      "Val Acc: 0.3636, Val F1 Macro: 0.2667, Val F1 Weighted: 0.2909, Val Geometric Mean: 0.3044\n",
      "Test Acc: 0.3636, Test F1 Macro: 0.2667, Test F1 Weighted: 0.2909\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.5065, Train Acc: 0.7619, Train F1 Macro: 0.7476, Train F1 Weighted: 0.7533\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.3911, Train Acc: 0.8095, Train F1 Macro: 0.8056, Train F1 Weighted: 0.8082\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4500, Val F1 Weighted: 0.4545, Val Geometric Mean: 0.4530\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4500, Test F1 Weighted: 0.4545\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.2770, Train Acc: 0.8571, Train F1 Macro: 0.8568, Train F1 Weighted: 0.8575\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6364, Val Geometric Mean: 0.6354\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6364\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.3104, Train Acc: 0.8571, Train F1 Macro: 0.8568, Train F1 Weighted: 0.8575\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.2159, Train Acc: 0.9048, Train F1 Macro: 0.9045, Train F1 Weighted: 0.9050\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.5455, Val F1 Weighted: 0.5455, Val Geometric Mean: 0.5455\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.5455, Test F1 Weighted: 0.5455\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1556, Train Acc: 0.9286, Train F1 Macro: 0.9282, Train F1 Weighted: 0.9287\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6364, Val Geometric Mean: 0.6354\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6364\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(2) tensor(0) tensor(15.9245)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6875, Train Acc: 0.5476, Train F1 Macro: 0.4997, Train F1 Weighted: 0.5144\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.3529, Val F1 Weighted: 0.3850, Val Geometric Mean: 0.4201\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.3529, Test F1 Weighted: 0.3850\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6701, Train Acc: 0.5476, Train F1 Macro: 0.5143, Train F1 Weighted: 0.5264\n",
      "Val Acc: 0.2727, Val F1 Macro: 0.2667, Val F1 Weighted: 0.2727, Val Geometric Mean: 0.2707\n",
      "Test Acc: 0.2727, Test F1 Macro: 0.2667, Test F1 Weighted: 0.2727\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.4588, Train Acc: 0.8095, Train F1 Macro: 0.8091, Train F1 Weighted: 0.8082\n",
      "Val Acc: 0.5455, Val F1 Macro: 0.4762, Val F1 Weighted: 0.4589, Val Geometric Mean: 0.4921\n",
      "Test Acc: 0.5455, Test F1 Macro: 0.4762, Test F1 Weighted: 0.4589\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3087, Train Acc: 0.8810, Train F1 Macro: 0.8809, Train F1 Weighted: 0.8806\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6071, Val F1 Weighted: 0.5974, Val Geometric Mean: 0.6134\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6071, Test F1 Weighted: 0.5974\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.2965, Train Acc: 0.8333, Train F1 Macro: 0.8325, Train F1 Weighted: 0.8336\n",
      "Val Acc: 0.4545, Val F1 Macro: 0.4107, Val F1 Weighted: 0.3961, Val Geometric Mean: 0.4197\n",
      "Test Acc: 0.4545, Test F1 Macro: 0.4107, Test F1 Weighted: 0.3961\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.1577, Train Acc: 0.9524, Train F1 Macro: 0.9523, Train F1 Weighted: 0.9525\n",
      "Val Acc: 0.6364, Val F1 Macro: 0.6333, Val F1 Weighted: 0.6303, Val Geometric Mean: 0.6333\n",
      "Test Acc: 0.6364, Test F1 Macro: 0.6333, Test F1 Weighted: 0.6303\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1321, Train Acc: 0.9524, Train F1 Macro: 0.9523, Train F1 Weighted: 0.9525\n",
      "Val Acc: 0.7273, Val F1 Macro: 0.7273, Val F1 Weighted: 0.7273, Val Geometric Mean: 0.7273\n",
      "Test Acc: 0.7273, Test F1 Macro: 0.7273, Test F1 Weighted: 0.7273\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(3) tensor(0) tensor(15.5283)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6898, Train Acc: 0.4651, Train F1 Macro: 0.3175, Train F1 Weighted: 0.3396\n",
      "Val Acc: 0.6000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4500, Val Geometric Mean: 0.4661\n",
      "Test Acc: 0.6000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4500\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6318, Train Acc: 0.6047, Train F1 Macro: 0.6047, Train F1 Weighted: 0.6047\n",
      "Val Acc: 0.2000, Val F1 Macro: 0.2000, Val F1 Weighted: 0.2000, Val Geometric Mean: 0.2000\n",
      "Test Acc: 0.2000, Test F1 Macro: 0.2000, Test F1 Weighted: 0.2000\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.5727, Train Acc: 0.6279, Train F1 Macro: 0.6261, Train F1 Weighted: 0.6279\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.2788, Val Geometric Mean: 0.2904\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.2788\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.4548, Train Acc: 0.8372, Train F1 Macro: 0.8369, Train F1 Weighted: 0.8374\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.3207, Train Acc: 0.8372, Train F1 Macro: 0.8369, Train F1 Weighted: 0.8374\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.2048, Train Acc: 0.9535, Train F1 Macro: 0.9533, Train F1 Weighted: 0.9535\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.4000, Val Geometric Mean: 0.3915\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.4000\n",
      "##################################################\n",
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.2085, Train Acc: 0.8837, Train F1 Macro: 0.8835, Train F1 Weighted: 0.8838\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2929, Val F1 Weighted: 0.3071, Val Geometric Mean: 0.2999\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2929, Test F1 Weighted: 0.3071\n",
      "##################################################\n",
      "isolated sample nodes, isolated gene nodes, mean degree: \n",
      "tensor(3) tensor(0) tensor(15.6981)\n",
      "\n",
      "Epoch: 050:\n",
      "Train Loss: 0.6931, Train Acc: 0.6279, Train F1 Macro: 0.5376, Train F1 Weighted: 0.5614\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.3333, Val F1 Weighted: 0.3333, Val Geometric Mean: 0.3816\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.3333, Test F1 Weighted: 0.3333\n",
      "##################################################\n",
      "\n",
      "Epoch: 100:\n",
      "Train Loss: 0.6581, Train Acc: 0.5814, Train F1 Macro: 0.5393, Train F1 Weighted: 0.5555\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.4505, Val F1 Weighted: 0.4505, Val Geometric Mean: 0.4665\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.4505, Test F1 Weighted: 0.4505\n",
      "##################################################\n",
      "\n",
      "Epoch: 150:\n",
      "Train Loss: 0.5945, Train Acc: 0.6977, Train F1 Macro: 0.6721, Train F1 Weighted: 0.6828\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.3750, Val Geometric Mean: 0.3832\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.3750\n",
      "##################################################\n",
      "\n",
      "Epoch: 200:\n",
      "Train Loss: 0.3798, Train Acc: 0.8140, Train F1 Macro: 0.8009, Train F1 Weighted: 0.8068\n",
      "Val Acc: 0.5000, Val F1 Macro: 0.4505, Val F1 Weighted: 0.4505, Val Geometric Mean: 0.4665\n",
      "Test Acc: 0.5000, Test F1 Macro: 0.4505, Test F1 Weighted: 0.4505\n",
      "##################################################\n",
      "\n",
      "Epoch: 250:\n",
      "Train Loss: 0.3787, Train Acc: 0.7674, Train F1 Macro: 0.7611, Train F1 Weighted: 0.7656\n",
      "Val Acc: 0.4000, Val F1 Macro: 0.3750, Val F1 Weighted: 0.3750, Val Geometric Mean: 0.3832\n",
      "Test Acc: 0.4000, Test F1 Macro: 0.3750, Test F1 Weighted: 0.3750\n",
      "##################################################\n",
      "\n",
      "Epoch: 300:\n",
      "Train Loss: 0.2496, Train Acc: 0.9302, Train F1 Macro: 0.9296, Train F1 Weighted: 0.9304\n",
      "Val Acc: 0.3000, Val F1 Macro: 0.2308, Val F1 Weighted: 0.2308, Val Geometric Mean: 0.2519\n",
      "Test Acc: 0.3000, Test F1 Macro: 0.2308, Test F1 Weighted: 0.2308\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-18 00:17:16,848] Trial 0 finished with value: 0.2703217721000839 and parameters: {}. Best is trial 0 with value: 0.2703217721000839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 350:\n",
      "Train Loss: 0.1322, Train Acc: 0.9535, Train F1 Macro: 0.9522, Train F1 Weighted: 0.9531\n",
      "Val Acc: 0.2000, Val F1 Macro: 0.1667, Val F1 Weighted: 0.1667, Val Geometric Mean: 0.1771\n",
      "Test Acc: 0.2000, Test F1 Macro: 0.1667, Test F1 Weighted: 0.1667\n",
      "##################################################\n",
      "New best score: 0.270\n",
      "Best model performance:\n",
      "Accuracy: 0.656 ± 0.109\n",
      "F1 Macro: 0.637 ± 0.120\n",
      "F1 Weighted: 0.646 ± 0.114\n",
      "Best model performance:\n",
      "Accuracy: 0.656 ± 0.109\n",
      "F1 Macro: 0.637 ± 0.120\n",
      "F1 Weighted: 0.646 ± 0.114\n"
     ]
    }
   ],
   "source": [
    "from src.evals.birgat import BiRGATEvaluator\n",
    "from src.data_managers.bipartite_graph import BipartiteGraphDataManager\n",
    "\n",
    "birgat_eval = BiRGATEvaluator(\n",
    "    data_manager=BipartiteGraphDataManager(\n",
    "        omic_data_loaders=omic_data_loaders,\n",
    "        n_splits=5,\n",
    "        params={\n",
    "            \"diff_exp_thresholds\" : {\n",
    "                \"mrna\": 1.5,\n",
    "                \"mirna\": 1.5,\n",
    "                \"circrna\": 1.7,\n",
    "                \"te\": 1.7,\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    params={\n",
    "        \"epochs\": 350,\n",
    "        \"log_interval\": 50,\n",
    "        \"hidden_channels\": [200, 64, 64, 16, 16],\n",
    "        \"heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"attention_dropout\": 0.0,\n",
    "        \"integrator_type\": \"vcdn\",\n",
    "        \"three_layers\": True,\n",
    "    },\n",
    "    n_trials=1,\n",
    ")\n",
    "birgat_eval.evaluate()\n",
    "birgat_eval.print_best_results()\n",
    "birgat_eval.save_results(results_file=save_folder, row_name=\"BiRGAT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
